query,output,flag,username,timestamp
Large Language Models as World Models,"'You are Perplexica, an AI model who is expert at searching the web and answering user's queries.

Generate a response that is informative and relevant to the user's query based on provided context (the context consits of search results containg a brief description of the content of that page).
You must use this context to answer the user's query in the best way possible. Use an unbaised and journalistic tone in your response. Do not repeat the text.
You must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself. If the user asks for links you can provide them.
Your responses should be long in length be informative and relevant to the user's query. You can use markdowns to format your response. You should use the template provided below in \`template\` section. Make sure the answer is not short and is informative.
You have to cite the answer using [number] notation. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.
Place these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].
However you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) used to generate that part of the answer.



Com base nos documentos em <context> retornados por um sistema de buscas em artigos científicos armazenados no ArXiv, use o <template> fornecido abaixo para criar um resumo abrangente que contenha cada um aplicando o template passando o nome do capítulo ou principal conceito explorado nele como o **X = ** do <template>. 

Diretrizes para o resumo:
Os resumos devem ser avançados;
Os resumos devem ser baseados nos principais aspectos do conceito abordado no texto, como técnicas ou funcionalidades específicas demonstradas em cada subcapítulo;
O resumo deve conter todas principais informações presentes no texto sem omitir nenhum dado importante, com foco especial em não pular nenhum conceitos, resultados importante, argumentos, etc;
O resumo deve conter as equações apresentadas, tabelas e outras informações críticas para um entendimento aprofundando e avançado do conteúdo;
O resumo deve ser escrito de uma maneira acadêmica, do not repeat text.
Você deve usar o <context> da melhor maneira possível para responder a query do usuário e escrever o resumo segundo as diretrizes;
You must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself;
Você não deve pedir para o usuário abrir um link ou visitar um site para ver a resposta. Você deve responder você mesmo;
You have to cite the answer using [number] notation. The number is the idx on the documents. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.
Place these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].
However you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) used to generate that part of the answer.
Coloque os resultados um texto coerente ao invés de apenas listar em tópicos, também foque em usar as formatações mostradas no template.

Aything inside the following \`context\` HTML block provided below is for your knowledge returned by the search engine and is not shared by the user. You have to answer question on the basis of it and cite the relevant information from it but you do not have to talk about the context in your response. 

<context>
    <Document index=1 title=Training Compute-Optimal Large Language Models><Sumary>We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher.</Sumary><Content>et al., 2021; Fedus et al., 2021; Zoph et al., 2022). The largest dense transformers have passed 500
billion parameters (Smith et al., 2022). The drive to train larger and larger models is clear—so far
increasing the size of language models has been responsible for improving the state-of-the-art in many
language modelling tasks. Nonetheless, large language models face several challenges, including
their overwhelming computational requirements (the cost of training and inference increase with
model size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality
training data. In fact, in this work we ﬁnd that larger, high quality datasets will play a key role in any
further scaling of language models.
Modelling the scaling behavior.
Understanding the scaling behaviour of language models and
their transfer properties has been important in the development of recent large models (Hernandez</Content></Document>
<Document index=2 title=Training Compute-Optimal Large Language Models><Sumary>We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher.</Sumary><Content>how performance of large language models and toxicity interact is an important future research
question.
While we have applied our methodology towards the training of auto-regressive language models,
we expect that there is a similar trade-oﬀbetween model size and the amount of data in other
modalities. As training large models is very expensive, choosing the optimal model size and training
steps beforehand is essential. The methods we propose are easy to reproduce in new settings.
6. Acknowledgements
We’d like to thank Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis,
Geoﬀrey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on
the manuscript. We’d like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and
other colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA
team for their support and assistance.
References</Content></Document>
<Document index=3 title=BLOOM: A 176B-Parameter Open-Access Multilingual Language Model><Sumary>Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.</Sumary><Content>lingual settings. In Challenges & Perspectives in Creating Large Language Models, 2022.
URL https://openreview.net/forum?id=rK-7NhfSIW5.
Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, Xavier
Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending
scaling laws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022.
Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar
Mirkin, and Aaron Gokaslan. Emergent structures and training dynamics in large lan-
guage models. In Proceedings of BigScience Episode #5 – Workshop on Challenges &
Perspectives in Creating Large Language Models, pages 146–159, virtual+Dublin, May
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.11.
URL https://aclanthology.org/2022.bigscience-1.11.
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Na-
joung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do</Content></Document>
<Document index=4 title=Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models><Sumary>We present Branch-Train-Merge (BTM), a communication-efficient algorithm for
embarrassingly parallel training of large language models (LLMs). We show it is
possible to independently train subparts of a new class of LLMs on different
subsets of the data, eliminating the massive multi-node synchronization
currently required to train LLMs. BTM learns a set of independent expert LMs
(ELMs), each specialized to a different textual domain, such as scientific or
legal text. These ELMs can be added and removed to update data coverage,
ensembled to generalize to new domains, or averaged to collapse back to a
single LM for efficient inference. New ELMs are learned by branching from
(mixtures of) ELMs in the current set, further training the parameters on data
for the new domain, and then merging the resulting model back into the set for
future use. Experiments show that BTM improves in- and out-of-domain
perplexities as compared to GPT-style Transformer LMs, when controlling for
training cost. Through extensive analysis, we show that these results are
robust to different ELM initialization schemes, but require expert domain
specialization; LM ensembles with random data splits do not perform well. We
also present a study of scaling BTM into a new corpus of 64 domains (192B
whitespace-separated tokens in total); the resulting LM (22.4B total
parameters) performs as well as a Transformer LM trained with 2.5 times more
compute. These gains grow with the number of domains, suggesting more
aggressive parallelism could be used to efficiently train larger models in
future work.</Sumary><Content>large language models with many smaller, independently trained ELMs. We envision that this
work lays the foundation for democratized model development at inclusive compute budgets — that
groups with different resource constraints and research interests may combine efforts to build open-
sourced, community-authored large language models, comprised of continually-evolving repositories
of EXPERT LMs.
Acknowledgments and Disclosure of Funding
This paper beneﬁted from thoughtful feedback from a number of people: Ari Holtzman, Candace
Ross, Colin Raffel, Gabriel Ilharco, Ishaan Gulrajani, Julian Michael, Mitchell Wortsman, Stephen
Roller, Swabha Swayamdipta and William Fedus.
At UW, this work was partially supported by the Ofﬁce of Naval Research under MURI grant
N00014-18-1-2670.
References
Roee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages</Content></Document>
<Document index=5 title=Scaling Language Models: Methods, Analysis & Insights from Training Gopher><Sumary>Language modelling provides a step towards intelligent communication systems
by harnessing large repositories of written human knowledge to better predict
and understand the world. In this paper, we present an analysis of
Transformer-based language model performance across a wide range of model
scales -- from models with tens of millions of parameters up to a 280 billion
parameter model called Gopher. These models are evaluated on 152 diverse tasks,
achieving state-of-the-art performance across the majority. Gains from scale
are largest in areas such as reading comprehension, fact-checking, and the
identification of toxic language, but logical and mathematical reasoning see
less benefit. We provide a holistic analysis of the training dataset and
model's behaviour, covering the intersection of model scale with bias and
toxicity. Finally we discuss the application of language models to AI safety
and the mitigation of downstream harms.</Sumary><Content>Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, JeﬀStanway,
Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoﬀrey Irving
Language modelling provides a step towards intelligent communication systems by harnessing large
repositories of written human knowledge to better predict and understand the world. In this paper, we
present an analysis of Transformer-based language model performance across a wide range of model
scales — from models with tens of millions of parameters up to a 280 billion parameter model called
Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across
the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and</Content></Document>
<Document index=6 title=BLOOM: A 176B-Parameter Open-Access Multilingual Language Model><Sumary>Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.</Sumary><Content>In Challenges & Perspectives
in Creating Large Language Models, 2022a. URL https://openreview.net/forum?id=
HRfzInfr8Z9.
Jason Alan Fries, Leon Weber, Natasha Seelam, Gabriel Altay, Debajyoti Datta, Samuele
Garda, Myungsun Kang, Ruisi Su, Wojciech Kusa, Samuel Cahyawijaya, Fabio Barth,
Simon Ott, Matthias Samwald, Stephen Bach, Stella Biderman, Mario S¨
anger, Bo Wang,
Alison Callahan, Daniel León Peri˜
nán, Théo Gigant, Patrick Haller, Jenny Chim,
Jose David Posada, John Michael Giorgi, Karthik Rangasai Sivaraman, Marc Pàmies,
Marianna Nezhurina, Robert Martin, Michael Cullan, Moritz Freidank, Nathan Dahlberg,
Shubhanshu Mishra, Shamik Bose, Nicholas Michio Broad, Yanis Labrak, Shlok S Desh-
mukh, Sid Kiblawi, Ayush Singh, Minh Chien Vu, Trishala Neeraj, Jonas Golde, Al-
bert Villanova del Moral, and Benjamin Beilharz.
BigBio:
A framework for data-
centric biomedical natural language processing.
In Thirty-sixth Conference on Neu-
ral Information Processing Systems Datasets and Benchmarks Track, 2022b.
URL</Content></Document>
<Document index=7 title=Training Compute-Optimal Large Language Models><Sumary>We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher.</Sumary><Content>Training Compute-Optimal Large Language Models
Jordan Hoﬀmann★, Sebastian Borgeaud★, Arthur Mensch★, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,
Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre★
★Equal contributions
We investigate the optimal model size and number of tokens for training a transformer language model
under a given compute budget. We ﬁnd that current large language models are signiﬁcantly under-
trained, a consequence of the recent focus on scaling language models whilst keeping the amount of
training data constant. By training over 400 language models ranging from 70 million to over 16 billion
parameters on 5 to 500 billion tokens, we ﬁnd that for compute-optimal training, the model size and</Content></Document>
<Document index=8 title=Emergent Abilities of Large Language Models><Sumary>Scaling up language models has been shown to predictably improve performance
and sample efficiency on a wide range of downstream tasks. This paper instead
discusses an unpredictable phenomenon that we refer to as emergent abilities of
large language models. We consider an ability to be emergent if it is not
present in smaller models but is present in larger models. Thus, emergent
abilities cannot be predicted simply by extrapolating the performance of
smaller models. The existence of such emergence implies that additional scaling
could further expand the range of capabilities of language models.</Sumary><Content>the following.
Further model scaling. Further scaling up models has so far appeared to increase the capabilities of
language models, and is a straightforward direction for future work. However, simply scaling up language
models is computationally expensive and requires solving substantial hardware challenges, and so other
approaches will likely play a key role in the future of the emergent abilities of large language models.
Improved model architectures and training. Improving model architecture and training procedures
may facilitate high-quality models with emergent abilities while mitigating computational cost. One direction
is using sparse mixture-of-experts architectures (Lepikhin et al., 2021; Fedus et al., 2021; Artetxe et al.,
2021; Zoph et al., 2022), which scale up the number of parameters in a model while maintaining constant
computational costs for an input. Other directions for better computational eﬃciency could involve variable</Content></Document>
<Document index=9 title=On the Opportunities and Risks of Foundation Models><Sumary>AI is undergoing a paradigm shift with the rise of models (e.g., BERT,
DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a
wide range of downstream tasks. We call these models foundation models to
underscore their critically central yet incomplete character. This report
provides a thorough account of the opportunities and risks of foundation
models, ranging from their capabilities (e.g., language, vision, robotics,
reasoning, human interaction) and technical principles(e.g., model
architectures, training procedures, data, systems, security, evaluation,
theory) to their applications (e.g., law, healthcare, education) and societal
impact (e.g., inequity, misuse, economic and environmental impact, legal and
ethical considerations). Though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent
capabilities,and their effectiveness across so many tasks incentivizes
homogenization. Homogenization provides powerful leverage but demands caution,
as the defects of the foundation model are inherited by all the adapted models
downstream. Despite the impending widespread deployment of foundation models,
we currently lack a clear understanding of how they work, when they fail, and
what they are even capable of due to their emergent properties. To tackle these
questions, we believe much of the critical research on foundation models will
require deep interdisciplinary collaboration commensurate with their
fundamentally sociotechnical nature.</Sumary><Content>models hold promise for expanding NLP to encompass more linguistic diversity. It remains an open
research question to understand whether it is possible to make foundation models that robustly and
equitably represent language with both its major and subtle variations, giving equal weight and
acuity to what makes each linguistic variety distinct [research posing and addressing this question
includes Ponti et al. 2019; Bender 2011; Joshi et al. 2020].
Following the success of foundation models for English, multilingual foundation models have
been released to extend that success to non-English languages. For most of the over 6,000 languages
in the world, the text data available is not enough to train a large-scale foundation model. To give
one example, there are over 65 million speakers of Fula, a West African language, but few if any
resources available for NLP in Fula [Nguer et al. 2020]. Multilingual foundation models address this</Content></Document>
<Document index=10 title=Holistic Evaluation of Language Models><Sumary>Language models (LMs) are becoming the foundation for almost all major
language technologies, but their capabilities, limitations, and risks are not
well understood. We present Holistic Evaluation of Language Models (HELM) to
improve the transparency of language models. First, we taxonomize the vast
space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)
that are of interest for LMs. Then we select a broad subset based on coverage
and feasibility, noting what's missing or underrepresented (e.g. question
answering for neglected English dialects, metrics for trustworthiness). Second,
we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,
robustness, fairness, bias, toxicity, and efficiency) for each of 16 core
scenarios when possible (87.5% of the time). This ensures metrics beyond
accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We
also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze
specific aspects (e.g. reasoning, disinformation). Third, we conduct a
large-scale evaluation of 30 prominent language models (spanning open,
limited-access, and closed models) on all 42 scenarios, 21 of which were not
previously used in mainstream LM evaluation. Prior to HELM, models on average
were evaluated on just 17.9% of the core HELM scenarios, with some prominent
models not sharing a single scenario in common. We improve this to 96.0%: now
all 30 models have been densely benchmarked on the same core scenarios and
metrics under standardized conditions. Our evaluation surfaces 25 top-level
findings. For full transparency, we release all raw model prompts and
completions publicly for further analysis, as well as a general modular
toolkit. We intend for HELM to be a living benchmark for the community,
continuously updated with new scenarios, metrics, and models.</Sumary><Content>The rise of language models.
Language modeling has a long-standing tradition of study across human
language processing and computational language processing (Shannon, 1948; Lounsburg, 1954; Goldman-
Eisler, 1958; Baker, 1975b;a; Jelinek, 1976; 1990; Hale, 2001; Levy, 2008; Merity et al., 2018; Radford et al.,
2018; Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022). Language modeling has also been
seen as a grand challenge for AI, most notably in the Hutter Prize and the associated enwiki8 benchmark
on data compression.77 However, in contrast to these prior framings, where language models were viewed as
standalone generative models, the models we study in this work instead are better understood by situating
language models in two broader contexts. First, given the models function as adaptable foundations for the
myriad scenarios they are tested on, we view language models as foundation models in service of building</Content></Document>
<Document index=11 title=Gemini: A Family of Highly Capable Multimodal Models><Sumary>This report introduces a new family of multimodal models, Gemini, that
exhibit remarkable capabilities across image, audio, video, and text
understanding. The Gemini family consists of Ultra, Pro, and Nano sizes,
suitable for applications ranging from complex reasoning tasks to on-device
memory-constrained use-cases. Evaluation on a broad range of benchmarks shows
that our most-capable Gemini Ultra model advances the state of the art in 30 of
32 of these benchmarks - notably being the first model to achieve human-expert
performance on the well-studied exam benchmark MMLU, and improving the state of
the art in every one of the 20 multimodal benchmarks we examined. We believe
that the new capabilities of Gemini models in cross-modal reasoning and
language understanding will enable a wide variety of use cases and we discuss
our approach toward deploying them responsibly to users.</Sumary><Content>Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,
Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.
Xi Chen, Xiao Wang, Soravit Changpinyo, A J Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian
Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver,
Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James
Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme,</Content></Document>
<Document index=12 title=API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs><Sumary>Recent research has demonstrated that Large Language Models (LLMs) can
enhance their capabilities by utilizing external tools. However, three pivotal
questions remain unanswered: (1) How effective are current LLMs in utilizing
tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What
obstacles need to be overcome to leverage tools? To address these questions, we
introduce API-Bank, a groundbreaking benchmark, specifically designed for
tool-augmented LLMs. For the first question, we develop a runnable evaluation
system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753
API calls to assess the existing LLMs' capabilities in planning, retrieving,
and calling APIs. For the second question, we construct a comprehensive
training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000
distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM
initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits
improved tool utilization compared to GPT-3, while GPT-4 excels in planning.
However, there is still significant potential for further improvement.
Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26
pts and approaches the effectiveness of GPT-3.5. Through error analysis, we
highlight the key challenges for future research in this field to answer the
third question.</Sumary><Content>plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021.
Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 320–335.
Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting
Hu. 2023.
Toolkengpt: Augmenting frozen lan-
guage models with massive tools via tool embeddings.
arXiv preprint arXiv:2305.11554.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022. Few-shot learning with re-
trieval augmented language models. arXiv preprint
arXiv:2208.03299.
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,
Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,</Content></Document>
<Document index=13 title=Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks><Sumary>Recently, there has been significant progress in teaching language models to
perform step-by-step reasoning to solve complex numerical reasoning tasks.
Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these
tasks. CoT uses language models to perform both reasoning and computation in
the multi-step `thought' process. To disentangle computation from reasoning, we
propose `Program of Thoughts' (PoT), which uses language models (mainly Codex)
to express the reasoning process as a program. The computation is relegated to
an external computer, which executes the generated programs to derive the
answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,
TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)
for both few-shot and zero-shot setups. Under both few-shot and zero-shot
settings, PoT can show an average performance gain over CoT by around 12\%
across all the evaluated datasets. By combining PoT with self-consistency
decoding, we can achieve SoTA performance on all math problem datasets and
near-SoTA performance on financial datasets. All of our data and code are
released in Github https://github.com/wenhuchen/Program-of-Thoughts</Sumary><Content>Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357–2367, 2019.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv
preprint arXiv:2108.07732, 2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021a.
Wenhu Chen. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710,
2022.</Content></Document>
<Document index=14 title=Scaling Relationship on Learning Mathematical Reasoning with Large Language Models><Sumary>Mathematical reasoning is a challenging task for large language models
(LLMs), while the scaling relationship of it with respect to LLM capacity is
under-explored. In this paper, we investigate how the pre-training loss,
supervised data amount, and augmented data amount influence the reasoning
performances of a supervised LLM. We find that pre-training loss is a better
indicator of the model's performance than the model's parameter count. We apply
supervised fine-tuning (SFT) with different amounts of supervised data and
empirically find a log-linear relation between data amount and model
performance, and we find better models improve less with enlarged supervised
datasets. To augment more data samples for improving model performances without
any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT
uses supervised models to generate and collect correct reasoning paths as
augmented fine-tuning datasets. We find with augmented samples containing more
distinct reasoning paths, RFT improves mathematical reasoning performance more
for LLMs. We also find RFT brings more improvement for less performant LLMs.
Furthermore, we combine rejection samples from multiple models which push
LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised
fine-tuning (SFT) accuracy of 35.9\% significantly.</Sumary><Content>smaller language models. In Findings of the Association for Computational Linguistics: ACL
2023, pp. 7059–7073, Toronto, Canada, July 2023. Association for Computational Linguistics.
URL https://aclanthology.org/2023.findings-acl.441.
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.
Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.
InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities.
https://github.com/InternLM/InternLM, 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´
ee
Lacroix, Baptiste Rozi`
ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-</Content></Document>
<Document index=15 title=BLOOM: A 176B-Parameter Open-Access Multilingual Language Model><Sumary>Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.</Sumary><Content>3.2 Model Architecture
This section discusses our design methodology and the architecture of the BLOOM model.
In-depth studies and experiments can be found in Le Scao et al. (2022) and Wang et al.
(2022a). We first review our design methodology, then motivate our choice of training a
causal decoder-only model. Finally, we justify the ways that our model architecture deviates
from standard practice.
3.2.1 Design Methodology
The design space of possible architectures is immense, making exhaustive exploration impos-
sible. One option would be to exactly replicate the architecture of an existing large language
model. On the other hand, a great deal of work on improving existing architectures has
seen relatively little adoption (Narang et al., 2021); adopting some of these recommended
practices could yield a significantly better model. We take a middle ground and focus on
model families that have been shown to scale well, and that have reasonable support in</Content></Document>
    </context>

Lembre-se de que o objetivo de cada resumo é servir como um guia de estudo para um Cientista de Dados especialista em AI, Estatística e Deep Learning, com conhecimentos avançados em tecnologia e programação.

!!! Expressões matemáticas usando $ ao invés de \( e \), e $$ ao invés de \[ e \] !!!
!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \[ e \( !!!

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!

<template>

Crie um resumo avançado, aprofundado e elaborado sobre X (mínimo de 8 páginas, extenso, não poupe detalhes, aprofunde-se em conceitos técnicos e matemáticos)

**X =** 

Utilize a formatação abaixo como inspiração para o resumo, mas faça as adaptações necessárias com o objetivo de criar o melhor resumo possível. Lembre-se de que o objetivo é servir como um guia de estudo para um Cientista de Dados especialista em AI, Estatística e Deep Learning, com conhecimentos avançados em tecnologia e programação.

Orientações para escrever o resumo:

**Organização e Estrutura**: Garanta que cada seção do resumo esteja bem organizada e siga uma lógica clara. Utilize títulos e subtítulos para facilitar a navegação. Crie uma estrutura hierárquica coerente, com uma introdução, desenvolvimento e conclusão bem definidos.

**Detalhamento**: Aprofunde-se nos conceitos técnicos e matemáticos, fornecendo explicações detalhadas, exemplos práticos e demonstrações passo a passo quando necessário.

**Destaques**: Sempre que mencionar os conceitos principais no texto, utilize **negrito** para destacá-los. Quando quiser inserir uma citação importante ou parafrasear alguém, utilize *itálico*. Utilize caixas de destaque, como notas, avisos e dicas, para enfatizar informações cruciais.

**Estilo e tom:** Escreva de forma acadêmica e formal, mas use emojis quando necessário para dar destaque a alguma informação, por exemplo, ao destacar um tópico usando blockquotes. Utilize emojis como ⚠️❗✔️💡 e outros que façam sentido dado o conteúdo. Mantenha um tom instrutivo e explicativo ao longo do texto.

Template para o resumo:

## Título do Resumo (seja breve)

Inicie com uma introdução concisa, porém abrangente, que contextualize a importância do tema.

### Principais Conceitos

| Conceito       | Explicação                                                   |
| -------------- | ------------------------------------------------------------ |
| **Conceito 1** | Forneça uma explicação concisa do conceito, explorando as bases teóricas e suas aplicações práticas. |
| **Conceito 2** | Forneça uma explicação concisa do conceito, explorando as bases teóricas e suas aplicações práticas. |

Utilize as formatações abaixo como exemplo para destacar informações importantes e críticas:

> ⚠️ **Nota Importante**: Use esta formatação para destacar informações críticas ou observações que não podem ser ignoradas, assegurando que se destaquem no contexto do resumo.

> ❗ **Ponto de Atenção**: Use esta formatação para destacar informações críticas ou observações que requerem maior atenção ao implementar, pois colocam em risco o uso correto do conceito e devem ser levadas em conta pelo usuário.

> ✔️ **Ponto de Destaque** (técnicos e teóricos): Use esta formatação para destacar informações críticas ou observações teóricas ou técnicas que impactam de forma positiva na compreensão do fenômeno, como resultados importantes que não podem ser ignorados.

### Abstract

Copie o abstract ou sumario consolidado dos documentos usados no resumo nessa de forma que o leitor tenha essa referência em mãos quando for estudar o artigo.

### [Explicação de algum tópico ou conceito]

Elabore de forma aprofundada sobre os tópicos e conceitos do tema X, de modo que o resumo seja avançado, detalhado, bem escrito e cumpra os objetivos do texto. Não poupe detalhes!

Quando for contrastar, comparar, etc., informações, use a formatação de lista de tópicos como no exemplo:

#### 👍Vantagens

* Vantagem 1: explicação detalhada e concisa do ponto de vantagem (exemplo)
* Vantagem 2: explicação detalhada e concisa do ponto de vantagem (exemplo)

#### 👎Desvantagens

* Desvantagem 1: explicação detalhada e concisa do ponto de desvantagem (exemplo)
* Desvantagem 2: explicação detalhada e concisa do ponto de desvantagem (exemplo)

Ou de tabela, dependendo de qual melhor se ajustar ao conteúdo:

| 👍 Vantagens                                                  | 👎 Desvantagens                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Explicação detalhada e concisa do ponto de vantagem (exemplo) | Explicação detalhada e concisa do ponto de desvantagem (exemplo) |
| Explicação detalhada e concisa do ponto de vantagem (exemplo) | Explicação detalhada e concisa do ponto de desvantagem (exemplo) |

Use esse exemplo apenas como inspiração e utilize esses tipos de formatação de acordo com a necessidade de elaborar sobre algum ponto tópico do tema.

### [Explicação de algum tópico ou conceito teórico]

Apresente definições matemáticas e teóricas detalhadas, sem economizar em complexidade. Use a seguinte formatação para equações importantes, garantindo que sejam facilmente legíveis e centralizadas. Por exemplo:

O Teorema de Bayes é um resultado fundamental na teoria da probabilidade que descreve como atualizar as probabilidades de uma hipótese com base em novas evidências. Ele estabelece uma relação entre as probabilidades condicionais de dois eventos.

Seja $A$ e $B$ dois eventos, o Teorema de Bayes afirma que:

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

onde:

- $P(A|B)$ é a probabilidade condicional de $A$ dado $B$, também conhecida como probabilidade a posteriori.
- $P(B|A)$ é a probabilidade condicional de $B$ dado $A$, também conhecida como verossimilhança.
- $P(A)$ é a probabilidade a priori de $A$.
- $P(B)$ é a probabilidade marginal de $B$, que atua como uma constante normalizadora.

A probabilidade marginal $P(B)$ pode ser calculada usando a lei da probabilidade total:

$$
P(B) = P(B|A)P(A) + P(B|\neg A)P(\neg A)
$$

onde $\neg A$ denota o evento complementar de $A$.

Prossiga com uma discussão detalhada para explicar o funcionamento da equação e suas implicações do conceito. Faça uma análise de seu comportamento matemático, se possível. Por exemplo:

O Teorema de Bayes permite atualizar nossas crenças (probabilidades) sobre uma hipótese $A$ após observar novas evidências $B$. Ele combina nossa crença prévia em $A$ (probabilidade a priori) com a probabilidade de observar $B$ dado que $A$ é verdadeiro (verossimilhança) para obter nossa crença atualizada em $A$ dado $B$ (probabilidade a posteriori).

> ✔️ **Ponto de Destaque**: O Teorema de Bayes fornece uma estrutura matemática para o raciocínio probabilístico e a atualização de crenças com base em novas informações. Ele é amplamente aplicado em áreas como aprendizado de máquina, estatística, ciência da computação e tomada de decisão.

!!! Expressões matemáticas usando $ ao invés de \( e \), e $$ ao invés de \[ e \] !!!
!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \[ e \( !!!

### [Explicação de algum tópico ou conceito técnico]

Coloque aqui informações relevantes e concisas para explicar a aplicação do tópico e como implementá-lo. Quando houver necessidade de mostrar um código na linguagem apropriada, use a formatação:

```python
import lib # assuma que as dependências já estão instaladas

# Comentário para elucidar apenas aspectos importantes
def minha_funcao(param):
	return lib.outra_funcao(param)
```

Mantenha os snippets claros, concisos e o menor possível, com foco na funcionalidade principal. Não adicione códigos de setup como pip install, downloads, etc.

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!

### [Aplicações|Trabalhos futuros|Extensões|etc]

Se houver necessidade de falar sobre aplicações do conceito, trabalhos e pesquisas futuras, áreas de interesse e extensões do conceito, use o seguinte formato:

| Conceito       | Explicação                                                   |
| -------------- | ------------------------------------------------------------ |
| **Conceito 1** | Explicação detalhada do conceito, incluindo exemplos práticos e aplicações. |
| **Conceito 2** | Explicação detalhada do conceito, incluindo exemplos práticos e aplicações. |

### [Tópicos Relacionados]

Para orientar o usuário desse guia, crie uma lista de próximos tópicos avançados relacionados, quando houver necessidade:

- [ ] Tópico relacionado 1
- [ ] Tópico relacionado 2
- [ ] etc.

### Conclusão

Resuma todos os tópicos apresentados em uma conclusão sucinta e objetiva.

### Referências

Adicione aqui as referências da seguinte forma:

[1] Attention is All You Need
[2] Other paper name
[3] Etc

!!! Lembre-se de que esse template é apenas um guia e você deve apenas se inspirar nele, sem a necessidade de replicar a mesma estrutura ao pé da letra. Foque no objetivo !!!

!!! NÃO POUPE DETALHES, SEJA O MAIS APROFUNDADO POSSÍVEL !!!

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!

!!! Expressões matemáticas usando $ ao invés de \( e \), e $$ ao invés de \[ e \] !!!
!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \[ e \( !!!

</template>

Diretrizes para o resumo:
Os resumos devem ser avançados;
Os resumos devem ser baseados nos principais aspectos do conceito abordado no texto, como técnicas ou funcionalidades específicas demonstradas em cada subcapítulo;
O resumo deve conter todas principais informações presentes no texto sem omitir nenhum dado importante, com foco especial em não pular nenhum conceitos, resultados importante, argumentos, etc;
O resumo deve conter as equações apresentadas, tabelas e outras informações críticas para um entendimento aprofundando e avançado do conteúdo;
O resumo deve ser escrito de uma maneira acadêmica, do not repeat text.
Você deve usar o <context> da melhor maneira possível para responder a query do usuário e escrever o resumo segundo as diretrizes;
You must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself;
Você não deve pedir para o usuário abrir um link ou visitar um site para ver a resposta. Você deve responder você mesmo;
You have to cite the answer using [number] notation. The number is the idx on the documents. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.
Place these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].
However you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) used to generate that part of the answer.
Coloque os resultados um texto coerente ao invés de apenas listar em tópicos, também foque em usar as formatações mostradas no template.

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!


X = Large Language Models as World Models",,,2024-07-04 19:47:52.940228
How chain of thought is used to improve reasoning in large language models,large-language-models,5,"'You are Perplexica, an AI model who is expert at searching the web and answering user's queries.

Generate a response that is informative and relevant to the user's query based on provided context (the context consits of search results containg a brief description of the content of that page).
You must use this context to answer the user's query in the best way possible. Use an unbaised and journalistic tone in your response. Do not repeat the text.
You must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself. If the user asks for links you can provide them.
Your responses should be long in length be informative and relevant to the user's query. You can use markdowns to format your response. You should use the template provided below in \`template\` section. Make sure the answer is not short and is informative.
You have to cite the answer using [number] notation. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.
Place these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].
However you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) used to generate that part of the answer.



Com base nos documentos em <context> retornados por um sistema de buscas em artigos científicos armazenados no ArXiv, use o <template> fornecido abaixo para criar um resumo abrangente que contenha cada um aplicando o template passando o nome do capítulo ou principal conceito explorado nele como o **X = ** do <template>. 

Diretrizes para o resumo:
Os resumos devem ser avançados;
Os resumos devem ser baseados nos principais aspectos do conceito abordado no texto, como técnicas ou funcionalidades específicas demonstradas em cada subcapítulo;
O resumo deve conter todas principais informações presentes no texto sem omitir nenhum dado importante, com foco especial em não pular nenhum conceitos, resultados importante, argumentos, etc;
O resumo deve conter as equações apresentadas, tabelas e outras informações críticas para um entendimento aprofundando e avançado do conteúdo;
O resumo deve ser escrito de uma maneira acadêmica, do not repeat text.
Você deve usar o <context> da melhor maneira possível para responder a query do usuário e escrever o resumo segundo as diretrizes;
You must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself;
Você não deve pedir para o usuário abrir um link ou visitar um site para ver a resposta. Você deve responder você mesmo;
You have to cite the answer using [number] notation. The number is the idx on the documents. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.
Place these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].
However you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) called index used to generate that part of the answer.
Coloque os resultados um texto coerente ao invés de apenas listar em tópicos, também foque em usar as formatações mostradas no template.

Aything inside the following \`context\` HTML block provided below is for your knowledge returned by the search engine and is not shared by the user. You have to answer question on the basis of it and cite the relevant information from it but you do not have to talk about the context in your response. 

<context>
    <Document index=0><Published>2023-01-10</Published><Title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</Title><Authors>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou</Authors><Summary>We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.</Summary><Source>https://arxiv.org/pdf/2201.11903.pdf</Source><Content>Chain-of-Thought Prompting Elicits Reasoning
in Large Language Models
Jason Wei
Xuezhi Wang
Dale Schuurmans
Maarten Bosma
Brian Ichter
Fei Xia
Ed H. Chi
Quoc V. Le
Denny Zhou
Google Research, Brain Team
{jasonwei,dennyzhou}@google.com
Abstract
We explore how generating a chain of thought—a series of intermediate reasoning
steps—signiﬁcantly improves the ability of large language models to perform
complex reasoning. In particular, we show how such reasoning abilities emerge
naturally in sufﬁciently large language models via a simple method called chain-of-
thought prompting, where a few chain of thought demonstrations are provided as
exemplars in prompting.
Experiments on three large language models show that chain-of-thought prompting
improves performance on a range of arithmetic, commonsense, and symbolic
reasoning tasks. The empirical gains can be striking. For instance, prompting a
PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art
accuracy on the GSM8K benchmark of math word problems, surpassing even
ﬁnetuned GPT-3 with a veriﬁer.
A: The cafeteria had 23 apples originally. They used 
20 to make lunch. So they had 23 - 20 = 3. They 
bought 6 more apples, so they have 3 + 6 = 9. The 
answer is 9.
Chain-of-Thought Prompting
Q: Roger has 5 tennis balls. He buys 2 more cans of 
tennis balls. Each can has 3 tennis balls. How many 
tennis balls does he have now?
A: The answer is 11.
Q: The cafeteria had 23 apples. If they used 20 to 
make lunch and bought 6 more, how many apples 
do they have?
A: The answer is 27.
Standard Prompting
Q: Roger has 5 tennis balls. He buys 2 more cans of 
tennis balls. Each can has 3 tennis balls. How many 
tennis balls does he have now?
A: Roger started with 5 balls. 2 cans of 3 tennis balls 
each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Q: The cafeteria had 23 apples. If they used 20 to 
make lunch and bought 6 more, how many apples 
do they have?
Model Input
Model Output
Model Output
Model Input
Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,
commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.
36th Conference on Neural Information Processing Systems (NeurIPS 2022).
arXiv:2201.11903v6  [cs.CL]  10 Jan 2023
1
Introduction
Math Word Problems (GSM8K)
0
20
40
60
80
100
33
55
18
57
Solve rate (%)
Finetuned GPT-3 175B
Prior best
PaLM 540B: standard prompting
PaLM 540B: chain-of-thought prompting
Figure 2:
PaLM 540B uses chain-of-
thought prompting to achieve new state-
of-the-art performance on the GSM8K
benchmark of math word problems.
Finetuned GPT-3 and prior best are from
Cobbe et al. (2021).
The NLP landscape has recently been revolutionized by
language models (Peters et al., 2018; Devlin et al., 2019;
Brown et al., 2020, inter alia). Scaling up the size of lan-
guage models has been shown to confer a range of beneﬁts,
such as improved performance and sample efﬁciency (Ka-
plan et al., 2020; Brown et al., 2020, inter alia). However,
scaling up model size alone has not proved sufﬁcient for
achieving high performance on challenging tasks such as
arithmetic, commonsense, and symbolic reasoning (Rae
et al., 2021).
This work explores how the reasoning ability of large
language models can be unlocked by a simple method
motivated by two ideas. First, techniques for arithmetic
reasoning can beneﬁt from generating natural language
rationales that lead to the ﬁnal answer. Prior work has
given models the ability to generate natural language inter-
mediate steps by training from scratch (Ling et al., 2017)
or ﬁnetuning a pretrained model (Cobbe et al., 2021), in
addition to neuro-symbolic methods that use formal lan-
guages instead of natural language (Roy and Roth, 2015;
Chiang and Chen, 2019; Amini et al., 2019; Chen et al.,
2019). Second, large language models offer the exciting
prospect of in-context few-shot learning via prompting. That is, instead of ﬁnetuning a separate
language model checkpoint for each new task, one can simply “prompt” the model with a few
input–output exemplars demonstrating the task. Remarkably, this has been successful for a range of
simple question-answering tasks (Brown et al., 2020).
Both of the above ideas, however, have key limitations. For rationale-augmented training and
ﬁnetuning methods, it is costly to create a large set of high quality rationales, which is much more
complicated than simple input–output pairs used in normal machine learning. For the traditional few-
shot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning
abilities, and often does not improve substantially with increasing language model scale (Rae et al.,
2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.
Speciﬁcally, we explore the ability of language models to perform few-shot prompting for reasoning
tasks, given a prompt that consists of triples: ⟨input, chain of thought, output⟩. A chain of thought is
a series of intermediate natural language reasoning steps that lead to the ﬁnal output, and we refer to
this approach as chain-of-thought prompting. An example prompt is shown in Figure 1.
We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks,
showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking
degree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems
(Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting
by a large margin and achieves new state-of-the-art performance. A prompting only approach is
important because it does not require a large training dataset and because a single model checkpoint
can perform many tasks without loss of generality. This work underscores how large language models
can learn via a few examples with natural language data about the task (c.f. automatically learning
the patterns underlying inputs and outputs via a large training dataset).
2
Chain-of-Thought Prompting
Consider one’s own thought process when solving a complicated reasoning task such as a multi-step
math word problem. It is typical to decompose the problem into intermediate steps and solve each
before giving the ﬁnal answer: “After Jane gives 2 ﬂowers to her mom she has 10 . . . then after she
gives 3 to her dad she will have 7 . . . so the answer is 7.” The goal of this paper is to endow language
models with the ability to generate a similar chain of thought—a coherent series of intermediate
reasoning steps that lead to the ﬁnal answer for a problem. We will show that sufﬁciently large
2
language models can generate chains of thought if demonstrations of chain-of-thought reasoning are
provided in the exemplars for few-shot prompting.
Figure 1 shows an example of a model producing a chain of thought to solve a math word problem
that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution
and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it
mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations
typically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,
2022, inter alia)).
Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning
in language models.
1. First, chain of thought, in principle, allows models to decompose multi-step problems into
intermediate steps, which means that additional computation can be allocated to problems
that require more reasoning steps.
2. Second, a chain of thought provides an interpretable window into the behavior of the model,
suggesting how it might have arrived at a particular answer and providing opportunities
to debug where the reasoning path went wrong (although fully characterizing a model’s
computations that support an answer remains an open question).
3. Third, chain-of-thought reasoning can be used for tasks such as math word problems,
commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least
in principle) to any task that humans can solve via language.
4. Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf
language models simply by including examples of chain of thought sequences into the
exemplars of few-shot prompting.
In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic
reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).
3
Arithmetic Reasoning
We begin by considering math word problems of the form in Figure 1, which measure the arithmetic
reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where
language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia). Strikingly, chain-
of-thought prompting when used with the 540B parameter language model performs comparably with
task-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging
GSM8K benchmark (Cobbe et al., 2021).
3.1
Experimental Setup
We explore chain-of-thought prompting for various language models on multiple benchmarks.
Benchmarks. We consider the following ﬁve math word problem benchmarks: (1) the GSM8K
benchmark of math word problems (Cobbe et al., 2021), (2) the SVAMP dataset of math word
problems with varying structures (Patel et al., 2021), (3) the ASDiv dataset of diverse math word
problems (Miao et al., 2020), (4) the AQuA dataset of algebraic word problems, and (5) the MAWPS
benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.
Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by
Brown et al. (2020), in which a language model is given in-context exemplars of input–output pairs
before outputting a prediction for a test-time example. Exemplars are formatted as questions and
answers. The model gives the answer directly, as shown in Figure 1 (left).
Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot
prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most
of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars
with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the
full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo
prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether
chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of
3
Q: Roger has 5 tennis balls. He buys 
2 more cans of tennis balls. Each can 
has 3 tennis balls. How many tennis 
balls does he have now?
A: Roger started with 5 balls. 2 cans 
of 3 tennis balls each is 6 tennis 
balls. 5 + 6 = 11. The answer is 11.
Q: Sammy wanted to go to where the 
people were. Where might he go?
Options: (a) race track (b) populated areas 
(c) desert (d) apartment (e) roadblock
A: The answer must be a place with a 
lot of people. Race tracks, desert, 
apartments, and roadblocks don't 
have a lot of people, but populated 
areas do. So the answer is (b). 
Q: Yes or no: Would a pear sink in 
water?
A: The density of a pear is about 0.6 
g/cm^3, which is less than water. 
Thus, a pear would float. So the 
answer is no.
Q: The concert was scheduled to be 
on 06/01/1943, but was delayed by 
one day to today. What is the date 10 
days ago in MM/DD/YYYY? 
A: One day after 06/01/1943 is 
06/02/1943, so today is 06/02/1943. 
10 days before today is 05/23/1943. 
So the answer is 05/23/1943. 
Q: Is the following sentence 
plausible? ""Joao Moutinho caught the 
screen pass in the NFC 
championship."" 
A: Joao Moutinho is a soccer player. 
The NFC championship is part of 
American football, not soccer. So the 
answer is no.
Q: Take the last letters of the words 
in “Lady Gaga” and concatenate 
them.
A: The last letter of “Lady” is “y”. The 
last letter of “Gaga” is “a”. 
Concatenating them is “ya”. So the 
answer is ya.
Q: A coin is heads up. Maybelle flips 
the coin. Shalonda does not flip the 
coin. Is the coin still heads up?
A: The coin was flipped by Maybelle. 
So the coin was flipped 1 time, which 
is an odd number. The coin started 
heads up, so after an odd number of 
flips, it will be tails up. So the answer 
is no.
Math Word Problems (free response)
Math Word Problems (multiple choice)
CSQA (commonsense)
StrategyQA
Date Understanding
Sports Understanding
Last Letter Concatenation
Coin Flip (state tracking)
Q: How many keystrokes are needed 
to type the numbers from 1 to 500?
Answer Choices: (a) 1156 (b) 1392 (c) 1480 
(d) 1562 (e) 1788
A: There are 9 one-digit numbers 
from 1 to 9. There are 90 two-digit 
numbers from 10 to 99. There are 
401 three-digit numbers from 100 to 
500. 9 + 90(2) + 401(3) = 1392. The 
answer is (b).
SayCan (Instructing a robot)
Human: How would you bring me 
something that isn’t a fruit?
Explanation: the user wants 
something to eat that isn’t a fruit. An 
energy bar is not a fruit, so I will bring 
the user an energy bar. 
Plan: 1. find(energy bar) 2. 
pick(energy bar) 3. find(user) 4. 
put(energy bar) 5. done().
Figure 3: Examples of ⟨input, chain of thought, output⟩triples for arithmetic, commonsense, and
symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.
math word problems, we used this single set of eight chain of thought exemplars for all benchmarks
except AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars
and solutions from the training set, as given in Appendix Table 21.
Language models. We evaluate ﬁve large language models. The ﬁrst is GPT-3 (Brown et al.,
2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which
presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang
et al., 2022).The second is LaMDA (Thoppilan et al., 2022), which has models of 422M, 2B, 8B,
68B, and 137B parameters. The third is PaLM, which has models of 8B, 62B, and 540B parameters.
The fourth is UL2 20B (Tay et al., 2022), and the ﬁfth is Codex (Chen et al., 2021, code-davinci-002
in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows
chain-of-thought prompting can be improved by taking the majority ﬁnal answer over many sampled
generations (Wang et al., 2022a)). For LaMDA, we report averaged results over ﬁve random seeds,
where each seed had a different randomly shufﬂed order of exemplars. As LaMDA experiments
did not show large variance among different seeds, to save compute we report results for a single
exemplar order for all other models.
3.2
Results
The strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental
outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix.
There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent
ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively
impact performance for small models, and only yields performance gains when used with models of
∼100B parameters. We qualitatively found that models of smaller scale produced ﬂuent but illogical
chains of thought, leading to lower performance than standard prompting.
4
0
20
40
60
GSM8K
solve rate (%)
LaMDA
GPT
PaLM
Standard prompting
Chain-of-thought prompting
Prior supervised best
0
20
40
60
80
SVAMP
solve rate (%)
0.4
8 137
0
25
50
75
100
MAWPS
solve rate (%)
0.4
7
175
8
62 540
Model scale (# parameters in billions)
Figure 4:
Chain-of-thought prompting enables
large language models to solve challenging math
problems. Notably, chain-of-thought reasoning
is an emergent ability of increasing model scale.
Prior best numbers are from Cobbe et al. (2021)
for GSM8K, Jie et al. (2022) for SVAMP, and Lan
et al. (2021) for MAWPS.
Second, chain-of-thought prompting has larger
performance gains for more-complicated prob-
lems. For instance, for GSM8K (the dataset
with the lowest baseline performance), perfor-
mance more than doubled for the largest GPT
and PaLM models. On the other hand, for Sin-
gleOp, the easiest subset of MAWPS which only
requires a single step to solve, performance im-
provements were either negative or very small
(see Appendix Table 3).
Third, chain-of-thought prompting via GPT-3
175B and PaLM 540B compares favorably to
prior state of the art, which typically ﬁnetunes a
task-speciﬁc model on a labeled training dataset.
Figure 4 shows how PaLM 540B uses chain-of-
thought prompting to achieve new state of the art
on GSM8K, SVAMP, and MAWPS (though note
that standard prompting already passed the prior
best for SVAMP). On the other two datasets,
AQuA and ASDiv, PaLM with chain-of-thought
prompting reaches within 2% of the state of the
art (Appendix Table 2).
To better understand why chain-of-thought
prompting works, we manually examined model-
generated chains of thought by LaMDA 137B
for GSM8K. Of 50 random examples where the
model returned the correct ﬁnal answer, all of
the generated chains of thought were also log-
ically and mathematically correct except two
that coincidentally arrived at the correct answer
(see Appendix D.1, and Table 8 for examples
of correct model-generated chains of thought).
We also randomly examined 50 random sam-
ples for which the model gave the wrong answer.
The summary of this analysis is that 46% of the
chains of thought were almost correct, barring
minor mistakes (calculator error, symbol map-
ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major
errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into
why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors
made by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary
is that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding
errors in the 62B model (see Appendix A.1).
3.3
Ablation Study
The observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the
same performance improvements can be conferred via other types of prompting. Figure 5 shows an
ablation study with three variations of chain of thought described below.
Equation only. One reason for why chain-of-thought prompting might help is that it produces the
mathematical equation to be evaluated, and so we test a variation where the model is prompted
to output only a mathematical equation before giving the answer. Figure 5 shows that equation
only prompting does not help much for GSM8K, which implies that the semantics of the questions
in GSM8K are too challenging to directly translate into an equation without the natural language
reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd
that equation only prompting does improve performance, since the equation can be easily derived
from the question (see Appendix Table 6).
5
LaMDA
PaLM
0
20
40
60
GSM8K solve rate (%)
Standard prompting
Equation only
Variable compute only
Reasoning after answer
Chain-of-thought prompting
Figure 5:
Ablation study for dif-
ferent variations of prompting us-
ing LaMDA 137B and PaLM 540B.
Results for other datasets are given
in Appendix Table 6 and Table 7.
Variable compute only. Another intuition is that chain of
thought allows the model to spend more computation (i.e.,
intermediate tokens) on harder problems. To isolate the effect
of variable computation from chain-of-thought reasoning, we
test a conﬁguration where the model is prompted to output a
only sequence of dots (. . .) equal to the number of characters in
the equation needed to solve the problem. This variant performs
about the same as the baseline, which suggests that variable
computation by itself is not the reason for the success of chain-
of-thought prompting, and that there appears to be utility from
expressing intermediate steps via natural language.
Chain of thought after answer. Another potential beneﬁt of
chain-of-thought prompting could simply be that such prompts
allow the model to better access relevant knowledge acquired
during pretraining. Therefore, we test an alternative conﬁgura-
tion where the chain of thought prompt is only given after the
answer, isolating whether the model actually depends on the
produced chain of thought to give the ﬁnal answer. This variant
performs about the same as the baseline, which suggests that
the sequential reasoning embodied in the chain of thought is
useful for reasons beyond just activating knowledge.
3.4
Robustness of Chain of Thought
GSM8K
0
5
10
15
20
Solve rate (%)
Standard prompting
Chain-of-thought prompting
· different annotator (B)
· different annotator (C)
· intentionally concise style
· exemplars from GSM8K (α)
· exemplars from GSM8K (β)
· exemplars from GSM8K (γ)
MAWPS
0
20
40
60
Figure 6: Chain-of-thought prompting
has variance for different prompt exam-
ples (as expected) but outperforms stan-
dard prompting for various annotators as
well as for different exemplars.
Sensitivity to exemplars is a key consideration of prompt-
ing approaches—for instance, varying the permutation of
few-shot exemplars can cause the accuracy of GPT-3 on
SST-2 to range from near chance (54.3%) to near state of
the art (93.4%) (Zhao et al., 2021). In this ﬁnal subsec-
tion, we evaluate robustness to chains of thought written
by different annotators. In addition to the results above,
which used chains of thought written by an Annotator
A, two other co-authors of this paper (Annotators B and
C) independently wrote chains of thought for the same
few-shot exemplars (shown in Appendix H). Annotator A
also wrote another chain of thought that was more concise
than the original, following the style of solutions given in
Cobbe et al. (2021).1
Figure 6 shows these results for LaMDA 137B on GSM8K
and MAWPS (ablation results for other datasets are given
in Appendix Table 6 / Table 7). Although there is variance
among different chain of thought annotations, as would be
expected when using exemplar-based prompting (Le Scao
and Rush, 2021; Reynolds and McDonell, 2021; Zhao
et al., 2021), all sets of chain of thought prompts outper-
form the standard baseline by a large margin. This result
implies that successful use of chain of thought does not
depend on a particular linguistic style.
To conﬁrm that successful chain-of-thought prompting
works for other sets of exemplars, we also run experiments
with three sets of eight exemplars randomly sampled from the GSM8K training set, an independent
1For instance, whereas original chain of thought uses several short sentences (“’There were originally 9
computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is
29.”), the concise chain of thought would read “5 * 4 = 20 new computers were added. So there are 9 + 20 = 29
new computers in the server room now”.
6
source (examples in this dataset already included reasoning steps like a chain of thought).2 Fig-
ure 6 shows that these prompts performed comparably with our manually written exemplars, also
substantially outperforming standard prompting.
In addition to robustness to annotators, independently-written chains of thought, different exemplars,
and various language models, we also ﬁnd that chain-of-thought prompting for arithmetic reasoning
is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).
4
Commonsense Reasoning
Although chain of thought is particularly suitable for math word problems, the language-based nature
of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems,
which involve reasoning about physical and human interactions under the presumption of general
background knowledge. Commonsense reasoning is key for interacting with the world and is still
beyond the reach of current natural language understanding systems (Talmor et al., 2021).
Benchmarks. We consider ﬁve datasets covering a diverse range of commonsense reasoning types.
The popular CSQA (Talmor et al., 2019) asks commonsense questions about the world involving
complex semantics that often require prior knowledge. StrategyQA (Geva et al., 2021) requires
models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets
from the BIG-bench effort (BIG-bench collaboration, 2021): Date Understanding, which involves
inferring a date from a given context, and Sports Understanding, which involves determining whether
a sentence relating to sports is plausible or implausible. Finally, the SayCan dataset (Ahn et al.,
2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete
set. Figure 3 shows examples with chain of thought annotations for all datasets.
Prompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA,
we randomly selected examples from the training set and manually composed chains of thought for
them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected
the ﬁrst ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on
the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al.
(2022) and also manually composed chains of thought.
Results. Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different
model scales are shown in Table 4). For all tasks, scaling up model size improved the performance
of standard prompting; chain-of-thought prompting led to further gains, with improvements appear-
ing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong
performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs
69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%).
These results demonstrate that chain-of-thought prompting can also improve performance on tasks
requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).
8 62 540
20
40
60
80
100
Solve rate (%)
CSQA
8 62 540
50
60
70
80
90
StrategyQA
Standard prompting
Chain of thought
Prior supervised best
Human
8 62 540
0
20
40
60
80
Model scale (# parameters in billions)
Date
8 62 540
40
60
80
100
Sports
8 62 540
20
40
60
80
100
SayCan
Figure 7:
Chain-of-thought prompting also improves the commonsense reasoning abilities of
language models. The language model shown here is PaLM. Prior best numbers are from the
leaderboards of CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) (single-model only,
as of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown
in Table 4.
2We sample examples ≤60 tokens to ﬁt into our input context window, and also limit the examples to ≤2
steps to solve for a fair comparison with the eight exemplars that we composed.
7
5
Symbolic Reasoning
0
25
50
75
100
Solve rate (%)
Letter Concat: 2
(in domain)
Letter Concat: 4
(OOD)
Standard prompting
Chain-of-thought prompting
8
62
540
40
60
80
100
Solve rate (%)
Coin Flip: 2
(in domain)
8
62
540
Model scale (# parameters in billions)
Coin Flip: 4
(OOD)
Figure 8:
Using chain-of-thought
prompting facilitates generalization to
longer sequences in two symbolic rea-
soning tasks.
Our ﬁnal experimental evaluation considers symbolic rea-
soning, which is simple for humans but potentially chal-
lenging for language models. We show that chain-of-
thought prompting not only enables language models to
perform symbolic reasoning tasks that are challenging in
the standard prompting setting, but also facilitates length
generalization to inference-time inputs longer than those
seen in the few-shot exemplars.
Tasks.
We use the following two toy tasks.
• Last letter concatenation. This task asks the model
to concatenate the last letters of words in a name (e.g.,
“Amy Brown” →“yn”). It is a more challenging version
of ﬁrst letter concatenation, which language models can
already perform without chain of thought.3 We generate
full names by randomly concatenating names from the
top one-thousand ﬁrst and last names from name census
data (https://namecensus.com/).
• Coin ﬂip. This task asks the model to answer whether a
coin is still heads up after people either ﬂip or don’t ﬂip
the coin (e.g., “A coin is heads up. Phoebe ﬂips the coin.
Osvaldo does not ﬂip the coin. Is the coin still heads up?”
→“no”).
As the construction of these symbolic reasoning tasks is
well-deﬁned, for each task we consider an in-domain test
set for which examples had the same number of steps as
the training/few-shot exemplars, as well as an out-of-domain (OOD) test set, for which evaluation
examples had more steps than those in the exemplars. For last letter concatenation, the model only
sees exemplars of names with two words, and then performs last letter concatenation on names with 3
and 4 words.4 We do the same for the number of potential ﬂips in the coin ﬂip task. Our experimental
setup uses the same methods and models as in the prior two sections. We again manually compose
chains of thought for the few-shot exemplars for each task, which are given in Figure 3.
Results.
The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM,
with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting
leads to almost 100% solve rates (note that standard prompting already solves coin ﬂip with PaLM
540, though not for LaMDA 137B). Note that these in-domain evaluations are “toy tasks” in the
sense that perfect solution structures are already provided by the chains of thought in the few-shot
exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time
example. And yet, small models still fail—the ability to perform abstract manipulations on unseen
symbols for these three tasks only arises at the scale of 100B model parameters.
As for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,
language models achieve upward scaling curves (though performance is lower than in the in-domain
setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of
thought for language models of sufﬁcient scale.
6
Discussion
We have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step rea-
soning behavior in large language models. We ﬁrst saw that chain-of-thought prompting improves
performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger
than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,
3We tested 10 common names using GPT-3 davinci and it got all but one correct.
4For names of length longer than 2 words, we concatenate multiple ﬁrst and last names together.
8
experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought
reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,
chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In
all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language
model. No language models were ﬁnetuned in the process of writing this paper.
The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme
(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-
of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting
appears to expand the set of tasks that large language models can perform successfully—in other
words, our work underscores that standard prompting only provides a lower bound on the capabilities
of large language models. This observation likely raises more questions than it answers—for instance,
how much more can we expect reasoning ability to improve with a further increase in model scale?
What other prompting methods might expand the range of tasks that language models can solve?
As for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of
human reasoners, this does not answer whether the neural network is actually “reasoning,” which
we leave as an open question. Second, although the cost of manually augmenting exemplars with
chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for
ﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot
generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct
and incorrect answers; improving factual generations of language models is an open direction for
future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia). Finally,
the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in
real-world applications; further research could explore how to induce reasoning in smaller models.
7
Related Work
This work is inspired by many research areas, which we detail in an extended related work section
(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.
The ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)
pioneer the idea of using natural language rationales to solve math word problems through a series
of intermediate steps. Their work is a remarkable contrast to the literature using formal languages
to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe
et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained
language model rather than training a model from scratch. In the domain of program synthesis,
Nye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via
ﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step
prediction method performs better than directly predicting the ﬁnal outputs.
Naturally, this paper also relates closely to the large body of recent work on prompting. Since the
popularization of few-shot prompting as given by Brown et al. (2020), several general approaches
have improved the prompting ability of models, such as automatically learning prompts (Lester et al.,
2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang
et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,
instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the
outputs of language models with a chain of thought.
8
Conclusions
We have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-
ing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense
reasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows
sufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.
Broadening the range of reasoning tasks that language models can perform will hopefully inspire
further work on language-based approaches to reasoning.
9
Acknowledgements
We thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the
paper. We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton,
and Aakanksha Chowdhery for helpful discussions. We thank Sid Maxwell for notifying us about a
mistake in the manual error analysis in the original manuscript.
References
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I
say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh
Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-
based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), Minneapolis, Minnesota. Association for Computational Linguistics.
Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding
operations and arguments with reading comprehension. EMNLP.
Jacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language. NAACL.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732.
BIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the
capabilities of language models. In preparation.
Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural
language deductions. EMNLP.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language models are few-shot learners. NeurIPS.
Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures
generalize via recursion. ICLR.
Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI:
Natural language inference with natural language explanations. NeurIPS.
Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization
improve robustness? NAACL.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating
large language models trained on code. arXiv preprint arXiv:2107.03374.
Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2019. Neural
symbolic reader: Scalable integration of distributed and symbolic representations for reading
comprehension. ICLR.
Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving
and reasoning math word problems. In Proceedings of the 2019 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 2656–2668, Minneapolis, Minnesota. Association for
Computational Linguistics.
10
Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over
language. IJCAI.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. 2021. Training veriﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language understanding. NAACL.
Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural
logic machines. ICLR.
Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Beneﬁts of intermediate annotations in reading
comprehension. ACL.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did
aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. TACL.
Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models
behind language models. NAACL.
Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher
Ré. 2018. Training classiﬁers with natural language explanations. ACL.
Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework
for understanding the roles of explanation data. ACL.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb categorization. EMNLP.
Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem
solving as complex relation extraction. arXiv preprint arXiv:2203.10316.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016.
MAWPS: A math word problem repository. NAACL.
Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry
Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language
models learn from explanations in context? arXiv preprint arXiv:2204.02329.
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,
and Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math
word problem solvers. arXiv preprint arXiv:2109.00799.
Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? NAACL.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efﬁcient
prompt tuning. EMNLP.
Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles:
From robust processing to precise semantics. Proceedings of the 2nd Workshop on Text Meaning
and Interpretation.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning: Optimizing continuous prompts for generation.
ACL.
11
Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal
reasoning through internal monologue. NAACL.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale
generation: Learning to solve and explain algebraic word problems. ACL.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. arXiv preprint arXiv:2107.13586.
Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley.
2021. Rationale-inspired natural language explanations with commonsense. arXiv preprint
arXiv:2106.13876.
Ana Marasovi´
c, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization
with natural language prompts. NAACL Findings.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and
factuality in abstractive summarization. In ACL.
Shen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and
developing English math word problem solvers. ACL.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke
Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work?
arXiv preprint arXiv:2202.12837.
Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan.
2020.
WT5?!
Training text-to-text models to explain their predictions.
arXiv preprint
arXiv:2004.14546.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work:
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to
follow instructions with human feedback. arXiv preprint arXiv:2203.02155.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve
simple math word problems? NAACL.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and
Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.
Piotr Pi˛
ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving
BERT’s mathematical abilities by predicting the order of reasoning. ACL.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. Journal of Machine Learning Research, 21:1–67.
Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain:
A self-explaining architecture for neural text classiﬁers. EMNLP.
Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain
yourself! Leveraging language models for commonsense reasoning. ACL.
12
Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading
comprehension with numerical reasoning. EMNLP.
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov,
Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language
generation models. arXiv preprint arXiv:2112.12870.
Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration.
arXiv preprint arXiv:2109.02102.
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022.
A recipe for arbitrary text style transfer with large language models. ACL.
Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond
the few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in
Computing Systems.
Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. EMNLP.
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language.
TACL.
Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching
soft rules to pre-trained language models. EMNLP.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted
training enables zero-shot task generalization. ICLR.
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021.
Generate & rank: A multi-task framework for math word problems. In Findings of the Association
for Computational Linguistics: EMNLP 2021.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A
question answering challenge targeting commonsense knowledge. NAACL.
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-
thought: Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS.
Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and
Jonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamiﬁcation.
NeurIPS Track on Datasets and Benchmarks.
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven
Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv
preprint arXiv:2205.05131.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for
dialog applications. arXiv preprint arXiv:2201.08239.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a.
Self-consistency improves chain of thought reasoning in language models.
arXiv preprint
arXiv:2203.11171.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana
Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b.
Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint
arXiv:2204.07705.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. ICLR.
13
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language
models. Transactions on Machine Learning Research.
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing
human-AI collaboration for generating free-text explanations. NAACL.
Sarah Wiegreffe and Ana Marasovi´
c. 2021. Teach me to explain: A review of datasets for explainable
NLP. NeurIPS.
Sarah Wiegreffe, Ana Marasovi´
c, and Noah A. Smith. 2021. Measuring association between labels
and free-text rationales. EMNLP.
Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and
Carrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual
programming. CHI Extended Abstracts.
Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable
human-AI interaction by chaining large language model prompts. CHI.
Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020.
Neural execution engines: Learning to execute subroutines. NeurIPS.
Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Reﬁning language models
with compositional explanations. NeurIPS.
Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning.
arXiv preprint arXiv:2205.03401.
Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot
out-of-domain transfer learning of natural language explanations. arXiv preprint arXiv:2112.06204.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve
machine learning for text categorization. NAACL.
Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615.
Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with
reasoning. arXiv preprint arXiv:2203.14465.
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use:
Improving few-shot performance of language models. ICML.
Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and
Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent
variables. NeurIPS.
14
Checklist
1. For all authors...
(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A.2.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] We don’t
expect negative societal impacts as a direct result of the contributions in our paper. One
consideration, however, is that generated chain of thought is not always factual, which
is noted as a limitation in Appendix D.1 (and note that we do not suggest using such
chains of thought in a factual manner or in any real-world scenario).
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments...
(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] We included
inputs, outputs, and targets for LaMDA and GPT-3 in the supplementary material.
Although we use proprietary models, we GPT-3 results are fully reproducible. Repro-
ducibility is further discussed in Appendix E.1.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] Data splits were speciﬁed, N/A for hyperparams.
(c) Did you report error bars (e.g., with respect to the random seed after running exper-
iments multiple times)? [Yes] Standard deviation for multiple seeds using LaMDA
137B, where each seed is a different random order of exemplars, is given in Table 6
and Table 7.
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] Type of resources are described in
Appendix E.2, though we did not estimate the total amount of compute.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] We used two models
that we anonymized based on the recommendation of the NeurIPS chairs. These models
will be cited in the camera-ready version of the paper.
(b) Did you mention the license of the assets? [Yes] See Appendix E.3.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
The coinﬂip and last letter concatenation datasets are the only new assets, and they are
given in the Supplementary Materials.
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A] No human data collected.
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [N/A] No human data collected.
5. If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
15
A
Frequently Asked Questions
A.1
Why does increasing model scale improve chain-of-thought prompting?
The ﬁnding that successful chain-of-thought reasoning predictably emerges only at certain model
scales is intriguing. Scaling up language models has been shown to confer beneﬁts such as improved
performance and sample efﬁciency (Kaplan et al., 2020), but chain-of-thought reasoning is emergent
in the sense that its success cannot be predicted only by extrapolating the performance of small scale
models, as chain of thought actually hurts performance for most models smaller than 10B parameters.
The question of why model scale improves chain-of-thought prompting is certainly multi-faceted, and
we made a preliminary attempt to shed insight into it via error analysis. This small analysis involved
manually reading 45 errors made by PaLM 62B and categorizing them into semantic understanding
(20 errors), one step missing (18 errors), and other errors (7 errors). The “other category” included
hallucinations, repetitive outputs, and symbol mapping errors. This categorization is a coarse one
borrowed from the initial error analysis done on LaMDA in Appendix D.2, for which categories were
conceived based on what improvements were needed to make the chain of thought correct.
As shown in Figure 9, scaling PaLM to 540B parameters ﬁxed a substantial portion of errors in all
three categories. Examples of semantic understanding and one-step missing errors that were ﬁxed by
scaling PaLM to 540B are given in Figure 10. This result appears consistent with a hypothesis that
language models acquire a range of semantic understanding and logical reasoning skills as a function
of model scale (though note that model scale is often conﬂated with other factors, such as amount of
training compute).
Semantic understanding 
(62B made 20 errors of this type, 
540B fixes 6 of them)
One step missing 
(62B made 18 errors of this type, 
540B fixes 12 of them)
Other 
(62B made 7 errors of this type, 
540B fixes 4 of them)
Types of errors made by 
a 62B language model:
Errors fixed by 
scaling from 
62B to 540B
Figure 9: Error analysis of 45 problems that PaLM 62B got incorrect. These errors were categorized
that semantic understanding, one step missing, and other. The other category includes hallucinations,
repetitive outputs, and symbol mapping errors. Scaling PaLM to 540B ﬁxed a substantial portion of
errors in all categories.
There are also three notable points regarding why small language models fail. The ﬁrst observation
is that small language models fail at even relatively easy symbol mapping tasks. As demonstrated
in Section 5, for even symbolic reasoning tasks that only require generalization to new examples
using the same chain of thought logical structure that was given in the few-shot exemplars, small
language models still failed. The second observation is that small language models seem to have
inherently weaker arithmetic abilities, as shown by Brown et al. (2020), the ability to do simple
arithmetic operations (without semantic understanding) requires sufﬁcient model scale. Finally, we
noticed qualitatively that small language models often did not generate a ﬁnal answer that could be
parsed, due to either repetitions or logic that never arrived at a ﬁnal answer.
In summary, the success of chain-of-thought reasoning as a result of model scale is a complicated
phenomena that likely involves a variety of emergent abilities (semantic understanding, symbol
mapping, staying on topic, arithmetic ability, faithfulness, etc). Future work could more thoroughly
investigate what properties of pretraining data, model architecture, and optimization objective causally
enable such reasoning capabilities.
16
Tracy used a piece of wire 4 
feet long to support tomato 
plants in the garden. The 
wire was cut into pieces 6 
inches long. How many 
pieces did she obtain?
She cut the wire into 6 inch pieces. 
This means she got 4 * 6 = 24 
pieces. The answer is 24. 
(Incorrect: semantic 
understanding error)
62B Model Output
Question
540B Model Output
The wire was 4 feet long. This 
means it was 4 * 12 = 48 inches 
long. It was cut into pieces 6 inches 
long. This means she obtained 48 / 
6 = 8 pieces. The answer is 8. 
(Correct)
Tom's ship can travel at 10 
miles per hour. He is sailing 
from 1 to 4 PM. He then 
travels back at a rate of 6 
mph. How long does it take 
him to get back?
He travels from 1 to 4 PM. That 
means he travels for 3 hours. He 
travels back at 6 mph. That means 
he travels for 3 * 6 = 18 miles. The 
answer is 18. (Incorrect: semantic 
understanding error)
He travels at 10 mph from 1 to 4 
PM. This means he travels 3 hours. 
3 hours at 10 mph means he travels 
3 * 10 = 30 miles. He then travels 
back at 6 mph. This means he 
travels 6 miles per hour. He has to 
travel 30 miles, so it takes him 30 / 6 
= 5 hours. The answer is 5. 
(Correct)
Stephen placed an online 
order for groceries. His final 
bill came to $40.00. 
Because this was through a 
delivery vendor, they tacked 
on a 25% fee to his final 
total and charged him $3.00 
in delivery fees. Stephen 
also added a $4.00 tip. After 
the extra fees, what was the 
final price of Stephen's 
groceries?
The final price of Stephen's 
groceries was $40.00. The delivery 
vendor added 25% to the final total, 
so the final total was 125% of 
$40.00. This means the final total 
was 125% * $40.00 = $50.00. The 
delivery vendor also charged $3.00 
in delivery fees. So the final price of 
Stephen's groceries was $50.00 + 
$3.00 = $53.00. The answer is 
$53.00. (Incorrect: one step 
missing error)
Stephen's bill was $40.00. The 
delivery vendor added 25% to the 
bill. This means the bill was 
increased by 25% of $40.00. This is 
0.25 * $40.00 = $10.00. The delivery 
fee was $3.00. The tip was $4.00. 
So the total bill was $40.00 + $10.00 
+ $3.00 + 540b model $4.00 = 
$57.00. The answer is 57. (Correct)
There are four schools 
competing at a basketball 
tournament. Each school 
has sent a girls’ basketball 
team and a boys’ basketball 
team and each team has 5 
players each. Each school 
has also sent a coach for 
each team. In total, how 
many people have all of the 
schools sent?
There are 4 schools. Each school 
has sent a girls’ basketball team and 
a boys’ basketball team. Each team 
has 5 players each. That means 
there are 4 * 2 * 5 = 40 players. 
Each team has a coach. That 
means there are 40 + 4 = 44 
coaches. The answer is 44. 
(Incorrect: one step missing 
error)
Each school has sent 2 teams, each 
with 5 players. This means each 
school has sent 2 * 5 = 10 players. 
Each school has also sent 2 
coaches. This means each school 
has sent 10 + 2 = 12 people. There 
are 4 schools, so in total all of the 
schools have sent 4 * 12 = 48 
people. The answer is 48. (Correct)
62B Model Output
62B Model Output
62B Model Output
540B Model Output
540B Model Output
540B Model Output
Question
Question
Question
Figure 10:
Examples of semantic understanding and one-step missing errors that were ﬁxed by
scaling PaLM from 62B to 540B.
A.2
What is the role of prompt engineering?
One of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage
of work showing that prompts affect language models in unexpected ways (Min et al., 2022). The
general way that we created chain of thought annotations was by taking eight exemplars from the
training set and decomposing the reasoning process into multiple steps leading to the ﬁnal answer.
Examples of chain of thought annotations are provided in Figure 3, with full prompts given in
Appendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed
robustness experiments with respect to various factors.
• Different annotators. We ﬁrst analyze robustness to three different annotators (Section 3.4 and
Figure 6). Although there is notable variance in performance (which we will discuss later), chain
of thought performed better than the baseline by a large margin for all three annotators on eight
datasets in arithmetic, commonsense, and symbolic reasoning (Table 6 and Table 7). Similar to the
annotation process in Cobbe et al. (2021), annotators were not given speciﬁc instructions about
17
how to write the chain of thought annotations other than to simply write the step-by-step reasoning
process that led to the ﬁnal answer. Thus, the annotations were written in each annotator’s own
linguistic “chain of thought” writing style.
• Annotators without machine learning background. The GSM8K dataset (Cobbe et al., 2021)
conveniently provides a training set with reasoning chains written by crowd compute workers,
which enables us to investigate whether chain of thought still works with reasoning chains from an
independent source without a background in machine learning. So we randomly sampled three sets
of eight exemplars with chains of thought from GSM8K. These chain of thought annotations also
outperformed the baseline by a large margin for all four arithmetic datasets (Table 6), indicating
that chain of thought is not dependent on a particular set of annotators.
• Different exemplars. The different GSM8K exemplars experiment above (Table 6) also shows
that chain-of-thought prompting works for different sets of exemplars. Notably, we test every set of
exemplars on all four arithmetic datasets (instead of picking exemplars from the training set for
each dataset), which suggests that the exemplars do not necessarily have to come from the same
dataset distribution as the test examples.
• Different order of exemplars. Prior work has shown that in some cases (e.g., classiﬁcation) even
the order of prompts matter—varying the permutation of few-shot exemplars can cause the accuracy
of GPT-3 on SST-2 to range from near chance (54.3%) to near SOTA (93.4%) (Zhao et al., 2021).
We show the standard deviation of performance from different exemplars in Table 6 and Table 7.
Standard deviations with respect to prompt order are relatively minimal in almost all cases. The
one exception is the coin ﬂip task, for which exemplar orders have high standard deviation, likely
for the reason cited in Zhao et al. (2021)—for classiﬁcation, many exemplars of the same category
in a row biases the model outputs).
• Different number of exemplars. We also found that gains from chain-of-thought prompting
generally still held when there was a varying number of few-shot exemplars. This is shown for ﬁve
datasets in Figure 11 (we did not have the compute to run this for all datasets). We also found in
preliminary experiments that further increasing the number of exemplars in standard prompting
did not lead to signiﬁcant gains (e.g., increasing from 8 to 16 exemplars did not improve the
performance of standard prompting enough to catch up with chain-of-thought prompting).
• Different language models. Another interesting question is whether certain prompts that work
better for one model work better for other large language models. We ﬁnd that with the same
prompts, chain-of-thought prompting improves performance across all three models (LaMDA,
GPT-3, and PaLM) for all datasets except CSQA and StrategyQA for GPT-3 (Table 1, Table 4,
Table 5). The fact that gains from chain of thought did not transfer perfectly among models is
a limitation; further work could investigate why how different pre-training datasets and model
architectures affect the performance gain from chain-of-thought prompting.
Prompt engineering still matters, though. Although the results are relatively robust to the prompt
for arithmetic reasoning, we want to be clear that prompt engineering still does matter, and can
improve performance signiﬁcantly in many cases. Though most chain of thought annotations
outperform standard prompting, there is large variation in many cases. For instance, for the coin
ﬂip task, the performance varied from 99.6% for Annotator A to 71.4% for Annotator C, though
both were above standard prompting = 50.0% (see Table 7). There are even tasks where prompt
engineering is a requirement for good performance. In preliminary experiments, we tried using chain
of thought to enable language models to reverse the order of a list of 5 items. While two co-authors
were not able to write chain of thought prompts that solved the task despite their best attempts, a third
co-author was able to write a chain of thought that perfectly solved the task.
How to generate chain of thought annotations in a robust fashion could be an interesting direction
for future work. For instance, an idea here could be to use a large language model to automatically
generate chains of thought via prompting (and potentially optimize this over a validation set).
A.3
Will chain-of-thought prompting improve performance for my task of interest?
While chain-of-thought prompting is in principle applicable for any text-to-text task, it is more
helpful for some tasks than others. Based on the experiments in this paper, our intuition is that chain
of thought helps the most when three conditions are met: (1) the task is challenging and requires
18
multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively ﬂat.
Conversely, the beneﬁts are smaller when one or more of these conditions are not met.
These intuitions are perhaps supported by the arithmetic reasoning results. The performance gain
from chain-of-thought prompting is largest for PaLM 540B on GSM8K (challenging multi-step
problems, ﬂat scaling curve), which meets these conditions. The performance gain is small for the
subsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which
PaLM 540B already achieves performance of 90% or higher (and it is also generally true that there is
less headroom for improvement when performance is already strong).
Although in this paper we focused on multi-step reasoning tasks (arithmetic, commonsense, and
symbolic), chain-of-thought prompting can potentially be applied to any task for which humans use a
“chain of thought” to solve (at least in principle). We leave the empirical evaluation of chain-of-thought
prompting on such diverse tasks (e.g., machine translation, etc.) to future work.
A.4
Why is prompting with the equation only not enough for some arithmetic reasoning
datasets?
Prompting with the equation only as an intermediate step does help on many datasets, especially when
the datasets only require a few reasoning steps (SVAMP, ASDiv, MAWPS). For GSM8K, however,
using the equation only did not improve performance substantially. Based on qualitative analysis, we
believe that these questions are too semantically challenging for the model to directly translate them
into a math equation. Consider this example from LaMDA 137B:
QUESTION: Mike plays ping pong for 40 minutes. In the ﬁrst 20 minutes, he scores 4
points. In the second 20 minutes, he scores 25% more points. How many total points did he
score?
EQUATION ONLY (WRONG ANSWER): (4 + 20 * 0.25) = 6. The answer is 6.
CHAIN OF THOUGHT (CORRECT): Mike played ping pong for 40 minutes. In the ﬁrst
20 minutes, he scored 4 points. In the second 20 minutes, he scored 25% more points. So
he scored 25% more in the second 20 minutes. 4 x 1.25 = 5. So he scored 5 points in the
second 20 minutes. So he scored 9 points in total. The answer is 9.
It is hard for the model to directly translate all of the semantics into a single equation, but chain of
thought allows it to better reason about each part of the question via intermediate steps in natural
language.
19
B
All Experimental Results
This section contains tables for experimental results for varying models and model sizes, on all
benchmarks, for standard prompting vs. chain-of-thought prompting.
For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced)
were correct, except the model performed an arithmetic operation incorrectly. A similar observation
was made in Cobbe et al. (2021). Hence, we can further add a Python program as an external
calculator (using the Python eval function) to all the equations in the generated chain of thought.
When there are multiple equations in a chain of thought, we propagate the external calculator results
from one equation to the following equations via string matching. As shown in Table 1, we see that
adding a calculator signiﬁcantly boosts performance of chain-of-thought prompting on most tasks.
Table 1: Chain of thought prompting outperforms standard prompting for various large language
models on ﬁve arithmetic reasoning benchmarks. All metrics are accuracy (%). Ext. calc.: post-hoc
external calculator for arithmetic computations only. Prior best numbers are from the following. a:
Cobbe et al. (2021). b & e: Pi et al. (2022), c: Lan et al. (2021), d: Pi˛
ekos et al. (2021).
Prompting
GSM8K
SVAMP
ASDiv
AQuA
MAWPS
Prior best
N/A (ﬁnetuning) 55a
57.4b
75.3c
37.9d
88.4e
UL2 20B
Standard
4.1
10.1
16.0
20.5
16.6
Chain of thought 4.4 (+0.3)
12.5 (+2.4) 16.9 (+0.9) 23.6 (+3.1)
19.1 (+2.5)
+ ext. calc
6.9
28.3
34.3
23.6
42.7
LaMDA 137B
Standard
6.5
29.5
40.1
25.5
43.2
Chain of thought 14.3 (+7.8)
37.5 (+8.0) 46.6 (+6.5) 20.6 (-4.9)
57.9 (+14.7)
+ ext. calc
17.8
42.1
53.4
20.6
69.3
GPT-3 175B
Standard
15.6
65.7
70.3
24.8
72.7
(text-davinci-002) Chain of thought 46.9 (+31.3) 68.9 (+3.2) 71.3 (+1.0) 35.8 (+11.0) 87.1 (+14.4)
+ ext. calc
49.6
70.3
71.1
35.8
87.5
Codex
Standard
19.7
69.9
74.0
29.5
78.7
(code-davinci-002) Chain of thought 63.1 (+43.4) 76.4 (+6.5) 80.4 (+6.4) 45.3 (+15.8) 92.6 (+13.9)
+ ext. calc
65.4
77.0
80.0
45.3
93.3
PaLM 540B
Standard
17.9
69.4
72.1
25.2
79.2
Chain of thought 56.9 (+39.0) 79.0 (+9.6) 73.9 (+1.8) 35.8 (+10.6) 93.3 (+14.2)
+ ext. calc
58.6
79.8
72.6
35.8
93.5
20
Table 2: Standard prompting versus chain of thought prompting on ﬁve arithmetic reasoning bench-
marks. Note that chain of thought prompting is an emergent ability of model scale—it does not
positively impact performance until used with a model of sufﬁcient scale.
GSM8K
SVAMP
ASDiv
AQuA
MAWPS
Model
standard
CoT standard
CoT standard
CoT standard
CoT standard
CoT
UL2
20B
4.1
4.4
10.1 12.5
16.0 16.9
20.5 23.6
16.6 19.1
LaMDA 420M
2.6
0.4
2.5
1.6
3.2
0.8
23.5
8.3
3.2
0.9
2B
3.6
1.9
3.3
2.4
4.1
3.8
22.9 17.7
3.9
3.1
8B
3.2
1.6
4.3
3.4
5.9
5.0
22.8 18.6
5.3
4.8
68B
5.7
8.2
13.6 18.8
21.8 23.1
22.3 20.2
21.6 30.6
137B
6.5 14.3
29.5 37.5
40.1 46.6
25.5 20.6
43.2 57.9
GPT
350M
2.2
0.5
1.4
0.8
2.1
0.8
18.1
8.7
2.4
1.1
1.3B
2.4
0.5
1.5
1.7
2.6
1.4
12.6
4.3
3.1
1.7
6.7B
4.0
2.4
6.1
3.1
8.6
3.6
15.4 13.4
8.8
3.5
175B
15.6 46.9
65.7 68.9
70.3 71.3
24.8 35.8
72.7 87.1
Codex
-
19.7 63.1
69.9 76.4
74.0 80.4
29.5 45.3
78.7 92.6
PaLM
8B
4.9
4.1
15.1 16.8
23.7 25.2
19.3 21.7
26.2 30.5
62B
9.6 29.9
48.2 46.7
58.7 61.9
25.6 22.4
61.8 80.3
540B
17.9 56.9
69.4 79.0
72.1 73.9
25.2 35.8
79.2 93.3
Table 3: Standard prompting versus chain of thought prompting on the four subsets of the MAWPS
benchmark. The point of stratifying the MAWPS benchmark is to show that performance gains are
minimal on easy one-step or two-step problems where large language models already achieve high
performance (e.g., SingleOp, SingleEq, and AddSub).
SingleOp
SingleEq
AddSub
MultiArith
Model
standard
CoT standard
CoT standard
CoT standard
CoT
UL2
20B
24.9 27.2
18.0 20.2
18.5 18.2
5.0 10.7
LaMDA 420M
2.8
1.0
2.4
0.4
1.9
0.7
5.8
1.5
2B
4.6
4.1
2.4
3.3
2.7
3.2
5.8
1.8
8B
8.0
7.0
4.5
4.4
3.4
5.2
5.2
2.4
68B
36.5 40.8
23.9 26.0
17.3 23.2
8.7 32.4
137B
73.2 76.2
48.8 58.7
43.0 51.9
7.6 44.9
GPT
350M
3.2
1.8
2.0
0.2
2.0
1.5
2.3
0.8
1.3B
5.3
3.0
2.4
1.6
2.3
1.5
2.2
0.5
6.7B
13.5
3.9
8.7
4.9
8.6
2.5
4.5
2.8
175B
90.9 88.8
82.7 86.6
83.3 81.3
33.8 91.7
Codex
-
93.1 91.8
86.8 93.1
90.9 89.1
44.0 96.2
PaLM
8B
41.8 46.6
29.5 28.2
29.4 31.4
4.2 15.8
62B
87.9 85.6
77.2 83.5
74.7 78.2
7.3 73.7
540B
94.1 94.1
86.5 92.3
93.9 91.9
42.2 94.7
21
Table 4: Standard prompting versus chain of thought prompting on ﬁve commonsense reasoning
benchmarks. Chain of thought prompting is an emergent ability of model scale—it does not positively
impact performance until used with a model of sufﬁcient scale.
CSQA
StrategyQA
Date
Sports
SayCan
Model
standard
CoT standard
CoT standard
CoT standard
CoT standard
CoT
UL2
20B
34.2 51.4
59.0 53.3
13.5 14.0
57.9 65.3
20.0 41.7
LaMDA 420M
20.1 19.2
46.4 24.9
1.9
1.6
50.0 49.7
7.5
7.5
2B
20.2 19.6
52.6 45.2
8.0
6.8
49.3 57.5
8.3
8.3
8B
19.0 20.3
54.1 46.8
9.5
5.4
50.0 52.1
28.3 33.3
68B
37.0 44.1
59.6 62.2
15.5 18.6
55.2 77.5
35.0 42.5
137B
53.6 57.9
62.4 65.4
21.5 26.8
59.5 85.8
43.3 46.6
GPT
350M
14.7 15.2
20.6
0.9
4.3
0.9
33.8 41.6
12.5
0.8
1.3B
12.0 19.2
45.8 35.7
4.0
1.4
0.0 26.9
20.8
9.2
6.7B
19.0 24.0
53.6 50.0
8.9
4.9
0.0
4.4
17.5 35.0
175B
79.5 73.5
65.9 65.4
43.8 52.1
69.6 82.4
81.7 87.5
Codex
-
82.3 77.9
67.1 73.2
49.0 64.8
71.7 98.5
85.8 88.3
PaLM
8B
19.8 24.9
55.6 53.5
12.9 13.1
55.1 75.2
34.2 40.0
62B
65.4 68.1
58.4 63.4
29.8 44.7
72.1 93.6
65.8 70.0
540B
78.1 79.9
68.6 77.8
49.0 65.3
80.5 95.4
80.8 91.7
Table 5: Standard prompting versus chain of thought prompting enables length generalization to
longer inference examples on two symbolic manipulation tasks.
Last Letter Concatenation
Coin Flip (state tracking)
2
OOD: 3
OOD: 4
2
OOD: 3
OOD: 4
Model
standard CoT standard CoT standard CoT standard
CoT standard CoT standard CoT
UL2
20B
0.6 18.8
0.0 0.2
0.0 0.0
70.4 67.1
51.6 52.2
48.7 50.4
LaMDA 420M
0.3 1.6
0.0 0.0
0.0 0.0
52.9 49.6
50.0 50.5
49.5 49.1
2B
2.3 6.0
0.0 0.0
0.0 0.0
54.9 55.3
47.4 48.7
49.8 50.2
8B
1.5 11.5
0.0 0.0
0.0 0.0
52.9 55.5
48.2 49.6
51.2 50.6
68B
4.4 52.0
0.0 0.8
0.0 2.5
56.2 83.2
50.4 69.1
50.9 59.6
137B
5.8 77.5
0.0 34.4
0.0 13.5
49.0 99.6
50.7 91.0
49.1 74.5
PaLM
8B
2.6 18.8
0.0 0.0
0.0 0.2
60.0 74.4
47.3 57.1
50.9 51.8
62B
6.8 85.0
0.0 59.6
0.0 13.4
91.4 96.8
43.9 91.0
38.3 72.4
540B
7.6 99.4
0.2 94.8
0.0 63.0
98.1 100.0
49.3 98.6
54.8 90.2
22
Table 6: Ablation and robustness results for arithmetic reasoning datasets. Chain of thought generally
outperforms ablations by a large amount. “Equation only” performs in between standard prompting
and chain of thought prompting, as it allows for intermediate reasoning steps via equations but does
not leverage natural language. Chain of thought prompting has variance (as expected) when used
with prompts written by different annotators or when using other exemplars, but still outperforms
standard prompting by a large margin. Standard deviation shown is for different order of few-shot
prompting exemplars, with ﬁve different random seeds. Results here are shown for LaMDA 137B, as
additional queries for GPT-3 and PaLM are both limited and expensive.
GSM8K
SVAMP
ASDiv
MAWPS
Standard prompting
6.5 ±0.4
29.5 ±0.6
40.1 ±0.6
43.2 ±0.9
Chain of thought prompting
14.3 ±0.4
36.7 ±0.4
46.6 ±0.7
57.9 ±1.5
Ablations
· equation only
5.4 ±0.2
35.1 ±0.4
45.9 ±0.6
50.1 ±1.0
· variable compute only
6.4 ±0.3
28.0 ±0.6
39.4 ±0.4
41.3 ±1.1
· reasoning after answer
6.1 ±0.4
30.7 ±0.9
38.6 ±0.6
43.6 ±1.0
Robustness
· different annotator (B)
15.5 ±0.6
35.2 ±0.4
46.5 ±0.4
58.2 ±1.0
· different annotator (C)
17.6 ±1.0
37.5 ±2.0
48.7 ±0.7
60.1 ±2.0
· intentionally concise style
11.1 ±0.3
38.7 ±0.8
48.0 ±0.3
59.6 ±0.7
· exemplars from GSM8K (α)
12.6 ±0.6
32.8 ±1.1
44.1 ±0.9
53.9 ±1.1
· exemplars from GSM8K (β)
12.7 ±0.5
34.8 ±1.1
46.9 ±0.6
60.9 ±0.8
· exemplars from GSM8K (γ)
12.6 ±0.7
35.6 ±0.5
44.4 ±2.6
54.2 ±4.7
Table 7: Ablation and robustness results for four datasets in commonsense and symbolic reasoning.
Chain of thought generally outperforms ablations by a large amount. Chain of thought prompting has
variance (as expected) when used with prompts written by different annotators or when using other
exemplars, but still outperforms standard prompting by a large margin. Standard deviation shown
is for different order of few-shot prompting exemplars, with ﬁve different random seeds. Results
here are shown for LaMDA 137B, as additional queries for GPT-3 and PaLM are both limited and
expensive. The exception is that we run SayCan using PaLM here, as the SayCan evaluation set is
only 120 examples and therefore less expensive to run multiple times.
Commonsense
Symbolic
Date
Sports
SayCan
Concat
Coin
Standard prompting
21.5 ±0.6
59.5 ±3.0
80.8 ±1.8
5.8 ±0.6
49.0 ±2.1
Chain of thought prompting
26.8 ±2.1
85.8 ±1.8
91.7 ±1.4
77.5 ±3.8
99.6 ±0.3
Ablations
· variable compute only
21.3 ±0.7
61.6 ±2.2
74.2 ±2.3
7.2 ±1.6
50.7 ±0.7
· reasoning after answer
20.9 ±1.0
63.0 ±2.0
83.3 ±0.6
0.0 ±0.0
50.2 ±0.5
Robustness
· different annotator (B)
27.4 ±1.7
75.4 ±2.7
88.3 ±1.4
76.0 ±1.9
77.5 ±7.9
· different annotator (C)
25.5 ±2.5
81.1 ±3.6
85.0 ±1.8
68.1 ±2.2
71.4 ±11.1
23
C
Extended Related Work
Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-
ing, natural language explanations, program synthesis/execution, numeric and logical reasoning, and
intermediate language steps.
C.1
Prompting
The recent success of large-scale language models has led to growing interest in improving their
capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a
survey). This paper falls in the category of general prompting approaches, whereby input prompts are
optimized to allow a single large language model to better perform a variety of tasks (Li and Liang,
2021; Lester et al., 2021; Reif et al., 2022, inter alia).
One recent line of work aims to improve the ability of language models to perform a task by providing
instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh
et al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output
pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically
prepended to the inputs), chain-of-thought prompting augments the outputs of language models.
Another related direction is sequentially combining the outputs of language models; human–computer
interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of
language models improves task outcomes in a 20-person user study.
C.2
Natural language explanations
Another closely related direction uses natural language explanations (NLEs), often with the goal of
improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi´
c, 2021, inter alia). That
line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,
2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal
prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,
the chain of thought processing considered in this paper occurs before the ﬁnal answer. And while
NLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of
chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple
steps—interpretability is just a side effect. Marasovi´
c et al. (2022) show that prompt-based ﬁnetuning
with NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating
explanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,
and symbolic tasks that require multi-hop reasoning.
C.3
Program synthesis and execution
Using intermediate reasoning steps has a long history in program synthesis and execution (Zaremba
and Sutskever, 2014, inter alia). Recent work along in this direction has included a number of
architectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of
large language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to
ours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit
addition, evaluate polynomials, and execute python programs. Whereas generating a program and
then executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc
primitives to natural language, which is open-domain and relevant to any text-to-text NLP task in
principle.
C.4
Numeric and logical reasoning
Numeric and logical reasoning has been a long-studied task in machine learning and natural language
processing (Lev et al., 2004, inter alia). Recent work has also aimed to inject numeric reasoning
abilities in language models in various ways, such as augmenting BERT with a predeﬁned set of
executable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and
using specialized training procedures (Pi˛
ekos et al., 2021). Another line of work aims to enable
language models to perform logical or formal reasoning, often by verablizing the rules in natural
language formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).
24
Perhaps the most-related work here is Recchia (2021), which shows that ﬁnetuning enables longhand
module operations, which has previously been difﬁcult for performers. Whereas work in this direction
is often task-speciﬁc and uses ﬁnetuning, we show that chain-of-thought prompting works for a broad
range of tasks without any ﬁnetuning.
C.5
Intermediate language steps
Extensive prior work has shown the beneﬁts of endowing neural networks with the ability to produce
intermediate steps via training or ﬁnetuning confers various beneﬁts in a range of scenarios. As
examples, it has been shown that natural language intermediate steps can improve performance
(Zaidan et al., 2007; Yao et al., 2021; Hase and Bansal, 2022; Gu et al., 2022), improve robustness
(Chen et al., 2022), speed up training (Hancock et al., 2018), mitigate bias (Dua et al., 2020), and
even help in image and reinforcement learning settings (Andreas et al., 2018). To endow models with
the ability to produce intermediate steps, prior work typically ﬁnetunes models on either manually
annotated training datasets (Camburu et al., 2018; Rajani et al., 2019, inter alia) or generates synthetic
datasets (Talmor et al., 2020; Zelikman et al., 2022). Compared with these training or ﬁnetuning
methods, our work shows that various natural language reasoning abilities can be elicited in off-the-
shelf language models of sufﬁcient scale simply via prompting. This prompting setup is important
because it allows for intermediate step reasoning without a large number of labeled annotations, and
because a single model can perform a range of reasoning tasks without any gradient updates.
D
Appendix: Additional Analysis
D.1
Correct Chain of Thought Analysis
As mentioned in the main text, we analyze 50 chains of thought from LaMDA 137B that led to
correct answers in the GSM8K dataset. Of these 50, only one arrived at the correct answer through
incorrect reasoning (shown in Table 9: “correct by chance”). The other 49 had correct logic and
math, with examples shown in Table 8. Five had minor imperfections while maintaining coherent and
understandable logic:
• One had underspeciﬁed statements (shown in Table 9: “correct but underspeciﬁed statements”).
• One made an unrelated but correct statement (shown in Table 9: “correct but unrelated statement”).
• Two had the correct logic but omitted speciﬁc steps in the equation, though it was still understand-
able (shown in Table 9: “correct but imperfect equation”).
• One had the correct math but inverted the semantics (shown in Table 9: “correct but inverted
semantics”)
Table 8: Example of correct chains of thought produced by the model for the GSM8K dataset.
QUESTION: James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total
meters does he run a week?
MODEL ANSWER (CORRECT): James decides to run 3 sprints 3 times a week. He runs 60 meters each
sprint. So he runs 60 meters x 3 sprints x 3 times a week. That is 60 meters x 9. The answer is 540. ✓
QUESTION: Brandon’s iPhone is four times as old as Ben’s iPhone. Ben’s iPhone is two times older than Suzy’s
iPhone. If Suzy’s iPhone is 1 year old, how old is Brandon’s iPhone?
MODEL ANSWER (CORRECT): Brandon’s iPhone is 4 times as old as Ben’s iPhone. Ben’s iPhone is 2
times older than Suzy’s iPhone. So Brandon’s iPhone is 4 x 2 = 8 times older than Suzy’s iPhone. Suzy’s iPhone
is 1 year old. So Brandon’s iPhone is 8 x 1 = 8 years old. The answer is 8. ✓
QUESTION: Jean has 30 lollipops. Jean eats 2 of the lollipops. With the remaining lollipops, Jean wants to
package 2 lollipops in one bag. How many bags can Jean ﬁll?
MODEL ANSWER (CORRECT): Jean started with 30 lollipops. She ate 2 of them. So she has 28 lollipops
left. She wants to package 2 lollipops in one bag. So she can package 28 / 2 = 14 bags. The answer is 14. ✓
Although we ﬁnd that chain-of-thought reasoning is mostly correct for math questions where the ﬁnal
answer was correct, this is likely because it is hard to arrive at the correct ﬁnal answer by coincidence
for free response questions. For multiple choice or binary classiﬁcation problems, it was much more
25
likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the
commmonsense reasoning datasets we evaluate). This is a limitation, and future work should perform
an analysis of the factuality of such chains of thought.
Table 9: Of 50 examples that the model got correct, there were 7 salient cases where the chain of
thought the model generated was imperfect. We found the other 43 to be equivalent to what a human
would potentially generate as a ground truth.
QUESTION: There are 36 penguins sunbathing in the snow. One-third of them jump in and swim in the ocean.
Another one-third go inside the cave to eat their dinner. How many penguins are still left sunbathing?
MODEL ANSWER (CORRECT BY CHANCE): There are 36 penguins. One-third of them jump in and swim in
the ocean. So that is 12 penguins. Another one-third go inside the cave to eat their dinner. So that is 12 penguins.
The answer is 12.
QUESTION: John has 2 houses with 3 bedrooms each. Each bedroom has 2 windows each. There are an
additional 4 windows in each house not connected to bedrooms. How many total windows are there between the
houses?
MODEL ANSWER (CORRECT BUT UNDERSPECIFIED STATEMENTS): There are 2 houses with 3 bedrooms
each. Each bedroom has 2 windows each. So there are 2 x 3 = 6 windows in each house. There are an additional
4 windows in each house not connected to bedrooms. So there are 4 + 4 = 8 windows in each house. So there are
6 x 2 = 12 windows in each house. So there are 12 + 8 = 20 windows in both houses. The answer is 20.
QUESTION: If Ann is 9 years old and her brother is twice her age, how old will her brother be in 3 years?
MODEL ANSWER (CORRECT BUT UNRELATED STATEMENT): Ann is 9 years old. Her brother is twice her
age. So her brother is 18 years old. In 3 years, she will be 12. So her brother will be 18 + 3 = 21 years old. The
answer is 21.
QUESTION: A local gas station is selling gas for $3.00 a gallon. An app company is offering $.20 cashback per
gallon if you ﬁll up at this station. If someone buys 10 gallons of gas, how much with their gas be, after the
cashback rewards?
MODEL ANSWER (CORRECT BUT IMPERFECT EQUATION): The gas is originally 3 dollars per gallon. If
you buy 10 gallons, that is 3 x 10 = 30 dollars. If you get 20 cents off per gallon, that is 20 x 10 = 2 dollars. So
the total is 30 - 2 = 28 dollars. The answer is 28 dollars.
QUESTION: Nissa hires 60 seasonal workers to play elves in her department store’s Santa village. A third of
the elves quit after children vomit on them, then 10 of the remaining elves quit after kids kick their shins. How
many elves are left?
MODEL ANSWER (CORRECT BUT IMPERFECT EQUATION): Nissa hires 60 seasonal workers. A third of
them quit. So 60 - 1/3 = 40 elves are left. Then 10 elves quit. So 40 - 10 = 30 elves are left. The answer is 30
elves.
QUESTION: Grandma Jones baked 5 apple pies for the ﬁreman’s luncheon. She cut each pie into 8 pieces and
set the ﬁve pies out on the buffet table for the guests to serve themselves. At the end of the evening, after the
guests had taken and eaten their pieces of pie, there were 14 pieces of pie remaining. How many pieces were
taken by the guests?
MODEL ANSWER (CORRECT BUT INVERTED SEMANTICS): 5 pies were baked and cut into 8 pieces each.
The 5 pies were then served to the guests. 8 x 5 = 40 pieces of pie. The guests ate 14 pieces of pie. 40 - 14 = 26
pieces of pie were left. The answer is 26. ✓
26
D.2
Incorrect Chain of Thought Analysis
We also manually analyze 50 randomly sampled outputs of the model that were incorrect on GSM8K
for LaMDA 137B. There are many ways that a chain of thought can be incorrect, making the design
of error categorization non-trivial. We decided to categorize errors into what changes are needed to
make the chain of thought correct, with the goal of elucidating how the model can be improved in the
future.
We found that many chains of thought can be made correct with one of the following three classes of
modiﬁcation.
• Calculator error only. We found that 8% of the chains of thought were completely correct except
for a calculator error—in other words, applying an external calculator to equations, as done in
Cobbe et al. (2021), would make the chain of thought correct. An example of this type of error is
shown in Table 10: “calculator error only”. Indeed, the solve rate of chain-of-thought prompting
on for LaMDA 137B GSM8K went up from 14.3% to 17.3% when we added a Python program
as an external calculator, as shown in Table 2. Also, 34% of the examples contained calculator
errors in addition to other types of errors. However, we perform the rest of the error categorization
independently of calculator errors.
• Symbol mapping error. We next found that 16% percent of the chains of thought were correct
except for what we call symbol mapping errors. We deﬁne a symbol mapping error as when the
chain of thought is correct except for the number symbols, and it could be made totally correct by
modifying only the equations and not the words. As one might argue that they could simply place
the correct ﬁnal equation in any chain of thought, we constrain this category to chains of thought
where the chain of thought can be modiﬁed to be a completely correct reasoning process (not just
ﬁnal answer). An example of this error category is shown in Table 10: “symbol mapping error”.
• One step missing error. Our next category of error is chains of thought which were correct except
that they were missing a single step. In other words, these chains of thoughts could be rewritten to
be correct by adding in an additional reasoning step that was missed by the model. An example of
this error category is shown in Table 10: “one step missing error”. We found that 22% percent of
the errors fell into this category.
Table 10: Example of incorrect chains of thought, categorized as described in Appendix D.2.
QUESTION: Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns
are there total?
MODEL ANSWER (INCORRECT; CALCULATOR ERROR ONLY): Dan plants 3 rose bushes. Each rose bush
has 25 roses. Each rose has 8 thorns. So 3 x 25 x 8 = 300. The answer is 300. 
EXPLANATION FOR ERROR CATEGORY: The produced chain of thought could be made correct just by
running the equation through an external calculator, instead of asking the model to do the computation.
QUESTION: Jill gets paid $20 per hour to teach and $30 to be a cheerleading coach. If she works 50 weeks a
year, 35 hours a week as a teacher and 15 hours a week as a coach, what’s her annual salary?
MODEL ANSWER (INCORRECT; SYMBOL MAPPING ERROR): Jill gets paid 20 dollars per hour to teach
and 30 dollars per hour to be a cheerleading coach. If she works 50 weeks a year, 35 hours a week as a teacher
and 15 hours a week as a coach, then she works 50 x 35 = 1750 hours as a teacher and 15 x 30 = 450 hours as a
coach. So she works 1750 + 450 = 2200 hours. She gets paid 20 dollars per hour for 1750 hours and 30 dollars
per hour for 450 hours. So her annual salary is 20 x 1750 + 30 x 450 = $36,500. The answer is $36,500. 
EXPLANATION FOR ERROR CATEGORY: The model incorrectly mapped “15 x 30” instead of “15 x 50”, and
the chain of thought would be made correct by ﬁxing just that one change (with an external calculator).
QUESTION: Kelian has two recipes for preparing dishes, one having 20 instructions and the second one having
twice as many instructions as the ﬁrst one. How many instructions does Kelian have to read to prepare the two
dishes?
MODEL ANSWER (INCORRECT; ONE STEP MISSING ERROR): Kelian has two recipes. One has 20 instruc-
tions. The other has twice as many instructions as the ﬁrst one. So the second one has 40 instructions. So Kelian
has to read 40 instructions to prepare the two dishes. The answer is 40. 
EXPLANATION FOR ERROR CATEGORY: The model answer could be made correct just adding one additional
step (in this case, adding 20 and 40).
27
We found that the remaining chains of thought (27 of 50; 54%) would require substantial edits to make
into a correct chain of thought. Almost all cases here involved some error in semantic understanding
(see Table 11: “semantic understanding error”), and 8 of the 27 also had incoherent chain of thoughts,
meaning that some statements in the generated chain of thought did not follow from prior ones or
violated basic world knowledge (see Table 11: “incoherent chain of thought error”).
Table 11: Example of incorrect chains of thought, categorized as described in Appendix D.2. These
chains of thought had multiple mistakes that could not be ﬁxed with minor edits.
QUESTION: Gretchen has 110 coins. There are 30 more gold coins than silver coins. How many gold coins
does Gretchen have?
MODEL ANSWER (INCORRECT; SEMANTIC UNDERSTANDING ERROR): Gretchen has 110 coins. There
are 30 more gold coins than silver coins. So there are 110 - 30 = 80 silver coins. So there are 80 silver coins and
110 - 80 = 30 gold coins. The answer is 30. 
EXPLANATION FOR ERROR CATEGORY: The model does not understand that 110 coins are in total and that
it must solve an equation to get the number of gold and silver coins.
QUESTION: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining
enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in
hip-hop dance?
MODEL ANSWER (INCORRECT; INCOHERENT CHAIN OF THOUGHT ERROR): 20% of the students en-
rolled in contemporary dance. 25% of the remaining students enrolled in jazz dance. The rest enrolled in hip-hop
dance. So the percentage of the entire students enrolled in hip-hop dance is the percentage of the students
enrolled in hip-hop dance, minus the percentage of the students enrolled in contemporary dance, minus the
percentage of the students enrolled in jazz dance. So the percentage of the entire students enrolled in hip-hop
dance is (25 + 20) - (25 + 20) = 100%. The answer is 100%. 
EXPLANATION FOR ERROR CATEGORY: This chain of thought is incoherent in that the percent of entire
students enrolled in hip-hope dance cannot be the percent of student enrolled in hip-hop dance minus another
term.
Overall, there are no guarantees that the reasoning processes generated by large language models
are coherent or factually correct, as underscored by the recent work evaluating the factuality of
language model generations and explanations (Maynez et al., 2020; Rashkin et al., 2021; Ye and
Durrett, 2022; Marasovi´
c et al., 2022; Wiegreffe et al., 2022). Incorrect reasoning processes can lead
to both incorrect ﬁnal answers as well as accidentally correct ﬁnal answers (with accidentally correct
ﬁnal answers being more likely for tasks such as binary classiﬁcation as opposed to free response).
Improving the factuality of language model generations with respect to context and world knowledge
is an important direction open problems in language model research and could also be expected to
potentially improve multi-step reasoning abilities of language models. One potential method for
improving the quality of decoding could involve generating multiple reasoning paths and scoring
each of them with a veriﬁer, though this requires training the veriﬁer (Cobbe et al., 2021; Shen et al.,
2021; Thoppilan et al., 2022).
D.3
Additional Robustness Analysis
As the experiments in the main paper use a ﬁxed number of few-shot exemplars (8; as constrained by
the input length of 1024 tokens), we verify that the chain-of-thought prompting is robust to various
numbers of few-shot exemplars. We run experiments for LaMDA 137B, comparing chain-of-thought
prompting with standard prompting for the ﬁve datasets where standard prompting had a mostly ﬂat
scaling curve (the largest model did not achieve high performance). As shown in Figure 11, the
improvement of chain-of-thought prompting over standard prompting remains robust to varying the
number of few-shot exemplars in the prompt.
28
1 2
4
6
8
0
5
10
15
Solve rate (%)
GSM8K
1 2
4
6
8
0
20
40
60
MultiArith
(MAWPS)
1 2
4
6
8
0
25
50
75
100
Number of few-shot exemplars
Sports
Understanding
Standard prompting
Chain of thought prompting
1 2
4
6
8
0
25
50
75
100
Coin Flip
1
2
3
4
0
25
50
75
100
Last Letter
Concatenation
Figure 11: The improvement of chain of thought prompting over standard prompting appears robust
to varying the number of few-shot exemplars in the prompt.
Table 12: Summary of math word problem benchmarks we use in this paper with examples. N:
number of evaluation examples.
Dataset
N
Example problem
GSM8K
1,319 Josh decides to try ﬂipping a house. He buys a house for $80,000 and then puts
in $50,000 in repairs. This increased the value of the house by 150%. How
much proﬁt did he make?
SVAMP
1,000 Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each
pack. How much do you have to pay to buy each pack?
ASDiv
2,096 Ellen has six more balls than Marin. Marin has nine balls. How many balls does
Ellen have?
AQuA
254
A car is being driven, in a straight line and at a uniform speed, towards the base
of a vertical tower. The top of the tower is observed from the car and, in the
process, it takes 10 minutes for the angle of elevation to change from 45◦to 60◦.
After how much more time will this car reach the base of the tower? Answer
Choices: (a) 5
√
3 + 1 (b) 6
√
3 +
√
2 (c) 7
√
3 - 1 (d) 8
√
3 - 2 (e) None of these
MAWPS: SingleOp
562
If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how
many bottle caps are in the box?
MAWPS: SingleEq
508
Benny bought a soft drink for 2 dollars and 5 candy bars. He spent a total of 27
dollars. How much did each candy bar cost?
MAWPS: AddSub
395
There were 6 roses in the vase. Mary cut some roses from her ﬂower garden.
There are now 16 roses in the vase. How many roses did she cut?
MAWPS: MultiArith
600
The school cafeteria ordered 42 red apples and 7 green apples for students
lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria
end up with?
29
E
Additional Details
Version Control
V5 →V6. Fixed minor typo in Figure 3.
V4 →V5. Added Codex and UL2 results. Small changes to writing and style of paper.
V3 →V4. Fixed typo in Figure 3 and added a couple citations.
V2 →V3. Added GPT-3 results. Added SVAMP and AQuA eval datasets for math. Added SayCan
eval for commonsense. Added Extended Related Work section (Appendix C). Added ablations for
Commonsense and Symbolic Reasoning (Table 7). Added FAQ section (Appendix A). Added raw
results in Appendix B.
V1 →V2. Added PaLM results (V1 only had LaMDA).
E.1
Reproducibility Statement
As our results make use of two sets of large language models that is not publicly available, we take
the following actions to facilitate reproducibility. First, we provide the exact input prompts for all
tasks in Table 20–Table 27 in Appendix G (and emphasize that we do not perform any ﬁnetuning and
only apply prompting to off-the-shelf language models). Second, we conduct experiments using the
publicly available GPT-3 API for four model scales text-ada-001, text-babbage-001, text-curie-001,
text-davinci-002). Finally, we make exact inputs, targets, and predictions for LaMDA 137B for each
task available as a zip ﬁle in the supplementary material.
E.2
Computational Resources
For all three language models we evaluated, we did prompting-based inference only. No ﬁnetuning
was done for this paper. For inference on LaMDA 137B we use TPU v3 (8x8 conﬁguration, 64 chips
/ 128 cores), and for inference on PaLM 540B we use TPU v4 (4x4x12 conﬁguration, 192 chips / 384
cores). GPT-3 experiments were done using the public API.5
E.3
Dataset Details and Licenses
We list the details and licenses for all arithmetic and commonsense datasets used in this paper. The
symbolic reasoning datasets were created synthetically, as described in Section 4.
Arithmetic reasoning
• Math Word Problem Repository (Koncel-Kedziorski et al., 2016): AddSub (Hosseini
et al., 2014): https://www.cs.washington.edu/nlp/arithmetic; MultiArith (Roy
and Roth, 2015), license: CC BY 4.0.
• ASDiv (Miao et al., 2020): https://github.com/chaochun/nlu-asdiv-dataset.
• AQuA (Ling et al., 2017): https://github.com/deepmind/AQuA, license: https://
github.com/deepmind/AQuA/blob/master/LICENSE.
• GSM8K (Cobbe et al., 2021): https://github.com/openai/grade-school-math,
MIT license:
https://github.com/openai/grade-school-math/blob/master/
LICENSE.
• SVAMP (Patel et al., 2021): https://github.com/arkilpatel/SVAMP, MIT license:
https://github.com/arkilpatel/SVAMP/blob/main/LICENSE.
Commonsense reasoning
• CSQA (Talmor et al., 2019): https://www.tau-nlp.org/commonsenseqa, https://
github.com/jonathanherzig/commonsenseqa.
5https://beta.openai.com/docs/api-reference/making-requests
30
• StrategyQA (Geva et al., 2021): we use the open-domain setting (question-only set)
from BIG-bench collaboration (2021):
https://github.com/google/BIG-bench/
tree/main/bigbench/benchmark_tasks/strategyqa. The original dataset is from
https://github.com/eladsegal/strategyqa, MIT license: https://github.com/
eladsegal/strategyqa/blob/main/LICENSE.
• Date understanding and sports understanding from BIG-Bench (BIG-bench collaboration,
2021): Apache License v.2: https://github.com/google/BIG-bench/blob/main/
LICENSE.
• SayCan (Ahn et al., 2022): SayCan dataset can be accessed at https://say-can.github.
io/ under CC BY 4.0 license.
31
F
Appendix: Input/Output Examples
Table 13: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the
letter concatenation task.
QUESTION: Take the last letters of the words in “Waldo Schmidt” and concatenate them.
MODEL ANSWER (CORRECT): The last letter of “Waldo” is “o”. The last letter of “Schmidt” is “t”.
Concatenating them is “ot”. So the answer is ot. ✓
QUESTION: Take the last letters of the words in “Daniel Friedman” and concatenate them.
MODEL ANSWER (INCORRECT): The last letter of “Daniel” is “l”. The last letter of “Friedman” is “m”.
Concatenating them is “lm”. So the answer is lm. 
Table 14: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the
coinﬂip task.
QUESTION: A coin is heads up. Andree ﬂips the coin. Audrie does not ﬂip the coin. Is the coin still heads up?
MODEL ANSWER (CORRECT): The coin was ﬂipped by Andree. So the coin was ﬂipped 1 time, which is
an odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is no.
✓
QUESTION: A coin is heads up. Kristian does not ﬂip the coin. Dallas does not ﬂip the coin. Is the coin still
heads up?
MODEL ANSWER (INCORRECT): The coin was ﬂipped by Kristian. So the coin was ﬂipped 1 time, which
is an odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is
no. 
Table 15: Examples of correct and incorrect chains of thought produced by LaMDA 137B on CSQA.
QUESTION: When a person is beginning work, what are they building?
Answer Choices: (a) time (b) accomplishing (c) working (d) momentum (e) tiredness
MODEL ANSWER (CORRECT): The answer must be something that is built. Of the above choices, only
momentum is built. So the answer is (d). ✓
QUESTION: Where are you likely to ﬁnd a hamburger?
Answer Choices: (a) fast food restaurant (b) pizza (c) ground up dead cows (d) mouth (e) cow carcus
MODEL ANSWER (CORRECT): The answer must be a place where hamburgers are found. Of the above
choices, only fast food restaurants serve hamburgers. So the answer is (a). ✓
QUESTION: Aside from water and nourishment what does your dog need?
Answer Choices: (a) bone (b) charm (c) petted (d) lots of attention (e) walked
MODEL ANSWER (INCORRECT): The answer must be something that a dog needs. Of the above choices,
only bone is something that a dog needs. So the answer is (a).  (Ground truth is (d).)
QUESTION: What are you waiting alongside with when you’re in a reception area?
Answer Choices: (a) motel (b) chair (c) hospital (d) people (e) hotels
MODEL ANSWER (INCORRECT): The answer must be something that is waiting with you in a reception
area. Of the above choices, only people are waiting with you in a reception area. So the answer is (e).  (Ground
truth is (d).)
32
Table 16:
Examples of correct and incorrect chains of thought produced by LaMDA 137B on
StrategyQA.
QUESTION: Will The Exorcist stimulate limbic system?
MODEL ANSWER (CORRECT): The Exorcist is a horror movie. Horror movies are scary. The limbic system
is involved in fear. Thus, The Exorcist will stimulate the limbic system. So the answer is yes. ✓
QUESTION: Was Jackson Pollock trained by Leonardo da Vinci?
MODEL ANSWER (CORRECT): Leonardo da Vinci lived in the 15th century. Jackson Pollock lived in the
20th century. Thus, Jackson Pollock could not have been trained by Leonardo da Vinci. So the answer is no. ✓
QUESTION: Can Harry Potter book a ﬂight on Asiana Airlines?
MODEL ANSWER (INCORRECT): Harry Potter is a ﬁctional character. Thus, Harry Potter can do anything.
So the answer is yes.  (Ground truth is “no”.)
QUESTION: Would a sophist use an épée?
MODEL ANSWER (INCORRECT): A sophist is a person who is skilled in the art of persuasion. An épée is
a type of sword. Thus, a sophist could use an épée. So the answer is yes.  (Ground truth is “no”.)
Table 17: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Date
Understanding.
QUESTION: May 6, 1992 is like yesterday to Jane, but that is actually ten years ago. What is the date a month
ago in MM/DD/YYYY?
MODEL ANSWER (CORRECT): May 6, 1992 is ten years ago, so today is May 6, 2002. So a month ago will
be April 6, 2002. So the answer is 04/06/2002. ✓
QUESTION: This is the last day of 1899. What is the date 24 hours later in MM/DD/YYYY?
MODEL ANSWER (CORRECT): Today is 12/31/1899. 24 hours later will be 01/01/1900. So the answer is
01/01/1900. ✓
QUESTION: Jane scheduled 3 apointments with 5 poeple for tomorrow (Tue, 7/9/1972). What is the date one
week ago from today in MM/DD/YYYY?
MODEL ANSWER (INCORRECT): Tomorrow is 7/9/1972. One week ago from today is 7/2/1972. So the
answer is 7/2/1972.  (Ground truth is 07/01/1972.)
QUESTION: Today is the palindrome day of 2020, because the MMDDYYYY format of the date is the same
backwards as forwards. What is the date tomorrow in MM/DD/YYYY?
MODEL ANSWER (INCORRECT): Today is 02/29/2020. Tomorrow will be 03/01/2020. So the answer is
03/01/2020.  (Ground truth is “02/03/2020”.)
Table 18: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Sports
Understanding.
QUESTION: Is the following sentence plausible? “Joao Moutinho was out at third.”
MODEL ANSWER (CORRECT): Joao Moutinho is a soccer player. Being out at third is part of baseball, not
soccer. So the answer is no. ✓
QUESTION: Is the following sentence plausible? “Malcolm Brogdon eurostepped to the basket in the NBA
Championship.”
MODEL ANSWER (CORRECT): Malcolm Brogdon is a basketball player. Eurostepping to the basket is part
of basketball. So the answer is yes. ✓
QUESTION: Is the following sentence plausible? “Derrick White backhanded a shot.”
MODEL ANSWER (INCORRECT): Derrick White is a basketball player. Backhanding a shot is part of
basketball. So the answer is yes.  (Ground truth is “no”, backhanding a shot is part of hockey.)
QUESTION: Is the following sentence plausible? “Nazem Kadri was out at home.”
MODEL ANSWER (INCORRECT): Nazem Kadri is a hockey player. Being out at home is part of hockey.
So the answer is yes.  (Ground truth is “no”, being out at home is part of baseball.)
33
Table 19: Examples of correct and incorrect chains of thought produced by PaLM 540B on SayCan
robot planning tasks.
QUESTION: I spilled my coke on the table, could you throw it away and then bring me something to help clean?
MODEL ANSWER (CORRECT): Explanation: The user has spilled their coke on the table. I will throw away
the coke and then bring the user a sponge. Plan: ﬁnd(coke), pick(coke), ﬁnd(trash), put(coke), ﬁnd(sponge),
pick(sponge), ﬁnd(table), put(sponge). ✓
QUESTION: Could you compost that apple for me?
MODEL ANSWER (CORRECT): Explanation: The user would like me to compost the apple. I will move the
apple to the compost. Plan: ﬁnd(apple), pick(apple), ﬁnd(trash), put(apple). ✓
QUESTION: Can you bring me a drink that is not coke?
MODEL ANSWER (INCORRECT): Explanation: The user asked me to bring a drink that is not coke, I will
bring a sprite. Plan: ﬁnd(sprite), pick(sprite), ﬁnd(user), put(sprite).  (Feasible answers including ﬁnding lime
soda, pepsi, grapefruit soda or water, because there is no sprite in the scene as deﬁned in Table 28.)
QUESTION: Can you bring me something salty?
MODEL ANSWER (INCORRECT): Explanation: The user would like something salty. There are several
options in the objects list, so I will bring the user a lime soda. Plan: ﬁnd(lime soda), pick(lime soda), ﬁnd(user),
put(lime soda).  (Feasible answers including ﬁnding chips. The model misidentify lime soda as salty.)
34
G
Appendix: Full Prompts
Table 20: Few-shot exemplars for full chain of thought prompt for math word problems. This set of
exemplars was used for all math word problem datasets except AQuA.
PROMPT FOR MATH WORD PROBLEMS
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there
will be 21 trees. How many trees did the grove workers plant today?
A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have
been 21 - 15 = 6. The answer is 6.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they
had 74 - 35 = 39. The answer is 39.
Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did
Jason give to Denny?
A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8.
The answer is 8.
Q: Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he
have now?
A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9.
The answer is 9.
Q: There were nine computers in the server room. Five more computers were installed each day, from monday
to thursday. How many computers are now in the server room?
A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20
computers were added. 9 + 20 is 29. The answer is 29.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf
balls did he have at the end of wednesday?
A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he
had 35 - 2 = 33 golf balls. The answer is 33.
Q: Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23
- 15 is 8. The answer is 8.
35
Table 21: Few-shot exemplars for full chain of thought prompt for AQuA.
PROMPT FOR AQUA ALGEBRAIC WORD PROBLEMS
Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers
is?
Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64
A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be
50. The answer is (a).
Q: If a / b = 3/4 and 8a + 5b = 22,then ﬁnd the value of a.
Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2
A: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simpliﬁes to 8a + 20a / 3 = 22, which means 44a / 3
= 22. So a is equal to 3/2. The answer is (b).
Q: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then ﬁnd the distance?
Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
A: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (e).
Q: How many keystrokes are needed to type the numbers from 1 to 500?
Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788
A: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401
three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).
Table 22: Few-shot exemplars for full chain of thought prompt for the last letter concatenation task.
PROMPT FOR LAST LETTER CONCATENATION
Q: Take the last letters of the words in ""Elon Musk"" and concatenate them.
A: The last letter of ""Elon"" is ""n"". The last letter of ""Musk"" is ""k"". Concatenating them is ""nk"". The answer is nk.
Q: Take the last letters of the words in ""Larry Page"" and concatenate them.
A: The last letter of ""Larry"" is ""y"". The last letter of ""Page"" is ""e"". Concatenating them is ""ye"". The answer is ye.
Q: Take the last letters of the words in ""Sergey Brin"" and concatenate them.
A: The last letter of ""Sergey"" is ""y"". The last letter of ""Brin"" is ""n"". Concatenating them is ""yn"". The answer is
yn.
Q: Take the last letters of the words in ""Bill Gates"" and concatenate them.
A: The last letter of ""Bill"" is ""l"". The last letter of ""Gates"" is ""s"". Concatenating them is ""ls"". The answer is ls.
36
Table 23: Few-shot exemplars for full chain of thought prompt for the coinﬂip task.
PROMPT FOR COIN FLIP
Q: Q: A coin is heads up. Ka ﬂips the coin. Sherrie ﬂips the coin. Is the coin still heads up?
A: The coin was ﬂipped by Ka and Sherrie. So the coin was ﬂipped 2 times, which is an even number. The coin
started heads up, so after an even number of ﬂips, it will still be heads up. So the answer is yes.
Q: A coin is heads up. Jamey ﬂips the coin. Teressa ﬂips the coin. Is the coin still heads up?
A: The coin was ﬂipped by Jamey and Teressa. So the coin was ﬂipped 2 times, which is an even number. The
coin started heads up, so after an even number of ﬂips, it will still be heads up. So the answer is yes.
Q: A coin is heads up. Maybelle ﬂips the coin. Shalonda does not ﬂip the coin. Is the coin still heads up?
A: The coin was ﬂipped by Maybelle. So the coin was ﬂipped 1 time, which is an odd number. The coin started
heads up, so after an odd number of ﬂips, it will be tails up. So the answer is no.
Q: A coin is heads up. Millicent does not ﬂip the coin. Conception ﬂips the coin. Is the coin still heads up?
A: The coin was ﬂipped by Conception. So the coin was ﬂipped 1 time, which is an odd number. The coin
started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is no.
Q: A coin is heads up. Sal ﬂips the coin. Raymond does not ﬂip the coin. Is the coin still heads up?
A: The coin was ﬂipped by Sal. So the coin was ﬂipped 1 time, which is an odd number. The coin started heads
up, so after an odd number of ﬂips, it will be tails up. So the answer is no.
Q: A coin is heads up. Conception ﬂips the coin. Kristian does not ﬂip the coin. Is the coin still heads up?
A: The coin was ﬂipped by Conception. So the coin was ﬂipped 1 time, which is an odd number. The coin
started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is no.
Q: A coin is heads up. Inga does not ﬂip the coin. Elanor does not ﬂip the coin. Is the coin still heads up?
A: The coin was ﬂipped by no one. So the coin was ﬂipped 0 times. The coin started heads up, and it was not
ﬂipped, so it is still heads up. So the answer is yes.
Q: A coin is heads up. Ryan ﬂips the coin. Shaunda ﬂips the coin. Is the coin still heads up?
A: The coin was ﬂipped by Ryan and Shaunda. So the coin was ﬂipped 2 times, which is an even number. The
coin started heads up, so after an even number of ﬂips, it will still be heads up. So the answer is yes.
37
Table 24:
Few-shot exemplars for full chain of thought prompt for CSQA. There are newlines
between the answer choices that are omitted in the table for space reasons.
PROMPT FOR CSQA
Q: What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)
calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter
A: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to absorb ink.
So the answer is (e).
Q: What home entertainment equipment requires cable?
Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet
A: The answer must require cable. Of the above choices, only television requires cable. So the answer is (c).
Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty ﬂowers (b)
hen house (c) natural habitat (d) storybook
A: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. So the
answer is (b).
Q: Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas
(b) race track (c) desert (d) apartment (e) roadblock
A: The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of
people. So the answer is (a).
Q: Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super
market (d) fruit basket (e) fruit market
A: The answer should be the place where grocery items are placed before checking out. Of the above choices,
grocery cart makes the most sense for holding grocery items. So the answer is (b).
Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united
states (b) mexico (c) countryside (d) atlas
A: The answer must be something that used to do what Google Maps and GPS services do, which is to give
directions. Of the above choices, only atlases are used to give directions. So the answer is (d).
Q: Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)
anguish (c) bitterness (d) tears (e) sadness
A: The answer should be the feeling of someone getting divorced who was doing all the work. Of the above
choices, the closest feeling is bitterness. So the answer is (c).
38
Table 25: Few-shot exemplars for full chain of thought prompt for StrategyQA.
PROMPT FOR STRATEGYQA
Q: Do hamsters provide food for any animals?
A: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So
the answer is yes.
Q: Could Brooke Shields succeed at University of Pennsylvania?
A: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the
University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the
answer is yes.
Q: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?
A: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic
number squared is less than 5. So the answer is no.
Q: Yes or no: Is it common to see frost during some college commencements?
A: College commencement ceremonies can happen in December, May, and June. December is in the winter, so
there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?
A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6
months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Q: Yes or no: Would a pear sink in water?
A: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water ﬂoat. Thus,
a pear would ﬂoat. So the answer is no.
Table 26: Few-shot exemplars for full chain of thought prompt for Date Understanding.
PROMPT FOR DATE UNDERSTANDING
Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?
A: If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today
is 12/30/2014. So one week from today will be 01/05/2015. So the answer is 01/05/2015.
Q: The ﬁrst day of 2019 is a Tuesday, and today is the ﬁrst Monday of 2019. What is the date today in
MM/DD/YYYY?
A: If the ﬁrst day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the ﬁrst monday, would be six
days later. So today is 01/07/2019. So the answer is 01/07/2019.
Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10
days ago in MM/DD/YYYY?
A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the
answer is 05/23/1943.
Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?
A: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. So the answer is
04/20/1969.
Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours
later in MM/DD/YYYY?
A: Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. So the answer is 03/13/2002.
Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date
yesterday in MM/DD/YYYY?
A: The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so
today is 02/28/2017. So yesterday was 02/27/2017. So the answer is 02/27/2017.
39
Table 27: Few-shot exemplars for full chain of thought prompt for Sports Understanding.
PROMPT FOR SPORTS UNDERSTANDING
Q: Is the following sentence plausible? “Kyle Palmieri was called for slashing.”
A: Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. So the answer is yes.
Q: Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”
A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the
answer is no.
Q: Is the following sentence plausible? “Carson Wentz set the pick and roll.”
A: Carson Wentz is an American football player. Pick and roll is part of basketball, not football. So the answer
is no.
Q: Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”
A: Jonas Valanciunas is a basketball player. Beating the buzzer is part of basketball. So the answer is yes.
Q: Is the following sentence plausible? “Jamel Murray was perfect from the line.”
A: Jamal Murray is a basketball player. Being perfect from the line is part of basketball. So the answer is yes.
Q: Is the following sentence plausible? “Sam Darnold passed the puck.”
A: Sam Darnold is a American football player. Passing the puck is part of hockey, not American football. So the
answer is no.
Q: Is the following sentence plausible? “Draymond Green threw a touchdown.”
A: Draymond Green is an basketball player. Throwing a touchdown is part of football, not basketball. So the
answer is no.
Q: Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”
A: Malcolm Brogdon is a basketball player. Banking the shot in is part of basketball. So the answer is yes.
40
Table 28: Few-shot exemplars for full chain of thought prompt for SayCan robot planning tasks.
PROMPT FOR SAYCAN ROBOT PLANNING
Locations = [counter, table, user, trash, bowl].
Objects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,
grapefruit soda, pepsi, redbull, energy bar, sponge, water].
The robot can pick up items with pick(object) and put down items with put(object) as well as ﬁnd objects or
locations with ﬁnd(). The robot can only understand the explicit locations and objects listed.
Human: I’m hungry, can you bring me some chips.
Explanation: The user is hungry and has asked for chips. There are several types of chips available, I will bring
the user the kettle chips.
Plan: 1. ﬁnd(kettle chips), 2. pick(kettle chips), 3. ﬁnd(user), 4. put(kettle chips), 5. done().
Human: How would you move the grapefruit drink from the table to the counter?
Explanation: The user has asked me to move the grapefruit drink to the counter.
Plan: 1. ﬁnd(grapefruit soda), 2. pick(grapefruit soda), 3. ﬁnd(counter), 4. put(grapefruit soda), 5. done().
Human: How would you bring me some snacks?
Explanation: The user has asked for snacks, I will choose two items and bring them. I will bring jalapeno chips
and an apple.
Plan: 1. ﬁnd(jalapeno chips), 2. pick(jalapeno chips), 3. ﬁnd(user), 4. put(jalapeno chips), 5. ﬁnd(apple), 6.
pick(apple), 7. ﬁnd(user), 8. put(apple), 9. done().
Human: How would you bring me something to eat that isn’t a fruit?
Explanation: The user has asked for a food that isn’t a fruit, I will bring an energy bar to them.
Plan: 1. ﬁnd(energy bar), 2. pick(energy bar), 3. ﬁnd(user), 4. put(energy bar), 5. done().
Human: How would you put the rice chips in the bowl and then move the tea to the table?
Explanation: The user has asked me to do two tasks, I will do one and then the other.
Plan: 1. ﬁnd(rice chips), 2. pick(rice chips), 3. ﬁnd(bowl), 4. put(rice chips), 5. ﬁnd(tea), 6. pick(tea), 7.
ﬁnd(table), 8. put(tea), 9. done().
Human: How would you throw away a redbull?
Explanation: The user has asked me to throw away the redbull, I will move it to the trash.
Plan: 1. ﬁnd(redbull), 2. pick(redbull), 3. ﬁnd(trash), 4. put(redbull), 5. done().
Human: Bring me a drink.
Explanation: The user has asked for a drink and there are many options. I will bring them a water.
Plan: 1. ﬁnd(water), 2. pick(water), 3. ﬁnd(user), 4. put(water), 5. done().
41
H
Appendix: Alternate Annotators for MWP
Table 29: Few-shot exemplars for full chain of thought prompt for math word problems. These
exemplars are the same as in Table 20, except that the chains of thought were written by a different
annotator (“Annotator B” instead of “Annotator A”). Annotators were co-authors and familiar with
the goal of chain of thought prompting.
PROMPT FOR MATH WORD PROBLEMS
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there
will be 21 trees. How many trees did the grove workers plant today?
A: There are 21 trees now and there are 15 trees in the beginning, so the workers plant 21 - 15 = 6 trees. The
answer is 6.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are 3 cars in the beginning, 2 more arrive, so now there should be 3 + 2 = 5 cars. The answer is 5.
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Leah had 32 chocolates and her sister had 42, in total they have 32 + 42 = 74 chocolates. After they ate 35,
now there are 74 - 35 = 39 chocolates. The answer is 39.
Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did
Jason give to Denny?
A: Jason started with 20 lollipops, but now he only has 12, so he gave Denny 20 - 12 = 8 lollipops. The answer
is 8.
Q: Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he
have now?
A: Shawn got 2 toys each from his mom and dad, so he got 2 * 2 = 4 more, now he will have 5 + 4 = 9 toys. The
answer is 9.
Q: There were nine computers in the server room. Five more computers were installed each day, from monday
to thursday. How many computers are now in the server room?
A: 5 computers were installed from monday to thursday, so in total 5 * 4 = 20 computers are installed. 9
computers are there in the beginning, so now there are 20 + 9 = 29 computers. The answer is 29.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf
balls did he have at the end of wednesday?
A: Michael started with 58 golf balls and lost 23, so he has 58 - 23 = 35. After he lost 2 more, he has 35 - 2 = 33
balls now. The answer is 33.
Q: Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A: 5 bagels for $3 each should cost 5 * 3 = 15 dollars. Olivia had $23 in the beginning, so now she has 23 - 15 =
8 dollars left. The answer is 8.
42
Table 30: Few-shot exemplars for full chain of thought prompt for math word problems. These
exemplars are the same as in Table 20, except that the chains of thought were written by a different
annotator (“Annotator C” instead of “Annotator A”).
PROMPT FOR MATH WORD PROBLEMS
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there
will be 21 trees. How many trees did the grove workers plant today?
A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So,
they must have planted 21 - 15 = 6 trees. The answer is 6.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74 chocolates.
35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.
Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did
Jason give to Denny?
A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of
lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.
Q: Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he
have now?
A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in
total he has 7 + 2 = 9 toys. The answer is 9.
Q: There were nine computers in the server room. Five more computers were installed each day, from monday
to thursday. How many computers are now in the server room?
A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20
computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The
answer is 29.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf
balls did he have at the end of wednesday?
A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday
he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.
Q: Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in beginning,
so now she has $23 - $15 = $8. The answer is 8.
43
</Content></Document>
<Document index=1><Published>2023-05-26</Published><Title>Towards Reasoning in Large Language Models: A Survey</Title><Authors>Jie Huang, Kevin Chen-Chuan Chang</Authors><Summary>Reasoning is a fundamental aspect of human intelligence that plays a crucial
role in activities such as problem solving, decision making, and critical
thinking. In recent years, large language models (LLMs) have made significant
progress in natural language processing, and there is observation that these
models may exhibit reasoning abilities when they are sufficiently large.
However, it is not yet clear to what extent LLMs are capable of reasoning. This
paper provides a comprehensive overview of the current state of knowledge on
reasoning in LLMs, including techniques for improving and eliciting reasoning
in these models, methods and benchmarks for evaluating reasoning abilities,
findings and implications of previous research in this field, and suggestions
on future directions. Our aim is to provide a detailed and up-to-date review of
this topic and stimulate meaningful discussion and future work.</Summary><Source>https://arxiv.org/pdf/2212.10403.pdf</Source><Content>Towards Reasoning in Large Language Models: A Survey
Jie Huang
Kevin Chen-Chuan Chang
Department of Computer Science, University of Illinois at Urbana-Champaign
{jeffhj, kcchang}@illinois.edu
Abstract
Reasoning is a fundamental aspect of human
intelligence that plays a crucial role in activi-
ties such as problem solving, decision making,
and critical thinking. In recent years, large
language models (LLMs) have made signifi-
cant progress in natural language processing,
and there is observation that these models may
exhibit reasoning abilities when they are suf-
ficiently large. However, it is not yet clear to
what extent LLMs are capable of reasoning.
This paper provides a comprehensive overview
of the current state of knowledge on reasoning
in LLMs, including techniques for improving
and eliciting reasoning in these models, meth-
ods and benchmarks for evaluating reasoning
abilities, findings and implications of previous
research in this field, and suggestions on future
directions. Our aim is to provide a detailed and
up-to-date review of this topic and stimulate
meaningful discussion and future work.1
1
Introduction
Reasoning is a cognitive process that involves using
evidence, arguments, and logic to arrive at conclu-
sions or make judgments. It plays a central role in
many intellectual activities, such as problem solv-
ing, decision making, and critical thinking. The
study of reasoning is important in fields like psy-
chology (Wason and Johnson-Laird, 1972), philoso-
phy (Passmore, 1961), and computer science (Huth
and Ryan, 2004), as it helps individuals make deci-
sions, solve problems, and think critically.
Recently,
large
language
models
(LLMs)
(Brown et al., 2020; Chowdhery et al., 2022; Chung
et al., 2022; OpenAI, 2022, inter alia) such as Chat-
GPT have made significant advancements in natu-
ral language processing and related fields. It has
been shown that these models exhibit emergent be-
haviors, including the ability to “reason”, when
1Paperlist
can
be
found
at
https://github.com/
jeffhj/LM-reasoning.
they are large enough (Wei et al., 2022a). For ex-
ample, by providing the models with “chain of
thoughts”, i.e., reasoning exemplars, or a simple
prompt “Let’s think step by step”, these models
are able to answer questions with explicit reason-
ing steps (Wei et al., 2022b; Kojima et al., 2022),
e.g., “all whales are mammals, all mammals have
kidneys; therefore, all whales have kidneys.” This
has sparked considerable interest in the commu-
nity since reasoning ability is a hallmark of human
intelligence that is frequently considered missed
in current artificial intelligence systems (Marcus,
2020; Russin et al., 2020; Mitchell, 2021; Bom-
masani et al., 2021).
However, despite the strong performance of
LLMs on certain reasoning tasks, it remains unclear
whether LLMs are actually reasoning and to what
extent they are capable of reasoning. For exam-
ple, Kojima et al. (2022) claim that “LLMs are de-
cent zero-shot reasoners (p. 1)”, while Valmeekam
et al. (2022) conclude that “LLMs are still far
from achieving acceptable performance on com-
mon planning/reasoning tasks which pose no issues
for humans to do (p. 2).” This limitation is also
stated by Wei et al. (2022b):
“we qualify that although chain of thought emu-
lates the thought processes of human reasoners,
this does not answer whether the neural network
is actually reasoning (p. 9).”
Therefore, in this paper, we aim to provide a
comprehensive overview and engage in an insight-
ful discussion on the current state of knowledge on
this fast-evolving topic. We initiate our exploration
with a clarification of the concept of reasoning (§2).
Subsequently, we turn our attention to the tech-
niques for enhancing/eliciting reasoning in LLMs
(§3), the methods and benchmarks for evaluating
reasoning in LLMs (§4), and the key findings and
implications in this field (§5). Finally, we reflect
on and discuss the current state of the field (§6).
arXiv:2212.10403v2  [cs.CL]  26 May 2023
Reasoning in LLMs
Techniques (§3)
Fully Supervised Finetuning (§3.1)
Prompting & In-Context Learning (§3.2)
Chain of Thought and Its Variants (§3.2.1)
Rationale Engineering (§3.2.2)
Problem Decomposition (§3.2.3)
Others (§3.2.4)
Hybrid Method (§3.3)
Reasoning-Enhanced Training & Prompting (§3.3.1)
Bootstrapping & Self-Improving (§3.3.2)
Evaluation & Analysis (§4)
End Task Performance (§4.1)
Analysis on Reasoning (§4.2)
Findings & Implications (§5)
Reflection, Discussion & Future Directions (§6)
Figure 1: The structure of the paper.
2
What is Reasoning?
Reasoning is the process of thinking about some-
thing in a logical and systematic way, using evi-
dence and past experiences to reach a conclusion or
make a decision (Wason and Johnson-Laird, 1972;
Wason, 1968; Galotti, 1989; Fagin et al., 2004;
McHugh and Way, 2018). Reasoning involves mak-
ing inferences, evaluating arguments, and drawing
logical conclusions based on available information.
Although “reasoning” is a term that is commonly
used in literature and daily life, it is also an abstract
concept that can refer to many things. To help the
reader better understand this concept, we summa-
rize several main categories of reasoning that are
commonly recognized:
Deductive reasoning. Deductive reasoning is a
type of reasoning in which a conclusion is drawn
based on the truth of the premises. In deductive
reasoning, the conclusion must necessarily follow
from the premises, meaning that if the premises are
true, the conclusion must also be true. For example:
• Premise: All mammals have kidneys.
• Premise: All whales are mammals.
• Conclusion: All whales have kidneys.
Inductive reasoning. Inductive reasoning is a type
of reasoning in which a conclusion is drawn based
on observations or evidence. The conclusion is
likely to be true based on the available evidence,
but it is not necessarily certain. For example:
• Observation: Every time we see a creature
with wings, it is a bird.
• Observation: We see a creature with wings.
• Conclusion: The creature is likely to be a bird.
Abductive reasoning. Abductive reasoning is a
type of reasoning in which a conclusion is drawn
based on the best explanation for a given set of
observations. The conclusion is the most likely
explanation based on the available evidence, but it
is not necessarily certain. For example:
• Observation: The car cannot start and there is
a puddle of liquid under the engine.
• Conclusion: The most likely explanation is
that the car has a leak in the radiator.
Other types of reasoning include analogical reason-
ing, which involves making comparisons between
two or more things in order to make inferences
or arrive at conclusions; causal reasoning, which
involves identifying and understanding the causes
and effects of events or phenomena; and probabilis-
tic reasoning, which involves making decisions or
arriving at conclusions based on the likelihood or
probability of certain outcomes.
Formal Reasoning vs Informal Reasoning. For-
mal reasoning is a systematic and logical process
that follows a set of rules and principles, often used
in mathematics and logic. Informal reasoning is a
less structured approach that relies on intuition, ex-
perience, and common sense to draw conclusions
and solve problems, and is often used in everyday
life. Formal reasoning is more structured and reli-
able, while informal reasoning is more adaptable
and open-ended, but may also be less reliable. We
refer the reader to Galotti (1989); Bronkhorst et al.
(2020) for a detailed distinction between them.
Reasoning in Language Models. The concept of
reasoning in language models has been around for
some time, but there is not a clear definition of
what it entails. In the literature, the term “reason-
ing” is often used to refer to informal reasoning,
although it is not always explicitly stated that it
is informal (Cobbe et al., 2021; Wei et al., 2022b,
inter alia). Different forms of reasoning may be
used depending on the task, benchmark, or method
being used, e.g., deductive reasoning (Cobbe et al.,
2021; Creswell et al., 2022; Han et al., 2022b, in-
ter alia), inductive reasoning (Yang et al., 2022;
Misra et al., 2022, inter alia) or abductive reason-
ing (Wiegreffe et al., 2022; Lampinen et al., 2022;
Jung et al., 2022, inter alia). In this paper, we
encompass various forms of reasoning, with a par-
ticular focus on “informal deductive reasoning” in
large language models since it is a widely used
form in which the conclusion is guaranteed to be
true as long as the premises are true.
3
Towards Reasoning in Large Language
Models
Reasoning, particularly multi-step reasoning, is of-
ten seen as a weakness in language models and
other NLP models (Bommasani et al., 2021; Rae
et al., 2021; Valmeekam et al., 2022). Recent re-
search has suggested that reasoning ability may
emerge in language models at a certain scale, such
as models with over 100 billion parameters (Wei
et al., 2022a,b; Cobbe et al., 2021). In this paper,
we follow Wei et al. (2022a) in considering rea-
soning as an ability that is rarely present in small-
scale models like GPT-2 (Radford et al., 2019) and
BERT (Devlin et al., 2019), and therefore focus
on techniques applicable to improving or eliciting
“reasoning”2 in LLMs such as GPT-3 (Brown et al.,
2020) and PaLM (Chowdhery et al., 2022).
3.1
Fully Supervised Finetuning
Before discussing reasoning in large language mod-
els, it is worth mentioning there is research work-
ing on eliciting/improving reasoning in small lan-
guage models through fully supervised finetuning
on specific datasets. For example, Rajani et al.
(2019) finetune a pretrained GPT model (Radford
et al., 2018) to generate rationales that explain
model predictions with the built CoS-E dataset,
and find that models trained with explanations
perform better on commonsense question answer-
ing tasks (Talmor et al., 2019).
Talmor et al.
(2020) train RoBERTa (Liu et al., 2019) to per-
form reasoning/inference based on both implicit
pre-trained knowledge and explicit free-text state-
ments. Hendrycks et al. (2021) finetune pretrained
2It is important to note that the term “reasoning” in this
paper does not necessarily imply that LLMs are truly capable
of reasoning or that they are able to reason in the same way
that humans do. We will discuss this issue in more detail in §6.
language models to solve competition mathematics
problems by generating full step-by-step solutions,
though the accuracy is relatively low. Nye et al.
(2022) train language models to do multi-step rea-
soning for program synthesis/execution by generat-
ing “scratchpads”, i.e., intermediate computations,
before producing the final answers. We refer the
reader to Helwe et al. (2021); Bhargava and Ng
(2022)’s survey for more studies in this line.
There are two major limitations of fully super-
vised finetuning. First, it requires a dataset contain-
ing explicit reasoning, which can be difficult and
time-consuming to create. Additionally, the model
is only trained on a specific dataset, which limits
its application to a specific domain and may result
in the model relying on artifacts in the training data
rather than actual reasoning to make predictions.
3.2
Prompting & In-Context Learning
Large language models such as GPT-3 (Brown
et al., 2020) have demonstrated remarkable few-
shot performance across a variety of tasks through
in-context learning. These models can be prompted
with a question and a few ⟨input, output⟩exemplars
to potentially solve a problem through “reasoning”,
either implicitly or explicitly. However, research
has shown that these models still fall short when
it comes to tasks that require multiple steps of rea-
soning to solve (Bommasani et al., 2021; Rae et al.,
2021; Valmeekam et al., 2022). This may be due
to a lack of exploration into the full capabilities of
these models, as recent studies have suggested.
3.2.1
Chain of Thought and Its Variants
To encourage LLMs to engage in reasoning rather
than simply providing answers directly, we may
guide LLMs to generate “reasoning” explicitly.
One approach for doing this is chain-of-thought
prompting, proposed by Wei et al. (2022b). This
approach involves providing a few examples of
“chain of thought” (CoT), which are intermediate
natural language reasoning steps, in the prompt to
LLMs (Figure 2). Specifically, in CoT prompting,
⟨input, output⟩demonstrations are replaced with
⟨input, chain of thought, output⟩triples, e.g., “[in-
put] Roger has 5 tennis balls. He buys 2 more cans
of tennis balls. Each can has 3 tennis balls. How
many tennis balls does he have now? [chain of
thought] Roger started with 5 balls. 2 cans of 3
tennis balls each is 6 tennis balls. 5 + 6 = 11. [out-
put] The answer is 11.” In this way, given a target
question, the model learns to generate explicit ratio-
LLM
Rationale* 1
Input A, Rationale A, Output A
Output*
Exemplars
Input B, Rationale B, Output B
Input C, Rationale C, Output C
Rationale Refinement
Rationale* 2
Rationale* 3
Rationale Exploration
Rationale Verification
[Input] Roger has 5 tennis balls. He buys 2 more cans of tennis
balls. Each can has 3 tennis balls. How many tennis balls does he
have now? [Rationale] Roger started with 5 balls. 2 cans of 3 tennis
balls each is 6 tennis balls. 5 + 6 = 11. [Output] The answer is 11.
Input*
Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the
target problem to be solved.
nale before producing the final answer. Experimen-
tal results show that this simple idea can improve
LLMs’ few-shot performance on arithmetic, sym-
bolic, and commonsense reasoning tasks, some-
times to a striking degree.
There are several variants of chain-of-thought
prompting that have been proposed in the literature,
in a different form or to solve a specific problem.
Different Form:
Kojima et al. (2022) intro-
duce Zero-shot-CoT, in which LLMs are simply
prompted with the phrase “Let’s think step by step”
after the input, in order to elicit reasoning without
the need for few-shot demonstrations. Madaan et al.
(2022); Gao et al. (2022); Chen et al. (2022) find
that LLMs trained with code, e.g., Codex (Chen
et al., 2021), can achieve better performance on
reasoning tasks by framing reasoning as code gen-
eration. Wang et al. (2022a) propose to iteratively
prompt chain of thought. He et al. (2023) attempt
to retrieve external knowledge in CoT to improve
faithfulness of reasoning.
Specific Problem/Setting:
Before chain of
thought, Nye et al. (2022) also try to use intermedi-
ate computations, named “scratchpads”, to improve
language models’ reasoning performance in both
finetuning and few-shot regimes, with a particular
focus on programs. Shi et al. (2022) attempt to
solve multilingual reasoning tasks with CoT in the
native language, CoT in English (regardless of the
problem language), and CoT in English (with the
problem translated to English). Chen (2022) apply
CoT to table-based reasoning, finding that LLMs
can achieve strong performance on table tasks with
only one exemplar. Prystawski et al. (2022) demon-
strate that CoT can improve LLMs’ performance
on paraphrase selection for metaphors. Lu et al.
(2022) apply chain of thought to solve multimodal
science questions.
3.2.2
Rationale Engineering
The original version of chain-of-thought prompting,
proposed by Wei et al. (2022b), relies on manually
crafted examples of intermediate reasoning steps
and applies greedy decoding in the generation. Ra-
tionale engineering aims to more effectively elicit
or utilize reasoning in LLMs. This can be achieved
through rationale refinement, which involves cre-
ating more effective examples of reasoning steps,
or through rationale exploration and rationale ver-
ification, which involve exploring and verifying
the rationales produced by LLMs. A summary of
raltionale engineering is illustrated in Figure 2.
Rationale refinement. The choice of exemplars
can significantly affect the few-shot performance of
LLMs, as demonstrated in research such as Liu et al.
(2022b), which also appears in chain-of-thought
prompting. Rationale refinement aims to create
and refine rationale examples that are better able to
elicit reasoning in LLMs. Fu et al. (2022b) propose
complexity-based prompting to create rationales
with more reasoning steps. Their experiments show
that the performance of LLMs improves with the in-
creased rationale complexity. Similarly, Zhou et al.
(2022c) propose algorithmic prompting, which sug-
gests that providing more thorough examples of
solutions can help improve reasoning performance
on some simple math calculations. Zhang et al.
(2022b) design Auto-CoT to automatically con-
struct exemplars by partitioning questions from
a given dataset into clusters and then using Zero-
Shot-CoT (Kojima et al., 2022) to generate the
rationale for a representative question from each
cluster. The analysis shows that making exemplars
diverse is important in prompting LLMs to produce
better rationales.
Rationale exploration. In addition to providing
better exemplars, we can allow LLMs to fully ex-
plore various ways of reasoning to improve their
performance on reasoning tasks, named rationale
exploration. Based on the idea that complex prob-
lems often admit multiple ways of thinking that
can lead to their unique correct answer, Wang et al.
(2022c) present a decoding strategy called self-
consistency to improve upon the traditional greedy
decoding used in chain-of-thought prompting. This
strategy involves sampling a diverse set of ratio-
nales, rather than just the greedy one, and selecting
the most consistent answer by marginalizing out
the sampled rationales. The idea is also used in
Fu et al. (2022b) to vote over the top complex ra-
tionales. To further improve performance, Li et al.
(2022b) suggest providing different demonstrations
for each question by sampling exemplars from an
exemplar base, in order to increase the diversity of
the sampled rationales.
Rationale verification. Ensuring that the ratio-
nales produced by LLMs are valid is critical, as in-
correct rationales can lead to incorrect final predic-
tions (Ye and Durrett, 2022). To address this issue,
the process of rationale verification aims to verify
whether the rationales produced by LLMs lead to
the correct final answers. Cobbe et al. (2021) pro-
pose augmenting LLMs with a trained verifier that
assigns a score to each rationale and solution gen-
erated by the LLM, selecting the highest-ranked
solution as the final answer when solving math
word problems. Li et al. (2022b) also use this tech-
nique to guide rationale selection, in conjunction
with the process of rationale exploration. Differ-
ent from the above methods that train an external
verifier to verify the rationales, Weng et al. (2022)
suggest using LLMs themselves as the verifiers.
3.2.3
Problem Decomposition
Chain-of-thought prompting, while effective for
eliciting reasoning in LLMs, can struggle with com-
plex tasks, e.g., tasks that require compositional
generalization (Lake and Baroni, 2018; Keysers
et al., 2020). To solve a complex problem, it is
helpful to first break it down into smaller, more
manageable subproblems. By solving each of these
subproblems, we can effectively solve the complex
problem. This technique is called problem decom-
position or divide and conquer (Talmor and Berant,
2018; Min et al., 2019; Perez et al., 2020).
Based on this idea, Zhou et al. (2022a) pro-
pose least-to-most prompting, which consists of
two steps: decomposing the complex problem into
subproblems and solving these subproblems in a
specific order, with each subproblem being facil-
itated by the answers obtained from previously
solved subproblems. As follow-up work, Droz-
dov et al. (2022) introduce dynamic least-to-most
prompting, which is designed to solve more realis-
tic semantic parsing problems by decomposing the
problems with prompting-based syntactic parsing
and dynamically selecting exemplars based on the
decomposition. In addition, Khot et al. (2022) de-
sign decomposed prompting, which breaks down
a complex problem into subproblems that can be
handled by a shared library of prompting-based
LLMs, each specialized in a particular subprob-
lem. Furthermore, Dua et al. (2022) develop suc-
cessive prompting, which iteratively decomposes a
complex problem into a simple problem, with the
next subproblem prediction having access to the
answers to the previous subproblems. While the
above methods decompose or solve compositional
questions with multiple forward passes, Press et al.
(2022) suggest decomposing and solving the input
question in one forward pass using CoT prompting.
Overall, these techniques show promise for helping
LLMs to solve complex tasks by decomposing the
problem into more manageable subproblems.
3.2.4
Others
There are other techniques that have been devel-
oped to facilitate reasoning in LLMs for specific
tasks or settings.
For instance, Creswell et al.
(2022); Creswell and Shanahan (2022) introduce a
selection-inference framework that uses LLMs as
modules to select and infer reasoning steps from
a set of facts that culminate in the final answer.
Kazemi et al. (2022) suggest using backward chain-
ing, i.e., from goal to the set of facts that support
it, instead of forward chaining like Creswell et al.
(2022); Creswell and Shanahan (2022). In addition,
Jung et al. (2022) propose a method for solving
binary questions by prompting LLMs abductively
and recursively to rationalize each option. Zhou
et al. (2022b) design a technique for performing
numerical reasoning on complex numbers by re-
placing the complex numbers with simple numbers
to produce simpler expressions, and then using
these expressions to perform calculations on the
complex numbers. There are also efforts to distill
reasoning from LLMs into smaller models, such
as the work by Li et al. (2022a); Shridhar et al.
(2022); Magister et al. (2022). Finally, we refer the
reader to Dohan et al. (2022)’s position paper on
language model cascade, which presents a unify-
ing framework for understanding chain-of-thought
prompting and research in this line.
3.3
Hybrid Method
While “prompting” techniques can help elicit or
better utilize reasoning in large language models
to solve reasoning tasks, they do not actually im-
prove the reasoning capabilities of the LLMs them-
selves, as the parameters of the models remain un-
changed. In contrast, the “hybrid approach” aims to
simultaneously improve the reasoning capabilities
of LLMs and make better use of these models in
order to solve complex problems. This approach in-
volves both enhancing the reasoning capabilities of
the LLMs and using techniques such as prompting
to effectively utilize these capabilities.
3.3.1
Reasoning-Enhanced Training and
Prompting
One approach to improving the reasoning capabili-
ties of LLMs is to pretrain or finetune the models
on datasets that include “reasoning”. Lewkowycz
et al. (2022); Taylor et al. (2022) find that LLMs
trained on datasets containing scientific and math-
ematical data can achieve better performance on
reasoning tasks like quantitative reasoning prob-
lems when using CoT prompting3. Pi et al. (2022)
show that continually pretraining with SQL data
can boost the performance of language models, e.g.,
T5 (Raffel et al., 2020), on natural language rea-
soning such as numerical reasoning and logical rea-
soning. Furthermore, Chung et al. (2022) develop
Flan models by finetuning PaLM (Chowdhery et al.,
2022) and T5 (Raffel et al., 2020) with 1.8k fine-
tuning tasks, including CoT data, and find that
CoT data are critical to keeping reasoning abilities.
Similarly, Yu et al. (2022) finetune OPT (Zhang
et al., 2022a) on 10 reasoning datasets and observe
that it can improve some reasoning capabilities of
LLMs. Anil et al. (2022) study the length gener-
alization abilities of LLMs, i.e., whether LLMs
learned with short problem instances can general-
ize to long ones. They discover that the combina-
tion of few-shot scratchpad (or chain of thought)
3This may also be true for models trained with code (Chen
et al., 2021; Fu et al., 2022a).
finetuning and scratchpad prompting results in a
significant improvement in LLMs’ ability to gener-
alize to longer problems, while this phenomenon
is not observed in the standard fully supervised
finetuning paradigm.
3.3.2
Bootstrapping & Self-Improving
Instead of finetuning LLMs on pre-built datasets
that include reasoning, there are studies that have
explored the idea of using LLMs to self-improve
their reasoning abilities through a process known
as bootstrapping. One example of this is the Self-
Taught Reasoner (STaR) introduced by Zelikman
et al. (2022), in which a LLM is trained and refined
on its own output iteratively. Specifically, with CoT
prompting, the model first generates initial ratio-
nales. And then, the model is finetuned on ratio-
nales that lead to correct answers. This process can
be repeated, with each iteration resulting in an im-
proved model that can generate better training data,
which in turn leads to further improvements. As a
follow-up to this work, Huang et al. (2022a) show
that LLMs are able to self-improve their reasoning
abilities without the need for supervised data by
leveraging the self-consistency of reasoning (Wang
et al., 2022c).
4
Measuring Reasoning in Large
Language Models
We summarize methods and benchmarks for evalu-
ating reasoning abilities of LLMs in this section.
4.1
End Task Performance
One way to measure reasoning abilities of LLMs is
to report their performance, e.g., accuracy, on end
tasks that require reasoning. We list some common
benchmarks as follows.
Arithmetic Reasoning. Arithmetic reasoning is
the ability to understand and apply mathemat-
ical concepts and principles in order to solve
problems involving arithmetic operations. This
involves using logical thinking and mathemat-
ical principles to determine the correct course
of action when solving mathematical problems.
Representative benchmarks for arithmetic rea-
soning include GSM8K (Cobbe et al., 2021),
Math (Hendrycks et al., 2021), MathQA (Amini
et al., 2019), SVAMP (Patel et al., 2021), AS-
Div (Miao et al., 2020), AQuA (Ling et al., 2017),
and MAWPS (Roy and Roth, 2015). It is worth
mentioning that Anil et al. (2022) generate the Par-
ity Datasets and the Boolean Variable Assignment
Dataset for analyzing the length generalization ca-
pabilities of LLMs (§3.3.1).
Commonsense Reasoning. Commonsense Rea-
soning is the use of everyday knowledge and under-
standing to make judgments and predictions about
new situations. It is a fundamental aspect of human
intelligence that enables us to navigate our envi-
ronment, understand others, and make decisions
with incomplete information. Benchmarks that can
be used for testing commonsense reasoning abili-
ties of LLMs include CSQA (Talmor et al., 2019),
StrategyQA (Geva et al., 2021), and ARC (Clark
et al., 2018). We refer the reader to Bhargava and
Ng (2022)’s survey for more work in this domain.
Symbolic Reasoning. Symbolic reasoning is a
form of reasoning that involves the manipulation
of symbols according to formal rules. In symbolic
reasoning, we use abstract symbols to represent
concepts and relationships, and then manipulate
those symbols according to precise rules in order
to draw conclusions or solve problems. Two bench-
marks of symbolic reasoning are presented in Wei
et al. (2022b), including Last Letter Concatenation
and Coin Flip.
Others. In practice, there are many benchmarks
that can be used to evaluate reasoning abilities
of LLMs (indirectly), as long as the downstream
task involves reasoning. BIG-bench (Srivastava
et al., 2022), for example, includes over 200 tasks
that test a range of reasoning skills, including
tasks like Date Understanding, Word Sorting, and
Causal Judgement. Other benchmarks, such as
SCAN (Lake and Baroni, 2018) and the one pro-
posed by Anil et al. (2022), focus on evaluating
generalization ability. LLMs can also be tested on
their table reasoning abilities using benchmarks
such as WikiTableQA (Pasupat and Liang, 2015),
FetaQA (Nan et al., 2022), as suggested by Chen
(2022). In addition, there are benchmarks for eval-
uating LLMs’ generative relational reasoning abil-
ities, such as CommonGen (Lin et al., 2020; Liu
et al., 2022a) and Open Relation Modeling (Huang
et al., 2022b,d).
4.2
Analysis on Reasoning
Although LLMs have demonstrated impressive per-
formance on various reasoning tasks, the extent to
which their predictions are based on true reasoning
or simple heuristics is not always clear. This is
because most existing evaluations focus on their ac-
curacy on end tasks, rather than directly assessing
their reasoning steps. While some error analysis
has been conducted on the generated rationales of
LLMs (Wei et al., 2022b; Kojima et al., 2022, inter
alia), this analysis has often been limited in depth.
There have been some efforts to develop metrics
and benchmarks that enable a more formal/deep
analysis of reasoning in LLMs. Golovneva et al.
(2022) design ROSCOE, a set of interpretable, de-
tailed step-by-step evaluation metrics covering vari-
ous perspectives including semantic alignment, log-
ical inference, semantic similarity, and language
coherence. Saparov and He (2022) create a syn-
thetic dataset called PrOntoQA that is generated
from real or fictional ontologies. Each example
in the dataset has a unique proof, which can be
converted to simple sentences and back again, al-
lowing for a formal analysis of each reasoning step.
Han et al. (2022a) introduce a dataset called FO-
LIO to test the first-order logic reasoning capabil-
ities of LLMs. FOLIO contains first-order logic
reasoning problems that require models to deter-
mine the correctness of conclusions given a set of
premises. In addition, Wang et al. (2022b) conduct
ablation experiments on CoT and find that LLMs
may also perform reasoning while prompting with
invalid rationals. Their study also suggests that be-
ing relevant to the query and correctly ordering the
reasoning steps are important for CoT prompting.
In summary, most existing studies primarily re-
port the performance of the models on downstream
reasoning tasks, without a detailed examination of
the quality of the rationales produced. This leaves
open the question of whether the models are ac-
tually able to reason in a way that is similar to
human reasoning, or whether they are simply able
to achieve good performance on the tasks through
other means. Further research is needed to more
formally analyze the reasoning abilities of LLMs.
5
Findings and Implications
In this section, we summarize the important find-
ings and implications of studies on reasoning in
large language models.
Reasoning seems an emergent ability of LLMs.
Wei et al. (2022a,b); Suzgun et al. (2022) show that
reasoning ability appears to emerge only in large
language models like GPT-3 175B, as evidenced by
significant improvements in performance on rea-
soning tasks at a certain scale (e.g., 100 billion
parameters). This suggests that it may be more ef-
fective to utilize large models for general reasoning
problems rather than training small models for spe-
cific tasks. However, the reason for this emergent
ability is not yet fully understood. We refer the
reader to Wei et al. (2022a); Fu et al. (2022a) for
some potential explanations.
Chain of thought elicits “reasoning” of LLMs.
The use of chain-of-thought (CoT) prompts (Wei
et al., 2022b) has been shown to improve the per-
formance of LLMs on various reasoning tasks,
as demonstrated in the experiments of Wei et al.
(2022a,b); Suzgun et al. (2022).
Additionally,
Saparov and He (2022) (§4.2) find that, when us-
ing CoT prompts, LLMs are able to produce valid
individual proof steps, even when the synthetic on-
tology is fictional or counterfactual. However, they
may sometimes choose the wrong steps when mul-
tiple options are available, leading to incomplete
or incorrect proofs. Moreover, for many reasoning
tasks where the performance of standard prompting
grows smoothly with model scale, chain-of-thought
prompting can lead to dramatic performance im-
provement. In addition to these benefits, the use of
CoT prompts has been shown to improve the out-of-
distribution robustness of LLMs (Wei et al., 2022b;
Zhou et al., 2022a; Anil et al., 2022, inter alia),
an advantage that is not typically observed with
standard prompting or fully supervised finetuning
paradigms.
LLMs show human-like content effects on rea-
soning. According to Dasgupta et al. (2022), LLMs
exhibit reasoning patterns that are similar to those
of humans as described in the cognitive literature.
For example, the models’ predictions are influ-
enced by both prior knowledge and abstract rea-
soning, and their judgments of logical validity are
impacted by the believability of the conclusions.
These findings suggest that, although language
models may not always perform well on reasoning
tasks, their failures often occur in situations that
are challenging for humans as well. This provides
some evidence that language models may “reason”
in a way that is similar to human reasoning.
LLMs are still unskilled at complex reasoning.
Although LLMs seem to possess impressive rea-
soning capabilities with the techniques described
in §3, they still struggle with more complex rea-
soning tasks or those involving implicature, accord-
ing to studies such as Valmeekam et al. (2022);
Han et al. (2022a); Ruis et al. (2022).
For in-
stance, Valmeekam et al. (2022) find that even in
relatively simple commonsense planning domains
that humans would have no trouble navigating,
LLMs such as GPT-3 (Brown et al., 2020) and
BLOOM (Scao et al., 2022) struggle to perform
effectively. These findings suggest that existing
benchmarks may be too simple to accurately gauge
the true reasoning abilities of LLMs, and that more
challenging tasks may be needed to fully evaluate
their abilities in this regard.
6
Reflection, Discussion, and Future
Directions
Why reasoning? Reasoning is the process of think-
ing about something in a logical and systematic
way, and it is a key aspect of human intelligence.
By incorporating reasoning capabilities into lan-
guage models, we can enable them to perform tasks
that require more complex and nuanced thinking,
such as problem solving, decision making, and
planning (Huang et al., 2022e,f; Song et al., 2022).
This can improve the performance of these mod-
els on downstream tasks and increase their out-of-
distribution robustness (Wei et al., 2022a,b; Suzgun
et al., 2022; Zhou et al., 2022a; Anil et al., 2022).
In addition, reasoning can make language models
more explainable and interpretable, as it provides
explicit rationales for their predictions.
Right task/application?
As Valmeekam et al.
(2022) point out, current benchmarks may not ade-
quately reflect the reasoning capabilities of LLMs.
In addition, tasks such as solving simple math prob-
lems and concatenating letters in strings (§4.1) are
artificial and do not accurately reflect real-world
situations. To truly understand the reasoning ability
of LLMs, it is important to consider more realistic
and meaningful applications such as decision mak-
ing (Edwards, 1954), legal reasoning (Levi, 2013),
and scientific reasoning (Zimmerman, 2000). Our
ultimate goal should not be to enable LLMs to solve
simple math problems, which can be simply done
with other programs. When conducting relevant
research, it is essential to ask whether the specific
task being tackled is meaningful and whether the
proposed method can be generalized to more real-
istic tasks and applications.
Are language models really able to reason?
There are several indications that LLMs are able
to reason, including 1) high performance on vari-
ous tasks requiring reasoning (Suzgun et al., 2022);
2) the ability to reason step-by-step with chain-
of-thought prompting (Wei et al., 2022b); and 3)
the reflection of human-like content effects on rea-
soning (Dasgupta et al., 2022). However, these
findings are not sufficient to conclude that LLMs
can truly reason. For 1), it is not clear whether the
models are making predictions based on reasoning
or heuristics (Patel et al., 2021). For many existing
benchmarks on reasoning, actually, we can design
a program with heuristic rules to achieve very high
performance. We usually do not think a program
relying on heuristic rules is capable of reasoning.
For 2), although the models seem to reason step-
by-step, the generated rationales may be incorrect
and inconsistent. It is possible that the models are
“generating reasoning-like response” rather than
“reasoning step-by-step”. For 3), while LLMs dis-
play some human-like reasoning patterns, this does
not necessarily mean that they behave like humans.
Additionally, there are several observations that
suggest LLMs may not be capable of reasoning:
1) LLMs still struggle with tasks that require com-
plex reasoning (Valmeekam et al., 2022; Han et al.,
2022a; Ruis et al., 2022). If LLMs are really de-
cent reasoners, they should handle tasks that can
be simply solved by humans through reasoning;
2) LLMs make mistakes in their reasoning, as ex-
plained above; 3)#4 The performance of LLMs on
downstream tasks has been found to be sensitive to
the frequency of certain terms, such as numbers, in
the training data (Razeghi et al., 2022; Jung et al.,
2022), which would not be expected if the models
were solving mathematical problems through rea-
soning; 4)# Language models have been found to
struggle with associating relevant information that
they have memorized (Huang et al., 2022c).
Overall, it is still too early to draw a conclusion
about the proposed question. In fact, there is also an
ongoing debate about whether language models can
actually understand language or capture meaning
(Bender and Koller, 2020; Li et al., 2021; Manning,
2022; Piantasodi and Hill, 2022). Further in-depth
analysis of factors such as training data, model
architecture, and optimization objectives is needed,
as well as the development of better benchmarks
for measuring the reasoning capabilities of LLMs.
However, it is clear that the current models are not
yet capable of robust reasoning.
Improving reasoning capabilities of LLMs.
4 #indicates the finding has not been carefully examined
in language models with more than 100 billion parameters.
While techniques like chain-of-thought prompt-
ing (Wei et al., 2022b) may help to elicit reasoning
abilities in large language models, they cannot en-
able the models to solve tasks beyond their current
capabilities. To truly enhance reasoning in LLMs,
we need to utilize training data, model architecture,
and optimization objectives that are designed to
encourage reasoning. For example, finetuning a
model with a dataset including CoT data has been
shown to improve reasoning (Chung et al., 2022),
and models can also self-improve through the pro-
cess of bootstrapping their reasoning (Zelikman
et al., 2022; Huang et al., 2022a). There is still
much research that needs to be done in this area,
and we look forward to future progress in improv-
ing reasoning in large language models.
7
Conclusion
In this paper, we have provided a detailed and up-
to-date review of the current state of knowledge
on reasoning in large language models. We have
discussed techniques for improving and eliciting
reasoning in LLMs, methods and benchmarks for
evaluating reasoning abilities, and the findings and
implications of previous studies in this topic. While
LLMs have made significant progress in natural
language processing and related fields, it remains
unclear to what extent they are capable of true rea-
soning or whether they are simply using memorized
patterns and heuristics to solve problems. Further
research is needed to fully understand the reason-
ing abilities of LLMs, improve LLMs’ reasoning
capabilities, and determine their potential for use
in a variety of applications. We hope that this paper
will serve as a useful overview of the current state
of the field and stimulate further discussion and
research on this interesting and important topic.
Limitations
In this paper, we provide an overview of the current
state of knowledge on reasoning in large language
models. Reasoning is a broad concept that encom-
passes various forms, making it impractical to sum-
marize all related work in a single paper. Therefore,
we focus on deductive reasoning, as it is the most
commonly studied in the literature. Other forms of
reasoning such as inductive reasoning (Yang et al.,
2022; Misra et al., 2022, inter alia) and abductive
reasoning (Wiegreffe et al., 2022; Lampinen et al.,
2022; Jung et al., 2022, inter alia) may not be dis-
cussed in depth.
Additionally, given the rapid evolution and sig-
nificance of reasoning within large language mod-
els, it is crucial to note that new contributions may
have emerged in the field concurrent with the writ-
ing of this paper. An additional resource to consider
is a parallel survey by Qiao et al. (2022), which em-
phasizes reasoning via language model prompting.
Our coverage may not extend to papers released
during or after 2023 such as evaluation on Chat-
GPT (Bang et al., 2023; Zheng et al., 2023). As
such, we recommend readers to check the papers
that cite this survey for a more comprehensive and
updated understanding of this field.
Acknowledgements
We would like to thank Jason Wei (OpenAI) and
Denny Zhou (Google DeepMind) for their valu-
able advice and constructive feedback on this work.
This material is based upon work supported by
the National Science Foundation IIS 16-19302 and
IIS 16-33755, Zhejiang University ZJU Research
083650, IBM-Illinois Center for Cognitive Com-
puting Systems Research (C3SR) and IBM-Illinois
Discovery Accelerator Institute (IIDAI), gift grants
from eBay and Microsoft Azure, UIUC OVCR
CCIL Planning Grant 434S34, UIUC CSBS Small
Grant 434C8U, and UIUC New Frontiers Initiative.
Any opinions, findings, and conclusions or recom-
mendations expressed in this publication are those
of the author(s) and do not necessarily reflect the
views of the funding agencies.
References
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. MathQA: Towards interpretable math
word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
2357–2367, Minneapolis, Minnesota. Association for
Computational Linguistics.
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor
Lewkowycz, Vedant Misra, Vinay Ramasesh, Am-
brose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam
Neyshabur. 2022.
Exploring length generaliza-
tion in large language models.
ArXiv preprint,
abs/2207.04901.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multi-
task, multilingual, multimodal evaluation of chatgpt
on reasoning, hallucination, and interactivity. ArXiv
preprint, abs/2302.04023.
Emily M. Bender and Alexander Koller. 2020. Climbing
towards NLU: On meaning, form, and understanding
in the age of data. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 5185–5198, Online. Association for
Computational Linguistics.
Prajjwal Bhargava and Vincent Ng. 2022. Common-
sense knowledge reasoning and generation with pre-
trained language models: A survey. Proceedings of
the AAAI Conference on Artificial Intelligence.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli,
Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosse-
lut, Emma Brunskill, et al. 2021. On the opportuni-
ties and risks of foundation models. ArXiv preprint,
abs/2108.07258.
Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, and Mar-
tin Goedhart. 2020.
Logical reasoning in formal
and everyday reasoning tasks. International Journal
of Science and Mathematics Education, 18(8):1673–
1694.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large lan-
guage models trained on code.
ArXiv preprint,
abs/2107.03374.
Wenhu Chen. 2022. Large language models are few (1)-
shot table reasoners. ArXiv preprint, abs/2210.06710.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022.
Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. ArXiv preprint,
abs/2211.12588.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. ArXiv preprint,
abs/2204.02311.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
ArXiv preprint, abs/2210.11416.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv
preprint, abs/1803.05457.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. 2021.
Training veri-
fiers to solve math word problems. ArXiv preprint,
abs/2110.14168.
Antonia Creswell and Murray Shanahan. 2022. Faith-
ful reasoning using large language models. ArXiv
preprint, abs/2208.14271.
Antonia Creswell, Murray Shanahan, and Irina Higgins.
2022. Selection-inference: Exploiting large language
models for interpretable logical reasoning. ArXiv
preprint, abs/2205.09712.
Ishita Dasgupta, Andrew K Lampinen, Stephanie CY
Chan,
Antonia Creswell,
Dharshan Kumaran,
James L McClelland, and Felix Hill. 2022. Lan-
guage models show human-like content effects on
reasoning. ArXiv preprint, abs/2207.07051.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
David Dohan, Winnie Xu, Aitor Lewkowycz, Ja-
cob Austin, David Bieber, Raphael Gontijo Lopes,
Yuhuai Wu, Henryk Michalewski, Rif A Saurous,
Jascha Sohl-Dickstein, et al. 2022. Language model
cascades. ArXiv preprint, abs/2207.10342.
Andrew Drozdov, Nathanael Schärli, Ekin Akyürek,
Nathan Scales, Xinying Song, Xinyun Chen, Olivier
Bousquet, and Denny Zhou. 2022. Compositional
semantic parsing with large language models. ArXiv
preprint, abs/2209.15003.
Dheeru Dua, Shivanshu Gupta, Sameer Singh, and
Matt Gardner. 2022.
Successive prompting for
decomposing complex questions. ArXiv preprint,
abs/2212.04092.
Ward Edwards. 1954. The theory of decision making.
Psychological bulletin, 51(4):380.
Ronald Fagin, Joseph Y Halpern, Yoram Moses, and
Moshe Vardi. 2004. Reasoning about knowledge.
MIT press.
Yao Fu, Hao Peng, and Tushar Khot. 2022a. How does
gpt obtain its ability? tracing emergent abilities of
language models to their sources.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,
and Tushar Khot. 2022b. Complexity-based prompt-
ing for multi-step reasoning.
ArXiv preprint,
abs/2210.00720.
Kathleen M Galotti. 1989. Approaches to studying for-
mal and everyday reasoning. Psychological bulletin,
105(3):331.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language
models. ArXiv preprint, abs/2211.10435.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics, 9:346–
361.
Olga Golovneva, Moya Chen, Spencer Poff, Martin
Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,
and Asli Celikyilmaz. 2022. Roscoe: A suite of
metrics for scoring step-by-step reasoning. ArXiv
preprint, abs/2212.07919.
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting
Qi, Martin Riddell, Luke Benson, Lucy Sun, Eka-
terina Zubova, Yujie Qiao, Matthew Burtell, et al.
2022a. Folio: Natural language reasoning with first-
order logic. ArXiv preprint, abs/2209.00840.
Simon Jerome Han, Keith Ransom, Andrew Perfors,
and Charles Kemp. 2022b.
Human-like property
induction is a challenge for large language models.
Hangfeng He, Hongming Zhang, and Dan Roth. 2023.
Rethinking with retrieval: Faithful large language
model inference. ArXiv preprint, abs/2301.00303.
Chadi Helwe, Chloé Clavel, and Fabian M Suchanek.
2021. Reasoning with transformer-based models:
Deep learning, but shallow reasoning. In 3rd Confer-
ence on Automated Knowledge Base Construction.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021. Measuring mathematical
problem solving with the math dataset. In Proceed-
ings of the Neural Information Processing Systems
Track on Datasets and Benchmarks, volume 1.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin
Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.
2022a.
Large language models can self-improve.
ArXiv preprint, abs/2210.11610.
Jie Huang, Kevin Chang, Jinjun Xiong, and Wen-mei
Hwu. 2022b.
Open relation modeling: Learning
to define relations between entities. In Findings of
the Association for Computational Linguistics: ACL
2022, pages 297–308, Dublin, Ireland. Association
for Computational Linguistics.
Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.
2022c. Are large pre-trained language models leak-
ing your personal information? In Findings of the
Association for Computational Linguistics: EMNLP
2022, pages 2038–2047, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.
Jie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun
Xiong, and Wen-mei Hwu. 2022d. DEER: Descrip-
tive knowledge graph for explaining entity relation-
ships. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing,
pages 6686–6698, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch. 2022e. Language models as zero-
shot planners: Extracting actionable knowledge for
embodied agents. In Proceedings of the 39th Inter-
national Conference on Machine Learning, volume
162 of Proceedings of Machine Learning Research,
pages 9118–9147. PMLR.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
Igor Mordatch, Yevgen Chebotar, et al. 2022f. Inner
monologue: Embodied reasoning through planning
with language models. In 2022 Conference on Robot
Learning.
Michael Huth and Mark Ryan. 2004. Logic in Computer
Science: Modelling and reasoning about systems.
Cambridge university press.
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-
man, Chandra Bhagavatula, Ronan Le Bras, and
Yejin Choi. 2022. Maieutic prompting: Logically
consistent reasoning with recursive explanations. The
2022 Conference on Empirical Methods for Natural
Language Processing.
Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia,
Xin Xu, and Deepak Ramachandran. 2022. Lam-
bada: Backward chaining for automated reasoning in
natural language. ArXiv preprint, abs/2212.13894.
Daniel Keysers, Nathanael Schärli, Nathan Scales,
Hylke Buisman, Daniel Furrer, Sergii Kashubin,
Nikola Momchev, Danila Sinopalnikov, Lukasz
Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang,
Marc van Zee, and Olivier Bousquet. 2020. Measur-
ing compositional generalization: A comprehensive
method on realistic data. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao
Fu, Kyle Richardson, Peter Clark, and Ashish Sab-
harwal. 2022. Decomposed prompting: A modular
approach for solving complex tasks. ArXiv preprint,
abs/2210.02406.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In Advances
in Neural Information Processing Systems.
Brenden M. Lake and Marco Baroni. 2018. General-
ization without systematicity: On the compositional
skills of sequence-to-sequence recurrent networks. In
Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmässan,
Stockholm, Sweden, July 10-15, 2018, volume 80 of
Proceedings of Machine Learning Research, pages
2879–2888. PMLR.
Andrew K Lampinen, Ishita Dasgupta, Stephanie CY
Chan, Kory Matthewson, Michael Henry Tessler, An-
tonia Creswell, James L McClelland, Jane X Wang,
and Felix Hill. 2022. Can language models learn
from explanations in context?
In Findings of the
Association for Computational Linguistics: EMNLP
2022.
Edward H Levi. 2013. An introduction to legal reason-
ing. University of Chicago Press.
Aitor Lewkowycz, Anders Andreassen, David Dohan,
Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo
Gutman-Solo, et al. 2022.
Solving quantitative
reasoning problems with language models. ArXiv
preprint, abs/2206.14858.
Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.
Implicit representations of meaning in neural lan-
guage models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 1813–1827, Online. Association for
Computational Linguistics.
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,
Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,
Baolin Peng, Yi Mao, et al. 2022a. Explanations
from large language models make small reasoners
better. ArXiv preprint, abs/2210.06726.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,
Jian-Guang Lou, and Weizhu Chen. 2022b. On the
advance of making language models better reasoners.
ArXiv preprint, abs/2206.02336.
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang
Ren. 2020. CommonGen: A constrained text gen-
eration challenge for generative commonsense rea-
soning. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020, pages 1823–1840,
Online. Association for Computational Linguistics.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 158–167, Vancouver,
Canada. Association for Computational Linguistics.
Chenzhengyi Liu, Jie Huang, Kerui Zhu, and Kevin
Chen-Chuan Chang. 2022a.
Dimongen:
Diver-
sified generative commonsense reasoning for ex-
plaining concept relationships.
ArXiv preprint,
abs/2212.10545.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022b. What
makes good in-context examples for GPT-3?
In
Proceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures,
pages 100–114, Dublin, Ireland and Online. Associa-
tion for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv preprint, abs/1907.11692.
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-
Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
Clark, and Ashwin Kalyan. 2022. Learn to explain:
Multimodal reasoning via thought chains for science
question answering. In Advances in Neural Informa-
tion Processing Systems.
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,
and Graham Neubig. 2022. Language models of code
are few-shot commonsense learners. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Lucie Charlotte Magister, Jonathan Mallinson, Jakub
Adamek, Eric Malmi, and Aliaksei Severyn. 2022.
Teaching small language models to reason. ArXiv
preprint, abs/2212.08410.
Christopher D Manning. 2022. Human language under-
standing & reasoning. Daedalus, 151(2):127–138.
Gary Marcus. 2020. The next decade in ai: four steps
towards robust artificial intelligence. ArXiv preprint,
abs/2002.06177.
Conor McHugh and Jonathan Way. 2018. What is rea-
soning? Mind, 127(505):167–196.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and developing
English math word problem solvers. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 975–984, Online.
Association for Computational Linguistics.
Sewon Min, Victor Zhong, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2019. Multi-hop reading compre-
hension through question decomposition and rescor-
ing. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, pages
6097–6109, Florence, Italy. Association for Compu-
tational Linguistics.
Kanishka Misra, Julia Taylor Rayz, and Allyson Et-
tinger. 2022.
A property induction framework
for neural language models.
ArXiv preprint,
abs/2205.06910.
Melanie Mitchell. 2021.
Abstraction and analogy-
making in artificial intelligence. Annals of the New
York Academy of Sciences, 1505(1):79–101.
Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria
Lin, Neha Verma, Rui Zhang, Wojciech Kry´
sci´
nski,
Hailey Schoelkopf, Riley Kong, Xiangru Tang,
Mutethia Mutuma, Ben Rosand, Isabel Trindade,
Renusree Bandaru, Jacob Cunningham, Caiming
Xiong, Dragomir Radev, and Dragomir Radev. 2022.
FeTaQA: Free-form table question answering. Trans-
actions of the Association for Computational Linguis-
tics, 10:35–49.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, Charles Sutton, and Augustus Odena.
2022. Show your work: Scratchpads for interme-
diate computation with language models. In Deep
Learning for Code Workshop.
OpenAI. 2022. Chatgpt: Optimizing language models
for dialogue. OpenAI.
John Arthur Passmore. 1961. Philosophical reasoning.
Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the 7th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 1470–
1480, Beijing, China. Association for Computational
Linguistics.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2080–2094, Online.
Association for Computational Linguistics.
Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun
Cho, and Douwe Kiela. 2020. Unsupervised question
decomposition for question answering. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
8864–8880, Online. Association for Computational
Linguistics.
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin,
Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu
Chen. 2022. Reasoning like program executors. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Steven T Piantasodi and Felix Hill. 2022. Meaning
without reference in large language models. ArXiv
preprint, abs/2208.02957.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A Smith, and Mike Lewis. 2022. Measuring
and narrowing the compositionality gap in language
models. ArXiv preprint, abs/2210.03350.
Ben Prystawski, Paul Thibodeau, and Noah Goodman.
2022. Psychologically-informed chain-of-thought
prompts for metaphor understanding in large lan-
guage models. ArXiv preprint, abs/2209.08141.
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,
Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,
and Huajun Chen. 2022.
Reasoning with lan-
guage model prompting: A survey. ArXiv preprint,
abs/2212.09597.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training gopher.
ArXiv preprint, abs/2112.11446.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21(140):1–67.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019.
Explain your-
self! leveraging language models for commonsense
reasoning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 4932–4942, Florence, Italy. Association for
Computational Linguistics.
Yasaman Razeghi, Robert L Logan IV, Matt Gardner,
and Sameer Singh. 2022. Impact of pretraining term
frequencies on few-shot reasoning. ArXiv preprint,
abs/2202.07206.
Subhro Roy and Dan Roth. 2015. Solving general arith-
metic word problems. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1743–1752, Lisbon, Portu-
gal. Association for Computational Linguistics.
Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker,
Tim Rocktäschel, and Edward Grefenstette. 2022.
Large language models are not zero-shot communi-
cators. ArXiv preprint, abs/2210.14986.
Jacob Russin, Randall C O’Reilly, and Yoshua Bengio.
2020. Deep learning needs a prefrontal cortex. Work
Bridging AI Cogn Sci, 107:603–616.
Abulhair Saparov and He He. 2022. Language models
are greedy reasoners: A systematic formal analysis of
chain-of-thought. ArXiv preprint, abs/2210.01240.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili´
c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022.
Bloom:
A 176b-
parameter open-access multilingual language model.
ArXiv preprint, abs/2211.05100.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,
Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,
Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.
Language models are multilingual chain-of-thought
reasoners. ArXiv preprint, abs/2210.03057.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya
Sachan. 2022. Distilling multi-step reasoning ca-
pabilities of large language models into smaller mod-
els via semantic decompositions. ArXiv preprint,
abs/2212.00193.
Chan Hee Song, Jiaman Wu, Clayton Washington,
Brian M Sadler, Wei-Lun Chao, and Yu Su. 2022.
Llm-planner: Few-shot grounded planning for em-
bodied agents with large language models. ArXiv
preprint, abs/2212.04088.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta, Adrià
Garriga-Alonso, et al. 2022. Beyond the imitation
game: Quantifying and extrapolating the capabilities
of language models. ArXiv preprint, abs/2206.04615.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
Zhou, et al. 2022. Challenging big-bench tasks and
whether chain-of-thought can solve them.
ArXiv
preprint, abs/2210.09261.
Alon Talmor and Jonathan Berant. 2018. The web as
a knowledge-base for answering complex questions.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 641–651, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-
berg, and Jonathan Berant. 2020. Leap-of-thought:
Teaching pre-trained models to systematically rea-
son over implicit knowledge. In Advances in Neural
Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas
Scialom, Anthony Hartshorn, Elvis Saravia, Andrew
Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
Galactica: A large language model for science. ArXiv
preprint, abs/2211.09085.
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,
and Subbarao Kambhampati. 2022. Large language
models still can’t plan (a benchmark for llms on plan-
ning and reasoning about change). In NeurIPS 2022
Foundation Models for Decision Making Workshop.
Boshi Wang, Xiang Deng, and Huan Sun. 2022a. Itera-
tively prompt pre-trained language models for chain
of thought. In The 2022 Conference on Empirical
Methods for Natural Language Processing.
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,
You Wu, Luke Zettlemoyer, and Huan Sun. 2022b.
Towards understanding chain-of-thought prompting:
An empirical study of what matters. ArXiv preprint,
abs/2212.10001.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022c. Self-consistency
improves chain of thought reasoning in language
models. ArXiv preprint, abs/2203.11171.
Peter C Wason. 1968. Reasoning about a rule. Quar-
terly journal of experimental psychology, 20(3):273–
281.
Peter Cathcart Wason and Philip Nicholas Johnson-
Laird. 1972. Psychology of reasoning: Structure
and content, volume 86. Harvard University Press.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022a. Emergent abilities of large language models.
Transactions on Machine Learning Research.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022b. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems.
Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu,
and Jun Zhao. 2022. Large language models are
reasoners with self-verification.
ArXiv preprint,
abs/2212.09561.
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,
Mark Riedl, and Yejin Choi. 2022.
Reframing
human-AI collaboration for generating free-text ex-
planations. In Proceedings of the 2022 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 632–658, Seattle, United States.
Association for Computational Linguistics.
Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik
Cambria, Xiaodong Liu, Jianfeng Gao, and Furu
Wei. 2022. Language models as inductive reason-
ers. ArXiv preprint, abs/2212.10923.
Xi Ye and Greg Durrett. 2022. The unreliability of
explanations in few-shot prompting for textual rea-
soning. Advances in neural information processing
systems.
Ping Yu,
Tianlu Wang,
Olga Golovneva,
Badr
Alkhamissy, Gargi Ghosh, Mona Diab, and Asli Ce-
likyilmaz. 2022. Alert: Adapting language models
to reasoning tasks. ArXiv preprint, abs/2212.08286.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-
man. 2022. STar: Bootstrapping reasoning with rea-
soning. In Advances in Neural Information Process-
ing Systems.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
2022a. Opt: Open pre-trained transformer language
models. ArXiv preprint, abs/2205.01068.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2022b. Automatic chain of thought prompt-
ing in large language models.
ArXiv preprint,
abs/2210.03493.
Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.
2023. Why does chatgpt fall short in providing truth-
ful answers? ArXiv preprint, abs/2304.10513.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Olivier Bousquet, Quoc Le, and Ed Chi. 2022a.
Least-to-most prompting enables complex reason-
ing in large language models.
ArXiv preprint,
abs/2205.10625.
Fan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng,
Shi Han, and Dongmei Zhang. 2022b. Reflection of
thought: Inversely eliciting numerical reasoning in
language models via solving linear systems. ArXiv
preprint, abs/2210.05075.
Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron
Courville, Behnam Neyshabur, and Hanie Sedghi.
2022c.
Teaching algorithmic reasoning via in-
context learning. ArXiv preprint, abs/2211.09066.
Corinne Zimmerman. 2000.
The development of
scientific reasoning skills. Developmental review,
20(1):99–149.
</Content></Document>
<Document index=2><Published>2022-10-06</Published><Title>Language Models are Multilingual Chain-of-Thought Reasoners</Title><Authors>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei</Authors><Summary>We evaluate the reasoning abilities of large language models in multilingual
settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by
manually translating 250 grade-school math problems from the GSM8K dataset
(Cobbe et al., 2021) into ten typologically diverse languages. We find that the
ability to solve MGSM problems via chain-of-thought prompting emerges with
increasing model scale, and that models have strikingly strong multilingual
reasoning abilities, even in underrepresented languages such as Bengali and
Swahili. Finally, we show that the multilingual reasoning abilities of language
models extend to other tasks such as commonsense reasoning and word-in-context
semantic judgment. The MGSM benchmark is publicly available at
https://github.com/google-research/url-nlp.</Summary><Source>https://arxiv.org/pdf/2210.03057.pdf</Source><Content>LANGUAGE MODELS ARE
MULTILINGUAL CHAIN-OF-THOUGHT REASONERS
Freda Shi1,2,∗
Mirac Suzgun1,3,∗
Markus Freitag1
Xuezhi Wang1
Suraj Srivats4
Soroush Vosoughi4
Hyung Won Chung1
Yi Tay1
Sebastian Ruder1
Denny Zhou1
Dipanjan Das1
Jason Wei1
1Google Research
2Toyota Technological Institute at Chicago
3Stanford University
4Dartmouth College
ABSTRACT
We evaluate the reasoning abilities of large language models in multilingual
settings. We introduce the Multilingual Grade School Math (MGSM) bench-
mark, by manually translating 250 grade-school math problems from the GSM8K
dataset (Cobbe et al., 2021) into ten typologically diverse languages. We ﬁnd
that the ability to solve MGSM problems via chain-of-thought prompting emerges
with increasing model scale, and that models have strikingly strong multilin-
gual reasoning abilities, even in underrepresented languages such as Bengali
and Swahili. Finally, we show that the multilingual reasoning abilities of lan-
guage models extend to other tasks such as commonsense reasoning and word-
in-context semantic judgment. The MGSM benchmark is publicly available at
https://github.com/google-research/url-nlp.
0.01%
1%
100%
0
10
20
30
40
50
60
70
Underrepresented
languages
(SW, BN, TE, TH)
High-resource
languages
(JA, ZH, RU, ES, FR, DE)
English
(EN)
Frequency of language in pre-training dataset (token percentage)
MGSM Accuracy (%)
Translate to English with Google Translate and solve with English intermediate steps
Intermediate reasoning steps in the language of the question
Intermediate reasoning steps in English
Figure 1: Correlation between language frequency and MGSM accuracy for PaLM-540B. The
accuracy is surprisingly high, even for underrepresented languages like Swahili (SW) and Bengali
(BN), which account for less than 0.01% of the pre-training dataset.
∗Equal contribution. Work done during internship at Google Research.
1
arXiv:2210.03057v1  [cs.CL]  6 Oct 2022
1
INTRODUCTION
Recent work has shown that presenting explicit reasoning steps (i.e., chains of thought; COT) in En-
glish elicits multi-step reasoning abilities of large language models such as GPT-3 and PaLM (Brown
et al., 2020; Chowdhery et al., 2022; Wei et al., 2022b, inter alia). Pretrained multilingual language
models have also achieved impressive performance on various NLP tasks across typologically distinct
languages (Conneau et al., 2020; Xue et al., 2021; Chowdhery et al., 2022; Clark et al., 2020; Hu
et al., 2020; Ruder et al., 2021, inter alia). Tasks in existing multilingual benchmarks usually require
only simple reasoning steps, and so it is still unclear how well language models perform on tasks that
require more complex reasoning in a multilingual setting.
In this work, we introduce the MGSM benchmark to bridge the gap between the progress on English-
based chain-of-thought reasoning and multilingual NLP. We extend a subset of the English-language
GSM8K dataset (Cobbe et al., 2021) to ten typologically diverse languages via manual translation of
problems into target languages. To the best of our knowledge, this is the ﬁrst multilingual benchmark
to evaluate the arithmetic reasoning abilities of language models.
We evaluate two large language models, GPT-3 (Brown et al., 2020; Ouyang et al., 2022) and PaLM
(Chowdhery et al., 2022), on this benchmark. While both models solve less than 20% of problems
with standard prompting, the 540-billion-parameter PaLM model in particular shows exceptional
multilingual reasoning abilities with intermediate reasoning steps (Figure 1), solving more than 40%
of the problems in any investigated language, including underrepresented languages such as Bengali
and Swahili. In our best setting, PaLM achieves an average solve rate of 55% across languages. We
ﬁnd that intermediate reasoning steps in English consistently lead to competitive or better results
than those written in the native language of the question, suggesting that English chain-of-thought
prompting may be a useful baseline for future multilingual reasoning work.
We further demonstrate that the multilingual reasoning abilities of pretrained models extend to
common-sense reasoning (Ponti et al., 2020) and word-in-context semantic judgment (Raganato
et al., 2020). By presenting the models with few-shot examples in different languages, PaLM sets a
new state-of-the-art performance (89.9%) on XCOPA (Ponti et al., 2020), outperforming the prior
approaches that require thousands of training examples.
2
THE MGSM BENCHMARK
In this section, we describe the collection process of Multilingual Grade School Math (MGSM), to
our knowledge the ﬁrst multilingual arithmetic reasoning benchmark.
2
3
4
5
6
7
8
# Steps
0
10
20
30
40
50
60
70
80
# Problems
78
61
55
29
13
13
1
Figure 2: MGSM problem distribution
with respect to the number of reasoning
steps in the standard solution.
Source data.
We used GSM8K (Cobbe et al., 2021), an
English-language human-annotated grade-school math
problem dataset, as the base data source. For MGSM,
we took the ﬁrst 250 examples from the GSM8K ofﬁcial
test example list. Each problem requires two to eight
steps to solve according to the ofﬁcial solution (Figure 2).
The answer for each question in GSM8K was written as
an Arabic numeral, which we kept consistent across all
languages to facilitate cross-lingual prediction.1
Target language selection.
We selected a typologi-
cally diverse set of ten languages other than English (EN),
spanning eight language families and different levels of
representation in standard pretraining datasets such as
mC4 (Xue et al., 2021): Bengali (BN), Chinese (ZH),
French (FR), German (DE), Japanese (JA), Russian (RU),
Spanish (ES), Swahili (SW), Telugu (TE), and Thai (TH).
1Certain scripts such as Devanagari employ different numerals. We restrict the data to Arabic numerals for
consistency but future work may investigate cross-lingual numeracy by mapping Arabic numerals to those of the
corresponding script (see Spithourakis & Riedel, 2018).
2
Original Question
Frage: Roger hat 5 Tennisbälle. Er kauft noch 2 Dosen Tennisbälle. In jeder
Dose sind 3 Tennisbälle. Wie viele Tennisbälle hat er jetzt?
DIRECT
Antwort: 11
NATIVE-COT
Schritt-für-Schritt-Antwort: Roger begann mit 5 Bällen. 2 Dosen von jeweils 3
Tennisbällen macht 6 Tennisbälle. 5 + 6 = 11. Die Antwort ist 11.
EN-COT
Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each
is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Translated
English Question
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each
can has 3 tennis balls. How many tennis balls does he have now?
TRANSLATE-EN
Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each
is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Table 1: Example solution formats (§3) for a German exemplar problem, where German-speciﬁc
components are underlined and are changed to the corresponding translations for other investigated
languages. For DIRECT, NATIVE-COT and EN-COT, we provide the original German question
as input to the model and expect an answer in the corresponding format; for TRANSLATE-EN, we
input the translated question in English, and expect a step-by-step solution in English. To obtain the
desirable output format, we prepend few-shot examples in the corresponding format.
Manual translation process.
We enlisted the help of paid professional translators (two for Chinese
and German, three for Russian, ﬁve for Thai, one for each remaining target language) for the
manual translation of the 250 selected English-language examples from GSM8K. All translators
involved were native speakers of the target language and had at least two years of professional
experience in translating between English and the target language. All translators had signed a
machine translation (MT) non-usage declaration before they started to work. To verify the quality of
the human translations, the vendor sent a random subset of translations to an additional translator
to verify the quality, and checked for n-gram overlap with popular MT providers to ensure that
no machine translation toolkit has been used. We employ the translation results as gold standard
translations.
3
MULTILINGUAL CHAIN-OF-THOUGHT PROMPTING
We provide an overview of standard prompting and chain-of-thought prompting, as well as their
extensions to the multilingual setting, which we illustrate in Table 1 and use in our experiments (§4).
In standard prompting, given a prompt in the source language, the model is asked to predict the
answer (Brown et al., 2020; Schick & Schütze, 2021). This can be done in a zero-shot or few-shot
setting by providing exemplars following the same template as additional input to the model. We
refer to this setting as direct answer prediction (DIRECT) as the model directly predicts the answer
to the problem. This setting measures the model’s ability to solve problems without any intermediate
reasoning steps.
Chain-of-thought (COT; Wei et al., 2022b) prompting helps improve many few-shot reasoning tasks,
by augmenting few-shot examples with intermediate reasoning steps that should be predicted by the
model. In the multilingual setting, we can apply CoT to solve the problem in the native language
(NATIVE-COT) by predicting the reasoning steps in the original language of the problem. This
measures the model’s ability to both understand and solve the problem in a speciﬁc language.
Alternatively, we can ask the model to predict the chain of thought in English (EN-COT), regard-
less of the problem language. Such an approach may be useful as English is often used as the source
language for cross-lingual transfer (Hu et al., 2020) and has been found effective when used as the
prompt language (Zhao & Schütze, 2021; Winata et al., 2021; Lin et al., 2021b).
Finally, we can translate the problem to English and solve it with English CoT (TRANSLATE-
EN). In this setting, we use the Google Translate API to translate problems into English. This mirrors
the translate-train setup (Hu et al., 2020; Xue et al., 2021; Ruder et al., 2021), the best-performing
setting for ﬁne-tuning multilingual models where the training data is translated to English.
3
DIRECT
NATIVE-COT
EN-COT
TRANSLATE-EN
NATIVE-EXEMPLARS
✓
✓
✓
✓
ENGLISH-EXEMPLARS
✓
N/A
✓
N/A
MULTILINGUAL-EXEMPLARS
✓
✓
✓
N/A
Table 2: Possible combinations between few-shot exemplar selection and solution strategies.
Model output
Model output
Model input (native exemplar prompting)
প্রশ্ন: রজােরর 5টি ĺটিনস বল আেছ। ĺস আরও 2 ক্যান ĺটিনস বল 
িকেনেছ। প্রিতটি ক্যােন 3টি কের ĺটিনস বল আেছ। তার কােছ এখন 
কতগুিল ĺটিনস বল আেছ?
ধােপ ধােপ উত্তর: রজােরর প্রথেম 5টি বল িছল। 2টি ক্যােনর প্রিতটিেত 
3ĺট ĺটিনস বল মােন 6টি ĺটিনস বল। 5 + 6 = 11। উত্তর হল 11।
প্রশ্ন: ĺজেনেটর হাঁসগুিল প্রিতিদন 16টি কের িডম পােড়। িতিন প্রিতিদন 
প্রাতরােশ িতনটি কের িডম খান এবং বন্ধুেদর জন্য প্রিতিদন চারটি িডম 
িদেয় মািফন Ļতির কেরন। অবিশষ্ট হাঁেসর িডমগুিল িতিন প্রিতিদন 
ক
ৃষকেদর বাজাের প্রিত িডম $2 দের িবক্রয় কেরন। িতিন ক
ৃষকেদর 
বাজাের প্রিতিদন কত ডলার উপাজ
র্জন কেরন?
ধােপ ধােপ উত্তর: 
প্রিতিদন 16টি িডম পােড়। প্রিতিদন িতনটি িডম খান এবং চারটি িডম 
িদেয় মািফন Ļতির কেরন। তাই প্রিতিদন 16 - 3 - 4 = 9টি িডম 
অবিশষ্ট থােক। প্রিতটি িডেমর মূল্য $2 হেল প্রিতিদন 9 * 2 = 18 
ডলার উপাজ
র্জন কেরন। উত্তর হল 18।
প্রিতিদন 16টি িডম পােড়। প্রিতিদন িতিন িতনটি িডম খান এবং চারটি 
িডম িদেয় মািফন Ļতির কেরন। তাই প্রিতিদন িতিন 16 - 3 - 4 = 9টি 
িডম িবক্রয় কেরন। প্রিতটি িডেমর দাম $2। তাই প্রিতিদন িতিন 9 * 2 
= $18 উপাজ
র্জন কেরন। উত্তরটি $18।
Задача: у Майкла было 58 мячей для гольфа. … 
Сколько мячей для гольфа осталось у него к концу 
среды?
Пошаговое решение: вначале у Майкла было 58 
мячей для гольфа, 23 он потерял, и у него осталось 
58 - 23 = 35. … Ответ — 33.
问题：奥利维亚有 23 美元。... 她还剩多少钱？
逐步解答：5 个单价 3 美元的百吉饼应该花费 5 * 3 = 15 
美元。... 答案是 8。
প্রশ্ন: ĺজেনেটর হাঁসগুিল প্রিতিদন 16টি কের িডম পােড়। িতিন প্রিতিদন 
প্রাতরােশ িতনটি কের িডম খান এবং বন্ধুেদর জন্য প্রিতিদন চারটি িডম 
িদেয় মািফন Ļতির কেরন। অবিশষ্ট হাঁেসর িডমগুিল িতিন প্রিতিদন 
ক
ৃষকেদর বাজাের প্রিত িডম $2 দের িবক্রয় কেরন। িতিন ক
ৃষকেদর 
বাজাের প্রিতিদন কত ডলার উপাজ
র্জন কেরন?
ধােপ ধােপ উত্তর:
Model input (multilingual exemplar prompting)
Bengali question
Russian question
Bengali question
Bengali chain 
of thought
Bengali chain 
of thought
Russian chain 
of thought
Chinese question
Chinese chain 
of thought
Bengali question
Bengali chain 
of thought
Figure 3: The chain-of-thought prompts and example model outputs in the MGSM experiments. The
solutions are written in the same language as the questions of interest (NATIVE-COT).
Beyond the prompting methods, there are different ways to provide few-shot examples in context for
multilingual prompting:
• All native question exemplars (NATIVE-EXEMPLARS). We use a few in-language questions
together with their solutions as the few-shot prompt exemplars. This is the most natural setting
when we have a few examples in each investigated language.
• All English question exemplars (ENGLISH-EXEMPLARS). When we are unable to access any
existing questions or solution examples in some languages, an intuitive way is to use English
questions and solutions as exemplars to perform zero-shot cross-lingual transfer. Note that it is
unrealistic to combine this exemplar selection setting with NATIVE-COT, since we assume no
access to the native language for prompting.
• Generic multilingual question exemplars (MULTILINGUAL-EXEMPLARS).
Similar to
ENGLISH-EXEMPLARS, we assume access to questions and solutions in a few languages, and test
if multilingual exemplars better elicit the multilingual reasoning ability of models.
For TRANSLATE-EN, as all exemplar questions and solutions are in English, we only experiment
with the translated native question exemplars and English CoT. We summarize the combinations
of prompting and exemplar methods in Table 2, and present an illustration in Figure 3. Detailed
prompting input for each investigated combination can be found in Appendix A.2.
4
EXPERIMENTS ON MGSM
In this section, we evaluate the multilingual reasoning abilities of two representative state-of-the-art
pretrained large language models—GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022)
—on our MGSM benchmark in various prompting settings using exemplars in the source language
4
AVG
HRL
URL
EN
DE
FR
ES
RU
ZH
JA
TH
TE
BN
SW
Lang. Freq. (PaLM, %)
–
–
–
78.0
3.5
3.3
2.1
.53
.40
.38
.04
.02 .006 .005
GPT-3 (text-davinci-002)
r DIRECT
11.7 15.1
5.7 16.0 14.8 16.8 17.2 12.4 18.0 11.2
8.8
0.8
4.4
8.8
r NATIVE-COT
26.4 34.7
7.2 53.6 36.0 37.6 40.4 28.4 40.0 26.0 10.8
0.4
6.4 11.2
r EN-COT
31.6 39.4 13.9 53.6 44.0 46.0 44.8 28.4 40.8 32.4 19.6
5.6
9.6 20.8
r TRANSLATE-EN
45.6 47.5 40.7 53.6 46.4 46.4 51.6 48.8 47.2 44.8 41.2 42.8 41.2 37.6
PaLM-540B
r DIRECT
18.6 19.3 16.8 22.0 18.8 19.6 20.0 22.0 19.2 16.0 16.8 17.6 17.2 15.6
r NATIVE-COT
48.1 47.9 44.9 62.4 49.2 46.4 56.8 48.4 46.8 40.0 52.8 45.6 46.0 35.2
r EN-COT
51.3 52.3 46.8 62.4 53.6 51.2 58.0 55.6 46.0 49.6 49.6 46.8 46.4 44.4
r TRANSLATE-EN
55.0 56.3 51.2 62.4 57.2 55.2 60.0 59.6 55.6 50.0 50.8 49.6 53.2 51.2
Table 3: Accuracy (%) on MGSM of different models and languages with exemplar questions in native
languages (NATIVE-EXEMPLARS). HRL: average performance across high-resource languages with
larger than 0.1% frequency in the training corpora; URL: average performance across underrepresented
languages. We use 6 questions and solutions as the few-shot exemplar whenever possible: while the
token number for 6-shot prompts in some languages may exceed the token number limit of GPT-3,
we use the maximum possible number of exemplars instead for these cases. Detailed numbers of
exemplars for each language in GPT-3 experiments can be found in Appendix A.1. The best numbers
in each column are in boldface.
(NATIVE-EXEMPLARS).2 Throughout this paper, we generate outputs using greedy decoding (i.e.,
sampling with temperature τ = 0).
4.1
MAIN RESULTS
We ﬁrst compare the few-shot NATIVE-EXEMPLARS performance with different solution strategies
(Table 3). In line with the English results reported by Wei et al. (2022b), we ﬁnd that intermediate
reasoning steps (NATIVE-COT and EN-COT) help both models achieve substantial reasoning perfor-
mance gains across all languages, outperforming direct answer prediction with no explicit reasoning
steps (DIRECT) by a signiﬁcant margin. PaLM shows exceptional multilingual reasoning ability:
while it outperforms GPT-3 on all languages with different settings, PaLM-540B with intermediate
reasoning steps (NATIVE-COT and EN-COT) achieves results similar to TRANSLATE-EN on all
languages, even on underrepresented languages such as Bengali (BN) and Swahili (SW), which cover
less than 0.01% of the training corpora.
In addition, reasoning in English (EN-COT) consistently achieves competitive or better performance
than reasoning in the native language of the question (NATIVE-COT), suggesting that English
intermediate steps can be considered as useful baseline in future work on multilingual reasoning.
4.2
FURTHER ANALYSIS
Effect of language frequency in training corpora.
We illustrate the main results of NATIVE-
COT, EN-COT and TRANSLATE-EN with respect to the language frequency in PaLM training data
(Figure 1). Surprisingly, there is no strong correlation between the performance and the language
frequency in the training corpora: the average accuracy among the four underrepresented languages
was only 3% lower than the that among the six high-resource languages (44.9% vs 47.9%). Moreover,
the performance of reasoning in Thai, Telugu, and Bengali is on par with reasoning in French,
Japanese, and Chinese, despite having signiﬁcantly much less data in the training corpora.
In contrast to prior work that identiﬁes language frequency as important for complex NLU tasks with
relatively smaller models (Hu et al., 2020; Lauscher et al., 2020; Ahuja et al., 2022), these results
thus indicate that the reasoning ability of large language models may not be primarily dependent on
2We focus on these two models due to their notable few-shot performance. In contrast, current multilingual
models perform poorly in few-shot settings and are generally used for ﬁnetuning with more data (Winata et al.,
2021).
5
A
B
C D1 D2
0
10
20
30
40
50
60
70
Model Names
MGSM Accuracy (%)
GPT-3
8B
62B
540B
0
10
20
30
40
50
60
70
Model Scale
PaLM
en
de
fr
es
ru
zh
ja
th
te
bn
sw
Figure 4: MGSM accuracy with different model scales.
The letters A, B, C, D1, and D2 denote text-ada-
001, text-babbage-001, text-curie-001, text-
davinci-001, and text-davinci-002 in the GPT-3
(Brown et al., 2020; Ouyang et al., 2022) family, respec-
tively. While the number of parameters in each GPT-3
model is not publicly available, we order them alphabet-
ically. Detailed numbers can be found in Table 8.
1
2
4
6
0
10
20
30
40
50
60
70
# few-shot exemplars
MGSM Accuracy (%)
en
de
fr
es
ru
zh
ja
th
te
bn
sw
Figure 5: MGSM accuracy of PaLM-540B
with different numbers of few-shot exem-
plars. Detailed numbers can be found in
Table 8.
AVG
HRL
URL
EN
DE
FR
ES
RU
ZH
JA
TH
TE
BN
SW
NATIVE-EXEMPLARS
NATIVE-COT 48.1 47.9 44.9 62.4 49.2 46.4 56.8 48.4 46.8 40.0 52.8 45.6 46.0 35.2
EN-COT
51.3 52.3 46.8 62.4 53.6 51.2 58.0 55.6 46.0 49.6 49.6 46.8 46.4 44.4
MULTILINGUAL-EXEMPLARS
NATIVE-COT 29.8 31.8 26.3 52.0 41.6
7.2 10.4 36.0 42.8 32.8 18.0 33.6 26.8 26.8
EN-COT
48.7 50.0 46.3 57.6 53.2 43.2 53.2 48.0 51.2 43.6 46.8 46.4 48.4 43.6
ENGLISH-EXEMPLARS
EN-COT
34.7 39.4 26.6 62.4 46.0 37.2 50.4 23.6 29.2 26.8 17.2 30.0 34.4 24.8
Table 4: Performance on MGSM with different prompt exemplar type choices: the ﬁrst section is
copied correspondingly from Table 3. The best numbers in each column are in boldface.
their presence in training data and that language models are able to transfer their knowledge from
high-resource to underrepresented languages to some extent.
Effect of model scale.
We analyze the effect of model scale (i.e., number of model parameters
and computational resources used for training) on their multilingual arithmetic reasoning abilities
(Figure 4). As the models scale up, the performance generally improves for both GPT-3 and PaLM
model series on all languages. Neither model achieves a substantial solve rate until a certain scale
(text-davinci-001 for GPT-3 and PaLM-62B for PaLM), hence multilingual reasoning can be
considered an emergent ability of large language models (Wei et al., 2022a). It is worth noting that
the amount of training data per language is constant across language model scales for PaLM—the fact
that scale facilitates reasoning implies that further scaling may continue to improve the multilingual
reasoning ability of large language models.
Effect of exemplar amount.
We analyze how the multilingual reasoning performance of PaLM-
540B, the overall best-performing model, is affected by the number of few-shot exemplars (Figure 5).
Although not all trends are strictly increasing with the number of exemplars, PaLM-540B beneﬁts
from having more examples in general for all languages.
6
MODEL
AVG
ET
HT
ID
IT
QU
SW
TA
TH
TR
VI
ZH
HUMAN
97.6
98.2
96.4
100
97
94.8
99
98.6
98.2
96.4
98.4
96.6
MAD-X Base
61.0
61.3
53.7
65.8
63.0
52.5
56.3
61.9
61.8
60.3
66.1
67.6
XLM-R Large
68.7
71.4
(50)
79.8
72.6
(50)
59.2
73
72.8
74.4
73.8
78.6
mT5-XXL
74.9
77.5
72.1
81.1
75.9
54.5
74.1
75.9
78.3
78.1
76.9
79.5
RoBERTa Large (TT)
76.1
81.0
73.8
82.2
77.8
(50)
74.2
79.6
71.4
79.6
81.0
86.0
Codex (code-davinci-002)
r DIRECT
73.3
73.8
55.6
88.8
95.4
51.2
56.0
54.6
70.2
88.6
80.4
91.4
r EN-COT
80.7
88.8
79.6
91.4
96.6
52.2
67.4
55.8
84.2
91.2
86.6
93.4
PaLM-540B
r DIRECT
83.7
77.4
78.0
92.6
96.0
61.0
69.4
85.4
87.2
92.8
89.8
91.6
r EN-COT
89.9
91.0
89.6
94.0
97.4
66.8
85.4
90.8
90.2
94.6
94.6
94.8
Table 5: Accuracy on the XCOPA languages compared to previous work. Human evaluation (HUMAN)
on XCOPA was performed by Ponti et al. (2020). The MAD-X Base, XLM-R Large, and RoBERTa
Large (translate test) results are from Ponti et al. (2020), whereas the mT5 results are from (Ruder
et al., 2021). Applying multilingual CoT-prompting to PaLM-540B has enabled us to achieve a new
state-of-the-art performance on XCOPA. The best model result in each column is in boldface.
Effect of exemplar type choice.
We compare the multilingual reasoning performance of PaLM-
540B across languages with different exemplar choices (Table 4).
For the MULTILINGUAL-
EXEMPLARS setting, we concatenate one example from each of the most frequent languages (English,
German, French, Spanish, Russian, and Chinese) as the generic prompt for all languages. While
the best choice is almost always to use NATIVE-EXEMPLARS and EN-COT, MULTILINGUAL-
EXEMPLARS with EN-COT achieves competitive performance across the board, suggesting an
effective approach when we do not have access to any existing example in some languages.
Most notably, with EN-COT, MULTILINGUAL-EXEMPLARS signiﬁcantly outperforms ENGLISH-
EXEMPLARS on all non-English languages, including those not covered by the few-shot examples,
suggesting that a multilingual few-shot prompt helps elicit the multilingual reasoning abilities of
models more effectively than a monolingual (English) one.
5
EXTENSION TO OTHER MULTILINGUAL REASONING BENCHMARKS
To better understand the multilingual reasoning abilities of large pretrained language models, we
extend our experiments to two additional multilingual reasoning benchmarks, XCOPA (Ponti et al.,
2020) and XL-WiC (Raganato et al., 2020). Throughout this section, we evaluate the Codex (code-
davinci-002; Chen et al., 2021)3 and PaLM-540B models.
5.1
XCOPA
XCOPA is a multilingual evaluation dataset designed to assess the causal commonsense reasoning
capabilities of language models across multiple languages.4 It is an extension and re-annotation of the
English COPA dataset (Gordon et al., 2012) where the validation and test set examples are carefully
translated to and annotated in 11 typologically diverse languages. These languages are Estonian
(ET), Indonesian (ID), Italian (IT), Cusco-Collao Quechua (QU), Swahili (SW), Tamil (TA), Thai (TH),
Turkish (TR), Vietnamese (VO), and Mandarin Chinese (ZH). The task objective is to determine the
causal relationship between the premise and two options based on a question (which is either “What
was the cause?” or “What happened as a result?”). A successful model is, therefore, expected to not
only perform commonsense reasoning but also generalize its reasoning capabilities to new languages.
For each target language, XCOPA contains 100 annotated examples in the validation set and 500
3For both investigated tasks, we ﬁnd that code-davinci-002 generally produces competitive or better
results than text-davinci-002 on a small set of samples. In consideration of budget, we choose to use
code-davinci-002 because it supports free access at the time of our experiment.
4https://github.com/cambridgeltl/xcopa
7
Model
AVG
BG
DA
DE
ET
FA
FR
HR
IT
JA
KO
NL
ZH
HUMAN
–
87.0
–
74.0
–
97.0
–
–
78.0 75.0 76.0
–
85.0
XLM-R Large 68.9 66.5 71.1 65.8 68.7 75.3 62.5 72.3 64.9 63.8 69.6 72.8 73.2
Codex (code-davinci-002)
DIRECT
60.8 59.2 59.6 68.2 59.0 58.0 58.6 65.7 55.4 56.0 62.0 64.8 63.0
EN-COT
61.4 60.2 66.6 70.6 60.3 63.6 64.6 61.0 54.2 52.2 56.6 62.8 64.0
PaLM-540B
DIRECT
66.7 62.6 67.4 72.6 62.3 75.0 64.6 65.0 59.4 64.0 70.2 72.0 64.8
EN-COT
63.2 63.4 64.6 68.6 61.5 67.2 64.6 55.9 57.4 55.6 66.4 69.4 64.0
Table 6: Accuracy on the XL-WiC languages with MULTILINGUAL-EXEMPLARS. XLM-R Large
denotes the previous state-of-the-art results trained with 5.4K English examples (Raganato et al.,
2020). The best model result in each column is in boldface.
examples in the test set. In our experiments, we focus on the examples in the test sets and use the
ones in the validation set as few-shot exemplars whenever needed.
We test the Codex and PaLM models under both DIRECT and EN-COT. In both settings, we include
the same set of examples, randomly selected from the validation sets of TR, ZH, TA, and QU, but for
EN-COT, we additionally write brief rationales (in English) before the ﬁnal answers ourselves.
Results.
Table 5 presents our main results, along with per-language breakdowns for each XCOPA
language. The previous state-of-the-art performance was around 76%, obtained by RoBERTa Large
in the translate-test setting where the English RoBERTa Large model was ﬁrst trained on the English
COPA (Gordon et al., 2012) and English SIQa (Sap et al., 2019) datasets and then applied to the
XCOPA test data, which was translated to English (Ponti et al., 2020). With only four multilingual
chain-of-thought examples (EN-COT), PaLM-540B outperforms RoBERTa Large by a signiﬁcant
margin (14%), thereby setting a new high bar on XCOPA. While Codex performs better than
RoBERTa Large, it still falls 9% behind PaLM-540B. We also highlight that PaLM-540B performs
noticeably better than all the other models on under-represented languages such as ET, HT, and SW;
this result suggests that PaLM-540B might have some internal knowledge about these languages.
5.2
XL-WIC
XL-WiC is a multilingual word in-context semantic judgment benchmark covering thirteen lan-
guages:5 Bulgarian (BG), Danish (DA), German (DE), Estonian (ET), Persian (FA), French (FR),
Croatian (HR), Italian (IT), Japanese (JA), Korean (KO), Dutch (NL) and Chinese (ZH). Given two
sentences in the same language and a word of interest which appears in both sentences, the model is
asked whether the word is of the same sense in the sentences. In order to arrive at the correct answer,
a model needs to be aware of the concept of word sense, and to infer the sense of a word based on its
context. Despite its simplicity, this task is extremely challenging; PaLM-540B only achieves a score
of 64.6 on WiC (Pilehvar & Camacho-Collados, 2019), the English version of the task.
Results.
We evaluate the cross-lingual word-in-context sense judgment performance of models
(Table 6). With the supervision from only four examples, PaLM-540B achieves competitive or better
results that the state-of-the-art model (XLM-R Large) on 6 (German, Persian, French, Japanese,
Korean and Dutch) of the 12 investigated languages. However, we do not observe an improvement
over direct answer prediction when using chain-of-thought prompting on this task.6
5https://pilehvar.github.io/xlwic/
6One potential reason is that our prompts are not necessarily optimal (Wang et al., 2022) and may beneﬁt
from a broader investigation of other prompt formats. On the other hand, rationales for this task are fairly
straight-forward and example-speciﬁc. It is thus unclear whether the WiC task requires true reasoning that
beneﬁts from the depiction of intermediate reasoning steps. We leave further investigation for future work.
8
6
RELATED WORK
Prompting.
Existing work (Radford et al., 2019; Brown et al., 2020; Schick & Schütze, 2021, inter
alia) has shown that prompting pre-trained large language models can lead to strong performance
on various tasks such as text classiﬁcation (Shin et al., 2020; Gao et al., 2021), question answering
(Khashabi et al., 2020), and program synthesis (Austin et al., 2021; Nye et al., 2021; Shi et al.,
2022a): taking a few examples of the task in a certain pattern as the prompting input, models are
often able to generate accurate output following the pattern. Wei et al. (2022b) have shown that
chain-of-thought prompting signiﬁcantly improves the reasoning performance of language models,
by adding explicit reasoning steps before the ﬁnal answer. Ahn et al. (2022) apply chain-of-thought
prompting in robotics scenarios, including a multilingual setting. In this work, we systematically
analyze multilingual few-shot chain-of-thought prompting on complicated reasoning benchmarks.
Multilingual pre-trained language models.
Through masked language modeling (Devlin et al.,
2019; Conneau et al., 2020), auto-regressive language modeling (Brown et al., 2020; Ouyang et al.,
2022) or encoder-decoder training (Liu et al., 2020; Chen et al., 2021; Xue et al., 2021), pre-trained
Transformer-based large language models have shown impressive performance on multiple NLP
tasks across languages. Previous work (Zhao & Schütze, 2021; Winata et al., 2021; Lin et al.,
2021b) investigated prompting in the multilingual setting and found that using English prompts with
non-English examples led to strong few-shot performance. Evaluation of multilingual models has
mostly focused on general information extraction tasks such as question answering (Clark et al., 2020;
Hu et al., 2020; Kassner et al., 2021; Ruder & Sil, 2021) as well as speciﬁc types of reasoning such
as commonsense reasoning (Ponti et al., 2020; Lin et al., 2021a) and temporal reasoning (Ruder et al.,
2021). To the best of our knowledge, this is the ﬁrst study to evaluate the multilingual multi-step
reasoning abilities of large language models.
Cross-lingual transfer and generalization.
Previous work has demonstrated that pre-trained
multilingual models signiﬁcantly help cross-lingual transfer on a wide range of NLP tasks such as
cross-lingual named entity recognition (Pires et al., 2019; Mulcaire et al., 2019), zero-shot cross-
lingual dependency parsing (Schuster et al., 2019; Shi et al., 2022b), and bilingual lexicon induction
(Shi et al., 2021). In this work, we demonstrate strong cross-lingual generalization of PaLM (§4.2,
§5) and Codex (§5), on three tasks that require complicated reasoning.
Multilingual benchmarks.
To test the multilingual NLP performance of existing models, there has
been work introducing benchmarks on various multilingual tasks, including cross-lingual question
answering (Liu et al., 2019; Clark et al., 2020), natural language inference (Conneau et al., 2018) and
bilingual lexicon induction (Lample et al., 2018), as well as collections across tasks (Hu et al., 2020;
Ruder et al., 2021). The tasks in these multilingual benchmarks, to the best of our knowledge, require
relatively simple reasoning processes. In this paper, we present MGSM, a multilingual arithmetic
reasoning benchmark, which can be used to test multilingual multi-step reasoning abilities of models.
7
CONCLUSION
In this paper, we introduce MGSM, the ﬁrst multilingual benchmark to evaluate arithmetic reasoning
abilities of language models. MGSM is an extension of the GSM8K dataset (Cobbe et al., 2021)
and contains 250 examples written in ten typologically diverse languages. We also present a com-
prehensive analysis of the multilingual reasoning abilities of large language models such as GPT-3
and PaLM on multiple multilingual benchmarks, including our own MGSM dataset. We ﬁnd that
large-scale language models appear to perform complex multi-step reasoning across multiple lan-
guages, including those underrepresented languages which are covered by less than 0.01% of training
corpora. Finally, we demonstrate that multilingual chain-of-thought prompting is an empirically
effective approach to multilingual commonsense reasoning, outperforming the previous best model
on the challenging XCOPA dataset by 13% on average.
9
REFERENCES
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I
say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. URL
https://arxiv.org/abs/2204.01691.
Kabir Ahuja, Shanu Kumar, Sandipan Dandapat, and Monojit Choudhury. Multi task learning for zero
shot performance prediction of multilingual models. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5454–5467, Dublin,
Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.
374. URL https://aclanthology.org/2022.acl-long.374.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/2108.
07732.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models
are few-shot learners. NeurIPS, 2020. URL https://papers.nips.cc/paper/2020/
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://
arxiv.org/abs/2107.03374.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Hyung Won
Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, et al. PaLM: Scaling language
modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.
org/abs/2204.02311.
Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,
and Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in
typologically diverse languages. Transactions of the Association for Computational Linguistics,
8:454–470, 2020. doi: 10.1162/tacl_a_00317. URL https://aclanthology.org/2020.
tacl-1.30.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. Training veriﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk,
and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2475–2485,
Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-
supervised cross-lingual representation learning at scale.
In Proceedings of the 58th An-
nual Meeting of the Association for Computational Linguistics, pp. 8440–8451, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL
https://aclanthology.org/2020.acl-main.747.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. NAACL, 2019. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423.
Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, 1994.
10
Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. ACL, 2021. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.
org/2021.acl-long.295.
Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of
plausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First
Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pp. 394–398, Montréal, Canada, 7-8 June 2012. Association
for Computational Linguistics. URL https://aclanthology.org/S12-1052.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson.
Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation.
In International Conference on Machine Learning, pp. 4411–4421. PMLR, 2020.
Nora Kassner, Philipp Dufter, and Hinrich Schütze. Multilingual LAMA: Investigating Knowledge
in Multilingual Pretrained Language Models. In Proceedings of EACL 2021, pp. 3250–3258, 2021.
URL http://arxiv.org/abs/2102.00894.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings
of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907, Online, Novem-
ber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.171.
URL https://aclanthology.org/2020.findings-emnlp.171.
Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou.
Word translation without parallel data. In International Conference on Learning Representations,
2018.
Anne Lauscher, Vinit Ravishankar, Ivan Vuli´
c, and Goran Glavaš. From Zero to Hero: On the
Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers. In Proceedings
of EMNLP 2020, 2020. URL http://arxiv.org/abs/2005.00633.
Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. Common sense beyond English:
Evaluating and improving multilingual language models for commonsense reasoning. In Proceed-
ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.
1274–1287, Online, August 2021a. Association for Computational Linguistics. doi: 10.18653/v1/
2021.acl-long.102. URL https://aclanthology.org/2021.acl-long.102.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle
Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh
Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona
Diab, Veselin Stoyanov, and Xian Li. Few-shot Learning with Multilingual Language Models.
arXiv preprint arXiv:2112.10668, 2021b. URL http://arxiv.org/abs/2112.10668.
Jiahua Liu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. XQA: A cross-lingual open-domain question
answering dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pp. 2358–2368, Florence, Italy, July 2019. Association for Computational Linguistics.
doi: 10.18653/v1/P19-1227. URL https://aclanthology.org/P19-1227.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis,
and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv
preprint arXiv:2001.08210, 2020. URL https://arxiv.org/pdf/2001.08210.pdf.
Phoebe Mulcaire, Jungo Kasai, and Noah A. Smith. Polyglot contextual representations improve
crosslingual transfer. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 3912–3918, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics. doi: 10.18653/v1/N19-1392. URL https://aclanthology.org/N19-1392.
11
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,
2021. URL https://openreview.net/forum?id=iedYJm92o0a.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. URL https://
arxiv.org/abs/2203.02155.
Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for
evaluating context-sensitive meaning representations. NAACL, 2019. doi: 10.18653/v1/N19-1128.
URL https://aclanthology.org/N19-1128.
Telmo Pires, Eva Schlinger, and Dan Garrette.
How multilingual is multilingual BERT?
In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
pp. 4996–5001, Florence, Italy, July 2019. Association for Computational Linguistics. URL
https://aclanthology.org/P19-1493.
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vuli´
c, and Anna Korhonen.
XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2362–2376,
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
emnlp-main.185. URL https://aclanthology.org/2020.emnlp-main.185.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al.
Language models are unsupervised multitask learners.
OpenAI blog, 1(8), 2019.
URL
https://d4mucfpksywv.cloudfront.net/better-language-models/
language_models_are_unsupervised_multitask_learners.pdf.
Alessandro Raganato, Tommaso Pasini, Jose Camacho-Collados, and Mohammad Taher Pilehvar.
XL-WiC: A multilingual benchmark for evaluating semantic contextualization. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.
7193–7206, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/
2020.emnlp-main.584. URL https://aclanthology.org/2020.emnlp-main.584.
Sebastian Ruder and Avirup Sil. Multi-domain multilingual question answering. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,
pp. 17–21, 2021.
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu,
Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. XTREME-R: Towards more chal-
lenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pp. 10215–10245, Online and Punta Cana, Dominican
Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
emnlp-main.802. URL https://aclanthology.org/2021.emnlp-main.802.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Com-
monsense reasoning about social interactions.
In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP), pp. 4463–4473, Hong Kong, China,
November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL
https://aclanthology.org/D19-1454.
Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are
also few-shot learners.
NAACL, June 2021.
doi: 10.18653/v1/2021.naacl-main.185.
URL
https://aclanthology.org/2021.naacl-main.185.
Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. Cross-lingual alignment of contextual
word embeddings, with applications to zero-shot dependency parsing. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1599–1613, Minneapolis,
12
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1162.
URL https://aclanthology.org/N19-1162.
Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural language
to code translation with execution. arXiv preprint arXiv:2204.11454, 2022a.
Freda Shi, Kevin Gimpel, and Karen Livescu. Substructure distribution projection for zero-shot
cross-lingual dependency parsing. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 6547–6563, Dublin, Ireland, May
2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.452. URL
https://aclanthology.org/2022.acl-long.452.
Haoyue Shi, Luke Zettlemoyer, and Sida I. Wang.
Bilingual lexicon induction via unsuper-
vised bitext construction and word alignment. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), pp. 813–826, Online, August
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.67. URL
https://aclanthology.org/2021.acl-long.67.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt:
Eliciting Knowledge from Language Models with Automatically Generated Prompts. EMNLP,
2020. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020.
emnlp-main.346.
Georgios Spithourakis and Sebastian Riedel. Numeracy for language models: Evaluating and
improving their ability to predict numbers. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 2104–2115, Melbourne,
Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1196.
URL https://aclanthology.org/P18-1196.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented
ensembles in language models. arXiv preprint arXiv:2207.00747, 2022.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
Transactions on Machine Learing Research (TMLR), 2022a. URL https://arxiv.org/
abs/2206.07682.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc
Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.
Conference on Neural Information Processing Systems (NeurIPS), 2022b. URL https://
arxiv.org/abs/2201.11903.
Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, and Pascale Fung.
Language Models are Few-shot Multilingual Learners. In Proceedings ofthe 1st Workshop on
Multilingual Representation Learning, 2021. URL http://arxiv.org/abs/2109.07684.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer.
In Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pp. 483–498, Online, June 2021.
Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https:
//aclanthology.org/2021.naacl-main.41.
Mengjie Zhao and Hinrich Schütze. Discrete and Soft Prompting for Multilingual Models. In
Proceedings of EMNLP 2021, pp. 8547–8555, 2021. URL http://arxiv.org/abs/2109.
03630.
13
en de fr es ru zh ja th te bn sw
# Exemplars
6
6
6
6
1
5
4
1
1
1
6
Table 7: Number of few-shot exemplars for GPT-3 experiments in Table 3.
Native Exemplars
Frage: Roger hat 5 Tennisbälle. Er kauft noch 2 Dosen Tennisbälle. In 
jeder Dose sind 3 Tennisbälle. Wie viele Tennisbälle hat er jetzt?
Antwort: 11
Frage: Es waren neun Computer im Serverraum. Von Montag bis 
Donnerstag wurden jeden Tag noch fünf Computer installiert. Wie 
viele Computer sind jetzt im Serverraum?
Antwort: 29
Frage: Leah hat 32 Pralinen und ihre Schwester hat 42. Wenn sie 35 
essen, wie viele sind dann insgesamt noch übrig?
Antwort:  39
Frage: Shawn hat fünf Spielzeuge. Zu Weihnachten hat er von seiner 
Mama und seinem Papa jeweils zwei Spielzeuge bekommen. Wie 
viele Spielzeuge hat er jetzt?
Antwort: 9
Frage: Michael hat 58 Golfbälle. Am Dienstag hat er 23 Golfbälle 
verloren. Am Mittwoch hat er 2 weitere verloren. Wie viele Golfbälle 
hat er Mittwoch am Ende des Tages?
Antwor:  33
Frage: Olivia hat 23 US-Dollar. Sie hat fünf Bagels für 3 US- Dollar pro 
Stück gekauft. Wie viel Geld hat sie übrig?
Antwor: 8
Frage: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden Morgen 
zum Frühstück und backt mit vier jeden Tag Muffins für ihre Freunde. 
Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 US-Dollar 
pro frischem Entenei. Wie viel Dollar nimmt sie täglich auf dem 
Bauernmarkt ein?
Antwort: 18
English Exemplars
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis 
balls. Each can has 3 tennis balls. How many tennis balls does he 
have now?
Answer: 11
Question:There were nine computers in the server room. Five more 
computers were installed each day, from Monday to Thursday. How 
many computers are now in the server room?
Answer: 29
Question: Leah had 32 chocolates and her sister had 42. If they ate 
35, how many pieces do they have left in total?
Answer:  39
Question: : Shawn has five toys. For Christmas, he got two toys each 
from his mom and dad. How many toys does he have now?
Answer: 9
Question: Michael had 58 golf balls. On Tuesday, he lost 23 golf 
balls. On Wednesday, he lost 2 more. How many golf balls did he 
have at the end of Wednesday?
Answer:  33
Question: Olivia has $23. She bought five bagels for $3 each. How 
much money does she have left?
Answer: 8
Question: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden 
Morgen zum Frühstück und backt mit vier jeden Tag Muffins für ihre 
Freunde. Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 
US-Dollar pro frischem Entenei. Wie viel Dollar nimmt sie täglich auf 
dem Bauernmarkt ein?
Answer: 18
Multilingual Exemplars
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis 
balls. Each can has 3 tennis balls. How many tennis balls does he 
have now?
Answer: 11
Frage: Es waren neun Computer im Serverraum. Von Montag bis 
Donnerstag wurden jeden Tag noch fünf Computer installiert. Wie 
viele Computer sind jetzt im Serverraum?
Antwort: 29
Question: Léa avait 32 chocolats et sa sœur en avait 42. Si elles en 
ont mangé 35, combien de morceaux leur reste-t-il en tout ?
Réponse:  39
Pregunta: Shawn tiene cinco juguetes. Para Navidad, recibió dos 
juguetes de su mamá y dos de su papá. ¿Cuántos juguetes tiene 
ahora?
Respuesta: 9
Задача: у Майкла было 58 мячей для гольфа. Во вторник он 
потерял 23 мяча для гольфа. В среду он потерял еще 2. Сколько 
мячей для гольфа осталось у него к концу среды?
Antwor:  33
问题：奥利维亚有 23 美元。她买了五个单价 3 美元的百吉饼。 她还
剩多少钱？
解答: 8
Frage: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden Morgen 
zum Frühstück und backt mit vier jeden Tag Muffins für ihre Freunde. 
Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 US-Dollar 
pro frischem Entenei. Wie viel Dollar nimmt sie täglich auf dem 
Bauernmarkt ein?
Antwort: 18
Figure 6: Prompt template in the direct answer prediction setting (DIRECT), solving a problem in
German. Above dotted lines: few-shot exemplars; below dotted lines: the question of interest and the
expected answer. The dotted lines are not included in our experiments.
A
DETAILS OF MGSM EXPERIMENTS
In this section, we present details of our experiments on MGSM, including the number of exemplars
used for GPT-3 (§A.1) and the detailed prompts in each setting summarized in Table 2 (§A.2).
A.1
NUMBER OF EXEMPLARS FOR EACH LANGUAGE
Given the unbalanced representation of languages in the training corpora, the byte-pair encoding
(BPE; Gage, 1994) algorithm tokenizes sentences in underrepresented languages, especially those in
a different alphabet from English, into more tokens. Given that the GPT-3 API supports a maximum
number of 2048 tokens as its input, it does not support 6-shot prompting in some languages, including
Russian, Chinese, Japanese, Thai, Telugu and Bengali; therefore, we use the maximum possible
number of exemplars (Table 7) instead for GPT-3, while using 6-shot for all languages in PaLM
experiments.
A.2
MGSM PROMPTS IN EACH SETTING
We present the prompts used in our MGSM experiments in Figures 6 to 8, where the TRANSLATE-EN
experiments can be viewed as a English one with EN-COT and ENGLISH-EXEMPLARS.
B
DETAILED MGSM PERFORMANCE
We report the detailed numbers in our analysis (Figures 4 and 5) in Table 8.
14
Native Exemplars
Frage: Roger hat 5 Tennisbälle. Er kauft noch 2 Dosen Tennisbälle. In 
jeder Dose sind 3 Tennisbälle. Wie viele Tennisbälle hat er jetzt?
Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis 
balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Frage: Es waren neun Computer im Serverraum. Von Montag bis 
Donnerstag wurden jeden Tag noch fünf Computer installiert. Wie 
viele Computer sind jetzt im Serverraum?
Step-by-Step Answer: There are 4 days from Monday to Thursday. 
5 computers were added each day. That means in total 4 * 5 = 20 
computers were added. There were 9 computers in the beginning, so 
now there are 9 + 20 = 29 computers. The answer is 29.
Frage: Leah hat 32 Pralinen und ihre Schwester hat 42. Wenn sie 35 
essen, wie viele sind dann insgesamt noch übrig?
Step-by-Step Answer: Leah had 32 chocolates and Leah’s sister 
had 42. That means there were originally 32 + 42 = 74 chocolates. 35 
have been eaten. So in total they still have 74 - 35 = 39 chocolates. 
The answer is 39.
Frage: Shawn hat fünf Spielzeuge. Zu Weihnachten hat er von seiner 
Mama und seinem Papa jeweils zwei Spielzeuge bekommen. Wie 
viele Spielzeuge hat er jetzt?
Step-by-Step Answer: He has 5 toys. He got 2 from mom, so after 
that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he 
has 7 + 2 = 9 toys. The answer is 9.
Frage: Michael hat 58 Golfbälle. Am Dienstag hat er 23 Golfbälle 
verloren. Am Mittwoch hat er 2 weitere verloren. Wie viele Golfbälle 
hat er Mittwoch am Ende des Tages?
Step-by-Step Answer: Michael started with 58 golf balls and lost 
23, so he has 58 - 23 = 35. After he lost 2 more, he has 35 - 2 = 33 
balls now. The answer is 33.
Frage: Olivia hat 23 US-Dollar. Sie hat fünf Bagels für 3 US- Dollar pro 
Stück gekauft. Wie viel Geld hat sie übrig?
Step-by-Step Answer: 5 bagels for $3 each should cost 5 * 3 = 15 
dollars. Olivia had $23 in the beginning, so now she has 23 - 15 = 8 
dollars left. The answer is 8.
Frage: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden Morgen 
zum Frühstück und backt mit vier jeden Tag Muffins für ihre Freunde. 
Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 US-Dollar 
pro frischem Entenei. Wie viel Dollar nimmt sie täglich auf dem 
Bauernmarkt ein?
Step-by-Step Answer: […] The answer is 18.
English Exemplars
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis 
balls. Each can has 3 tennis balls. How many tennis balls does he 
have now?
Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis 
balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Question:There were nine computers in the server room. Five more 
computers were installed each day, from Monday to Thursday. How 
many computers are now in the server room?
Step-by-Step Answer: There are 4 days from Monday to Thursday. 
5 computers were added each day. That means in total 4 * 5 = 20 
computers were added. There were 9 computers in the beginning, so 
now there are 9 + 20 = 29 computers. The answer is 29.
Question: Leah had 32 chocolates and her sister had 42. If they ate 
35, how many pieces do they have left in total?
Step-by-Step Answer: Leah had 32 chocolates and Leah’s sister 
had 42. That means there were originally 32 + 42 = 74 chocolates. 35 
have been eaten. So in total they still have 74 - 35 = 39 chocolates. 
The answer is 39.
Question: : Shawn has five toys. For Christmas, he got two toys each 
from his mom and dad. How many toys does he have now?
Step-by-Step Answer: He has 5 toys. He got 2 from mom, so after 
that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he 
has 7 + 2 = 9 toys. The answer is 9.
Question: Michael had 58 golf balls. On Tuesday, he lost 23 golf 
balls. On Wednesday, he lost 2 more. How many golf balls did he 
have at the end of Wednesday?
Step-by-Step Answer: Michael started with 58 golf balls and lost 
23, so he has 58 - 23 = 35. After he lost 2 more, he has 35 - 2 = 33 
balls now. The answer is 33.
Question: Olivia has $23. She bought five bagels for $3 each. How 
much money does she have left?
Step-by-Step Answer: 5 bagels for $3 each should cost 5 * 3 = 15 
dollars. Olivia had $23 in the beginning, so now she has 23 - 15 = 8 
dollars left. The answer is 8.
Frage: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden Morgen 
zum Frühstück und backt mit vier jeden Tag Muffins für ihre Freunde. 
Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 US-Dollar 
pro frischem Entenei. Wie viel Dollar nimmt sie täglich auf dem 
Bauernmarkt ein?
Step-by-Step Answer: […] The answer is 18.
Multilingual Exemplars
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis 
balls. Each can has 3 tennis balls. How many tennis balls does he 
have now?
Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis 
balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Frage: Es waren neun Computer im Serverraum. Von Montag bis 
Donnerstag wurden jeden Tag noch fünf Computer installiert. Wie 
viele Computer sind jetzt im Serverraum?
Step-by-Step Answer: There are 4 days from Monday to Thursday. 
5 computers were added each day. That means in total 4 * 5 = 20 
computers were added. There were 9 computers in the beginning, so 
now there are 9 + 20 = 29 computers. The answer is 29.
Question: Léa avait 32 chocolats et sa sœur en avait 42. Si elles en 
ont mangé 35, combien de morceaux leur reste-t-il en tout ?
Step-by-Step Answer: Leah had 32 chocolates and Leah’s sister 
had 42. That means there were originally 32 + 42 = 74 chocolates. 35 
have been eaten. So in total they still have 74 - 35 = 39 chocolates. 
The answer is 39.
Pregunta: Shawn tiene cinco juguetes. Para Navidad, recibió dos 
juguetes de su mamá y dos de su papá. ¿Cuántos juguetes tiene 
ahora?
Step-by-Step Answer: He has 5 toys. He got 2 from mom, so after 
that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he 
has 7 + 2 = 9 toys. The answer is 9.
Задача: у Майкла было 58 мячей для гольфа. Во вторник он 
потерял 23 мяча для гольфа. В среду он потерял еще 2. Сколько 
мячей для гольфа осталось у него к концу среды?
Step-by-Step Answer: Michael started with 58 golf balls and lost 
23, so he has 58 - 23 = 35. After he lost 2 more, he has 35 - 2 = 33 
balls now. The answer is 33.
问题：奥利维亚有 23 美元。她买了五个单价 3 美元的百吉饼。 她还
剩多少钱？
Step-by-Step Answer: 5 bagels for $3 each should cost 5 * 3 = 15 
dollars. Olivia had $23 in the beginning, so now she has 23 - 15 = 8 
dollars left. The answer is 8.
Frage: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden Morgen 
zum Frühstück und backt mit vier jeden Tag Muffins für ihre Freunde. 
Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 US-Dollar 
pro frischem Entenei. Wie viel Dollar nimmt sie täglich auf dem 
Bauernmarkt ein?
Step-by-Step Answer: […] The answer is 18.
Figure 7: Prompt template in the English CoT setting (EN-COT), solving a problem in German.
Above dotted lines: few-shot exemplars; below dotted lines: the question of interest and the expected
answer. The dotted lines are not included in our experiments.
15
Native Exemplars
Frage: Roger hat 5 Tennisbälle. Er kauft noch 2 Dosen Tennisbälle. In 
jeder Dose sind 3 Tennisbälle. Wie viele Tennisbälle hat er jetzt?
Schritt-für-Schritt-Antwort: Roger begann mit 5 Bällen. 2 Dosen von 
jeweils 3 Tennisbällen macht 6 Tennisbälle. 5 + 6 = 11. Die Antwort ist 11.
Frage: Es waren neun Computer im Serverraum. Von Montag bis 
Donnerstag wurden jeden Tag noch fünf Computer installiert. Wie viele 
Computer sind jetzt im Serverraum?
Schritt-für-Schritt-Antwort: Von Montag bis Donnerstag sind es 4 
Tage. Jeden Tag kamen 5 neue Computer hinzu. Das macht insgesamt 
4 x 5 = 20 Computer, die hinzugefügt wurden. Am Anfang waren es 9 
Computer, also sind es jetzt 9 + 20 =29 Computer. Die Antwort lautet 
29.
Frage: Leah hat 32 Pralinen und ihre Schwester hat 42. Wenn sie 35 
essen, wie viele sind dann insgesamt noch übrig?
Schritt-für-Schritt-Antwort: Leah hat 32 Pralinen und Leahs 
Schwester 42. Das bedeutet, dass es ursprünglich 32 + 42 =74 Pralinen 
waren. 35 wurden gegessen. Also haben sie insgesamt noch 74 - 35 = 
39 Pralinen übrig. Die Antwort lautet 39.
Frage: Shawn hat fünf Spielzeuge. Zu Weihnachten hat er von seiner 
Mama und seinem Papa jeweils zwei Spielzeuge bekommen. Wie viele 
Spielzeuge hat er jetzt?
Schritt-für-Schritt-Antwort: Er hat 5 Spielzeuge. Er hat 2 von seiner 
Mama bekommen, sodass er nun 5 + 2 = 7 Spielzeuge hat. Dann hat er 
noch 2 von seinem Papa bekommen, also hat er insgesamt 7 + 2 = 9 
Spielzeuge. Die Antwort lautet 9.
Frage: Michael hat 58 Golfbälle. Am Dienstag hat er 23 Golfbälle 
verloren. Am Mittwoch hat er 2 weitere verloren. Wie viele Golfbälle hat 
er Mittwoch am Ende des Tages?
Schritt-für-Schritt-Antwort: Michael hatte anfangs 58 Golfbälle und 
hat 23 verloren, sodass er 58 - 23 = 35 hat. Nachdem er 2 weitere 
verloren hat, hat er jetzt 35 - 2 = 33 Bälle. Die Antwort lautet 33.
Frage: Olivia hat 23 US-Dollar. Sie hat fünf Bagels für 3 US- Dollar pro 
Stück gekauft. Wie viel Geld hat sie übrig?
Schritt-für-Schritt-Antwort:5 Bagels für 3 US-Dollar pro Stück 
kosten 5 x 3 = 15 Dollar. Olivia hat anfangs 23 US-Dollar, also hat sie 
jetzt 23 - 15 = 8 Dollar übrig. Die Antwort lautet 8.
Frage: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden Morgen 
zum Frühstück und backt mit vier jeden Tag Muffins für ihre Freunde. 
Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 US-Dollar pro 
frischem Entenei. Wie viel Dollar nimmt sie täglich auf dem 
Bauernmarkt ein?
Schritt-für-Schritt-Antwort: […] Die Antwort lautet 18.
Multilingual Exemplars
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis 
balls. Each can has 3 tennis balls. How many tennis balls does he have 
now?
Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis 
balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Frage: Es waren neun Computer im Serverraum. Von Montag bis 
Donnerstag wurden jeden Tag noch fünf Computer installiert. Wie viele 
Computer sind jetzt im Serverraum?
Schritt-für-Schritt-Antwort: Von Montag bis Donnerstag sind es 4 
Tage. Jeden Tag kamen 5 neue Computer hinzu. Das macht insgesamt 
4 x 5 = 20 Computer, die hinzugefügt wurden. Am Anfang waren es 9 
Computer, also sind es jetzt 9 + 20 =29 Computer. Die Antwort lautet 
29.
Question: Léa avait 32 chocolats et sa sœur en avait 42. Si elles en ont 
mangé 35, combien de morceaux leur reste-t-il en tout ?
Réponse étape par étape: Léa avait 32 chocolats et sa sœur en avait 
42. Cela signifie qu'il y avait à l'origine 32 + 42 = 74 chocolats. 35 
chocolats ont été mangés. Il leur en reste donc au total 74 - 35 = 39 
chocolats. La réponse est 39.
Pregunta: Shawn tiene cinco juguetes. Para Navidad, recibió dos 
juguetes de su mamá y dos de su papá. ¿Cuántos juguetes tiene 
ahora?
Respuesta paso a paso: Tiene 5 juguetes. Recibió 2 de la mamá, por lo 
que después de eso tiene 5 + 2 = 7 juguetes. Luego, recibió 2 más del 
papá, así que en total tiene 7 + 2 = 9 juguetes. La respuesta es 9.
Задача: у Майкла было 58 мячей для гольфа. Во вторник он 
потерял 23 мяча для гольфа. В среду он потерял еще 2. Сколько 
мячей для гольфа осталось у него к концу среды?
Пошаговое решение: вначале у Майкла было 58 мячей для 
гольфа, 23 он потерял, и у него осталось 58 - 23 = 35. После этого 
он потерял еще 2, и теперь у него осталось 35 - 2 = 33 мяча. Ответ 
— 33.
问题: 奥利维亚有 23 美元。她买了五个单价 3 美元的百吉饼。她还剩多
少钱
逐步解答: 5 个单价 3 美元的百吉饼应该花费 5 * 3 = 15 美元。奥利维亚
⼀开始有 23 美元，所以现在她还剩 23 - 15 = 8 美元。答案是 8。
Frage: Janets Enten legen 16 Eier pro Tag. Sie isst drei jeden Morgen 
zum Frühstück und backt mit vier jeden Tag Muffins für ihre Freunde. 
Den Rest verkauft sie täglich auf dem Bauernmarkt für 2 US-Dollar pro 
frischem Entenei. Wie viel Dollar nimmt sie täglich auf dem 
Bauernmarkt ein?
Schritt-für-Schritt-Antwort: […] Die Antwort lautet 18.
Figure 8: Prompt template with CoT in the question language (NATIVE-COT), solving a problem in
German. Above dotted lines: few-shot exemplars; below dotted lines: the question of interest and the
expected answer. The dotted lines are not included in our experiments.
16
AVG
HRL
LRL
EN
DE
FR
ES
RU
ZH
JA
TH
TE
BN
SW
Lang, freq. (%)
-
-
- 78.0
3.5
3.3
2.1 0.53 0.40 0.38 0.04 0.02 0.006 0.005
PaLM
r Exemplar token length (avg.)
95
108
119
105
113
118
118
193
199
173
130
r NATIVE-COT
- 8B 6-shot
4.0
4.1
3.1
6.4
6.8
4.4
2.4
2.8
4.0
4.4
3.2
3.6
3.2
2.4
- 62B 6-shot
20.0 22.7 13.2 30.4 24.0 24.0 26.0 22.8 24.8 14.8 18.0 11.6
13.6
9.6
- 540B 1-shot 38.9 39.7 34.8 50.8 42.8 44.8 44.8 41.2 34.8 29.6 40.0 38.0
34.0
27.2
- 540B 2-shot 43.7 44.0 39.8 57.2 47.2 43.2 50.4 44.4 44.4 34.4 47.2 38.0
40.8
33.2
- 540B 4-shot 45.1 45.5 41.0 58.8 44.8 49.6 47.6 46.4 46.4 38.4 46.4 41.2
44.4
32.0
- 540B 6-shot 48.1 47.9 44.9 62.4 49.2 46.4 56.8 48.4 46.8 40.0 52.8 45.6
46.0
35.2
Table 8: Detailed performances corresponding to Figures 4 and 5.
17
C
THE CHAIN-OF-THOUGHT PROMPTS USED IN THE PAPER
In this section, we present the details of the chain-of-thought prompts used in our paper for the
XCOPA (Figure 9) and the XL-WiC (Figures 10 and 11) tasks.
Given a premise and a prompt, select the more meaningful of the two choices.  
Q: What might have happened as a result of ""Adam piyangoyu kazandı.""? 
Options: 
- ""Borçlandı."" 
- ""Zengin oldu."" 
A: Let's think step by step. 
The premise ""Adam piyangoyu kazandı."" can be translated from Turkish into English as ""The man won the lo""ery."" 
The #rst option ""Borçlandı."" can be translated as ""He owes money."", whereas the second option ""Zengin oldu."" can be translated as ""He 
became rich."" 
If the man won the lo""ery, then it makes sense that he became rich as a result. 
Therefore, the answer is ""Zengin oldu.” 
Q: What might be the cause of ""厨师的眼睛流泪了。""? 
Options: 
- ""他切了洋葱。"" 
- ""他没有洋葱了。"" 
A: Let's think step by step. 
The premise ""厨师的眼睛流泪了。"" can be translated from Mandarin Chinese into English as ""The chef's eyes #lled with tears."" 
The #rst option ""他切了洋葱。"" can be translated as ""He chopped onions."", whereas the second option ""他没有洋葱了。"" can be translated 
as ""He had run out of onions."" 
It makes sense that the chef's eyes #lled with tears because he chopped onions. 
Therefore, the answer is ""他切了洋葱。"" 
Q: What might be the cause of ""!""# $%#& '()*+,-* ./01 (2&345673#8( 9:;""#8&""? 
Options: 
- ""!<& #& =>3%& !?@7?3 'A8(BC#8&"" 
- ""!<& #& =>3%2D #8& <6<#8( <8,- .(8EC#8&"" 
A: Let's think step by step. 
The premise ""!""# $%#& '()*+,-* ./01 (2&345673#8( 9:;""#8&"" can be translated from Tamil into English as ""The man felt 
obliged to a""end the event."" 
The #rst option ""!<& #& =>3%& !?@7?3 'A8(BC#8&"" can be translated as ""He refused his friend's invitation to leave."", whereas 
the second option ""!<& #& =>3%2D #8& <6<#8( <8,- .(8EC#8&"""" can be translated as ""He had promised his friend to go."" 
It makes sense that the man felt obliged to a""end the event because he had promised his friend to go. 
Therefore, the answer is ""!<& #& =>3%2D #8& <6<#8( <8,- .(8EC#8&"" 
Q:  What might have happened as a result of ""Warmiqa wasi qhatuqwan huñukurqan.""? 
Options: 
- ""Warmiqa wasita rantinanpaqmi yuyaychakurqan."" 
- ""Warmiqa wasintam pichayta munarqan."" 
A: Let's think step by step. 
The premise ""Warmiqa wasi qhatuqwan huñukurqan."" can be translated from Cusco-Collao Quechua into English as ""The woman called a 
real estate agent."" 
The #rst option ""Warmiqa wasita rantinanpaqmi yuyaychakurqan."" can be translated as ""The woman plans to buy a condo."", whereas the 
second option ""Warmiqa wasintam pichayta munarqan."" can be translated as ""The woman needs to clean her house."" 
If the woman called a real estate agent, then it makes sense that the woman plans to buy a condo as a result. 
Therefore, the answer is ""Warmiqa wasita rantinanpaqmi yuyaychakurqan.""
XCOPA
Figure 9: The chain-of-thought prompt used in the XCOPA experiments. The four examples are
randomly selected from the validation sets of Turkish (TR), Mandarin Chinese (ZH), Tamil (TA), and
Cusco-Collao Quechua (QU). The rationales are written by the authors, and the task description is
taken directly from (Ponti et al., 2020). Under the direct prompting setup, the answers (bolded) are
given directly and rationales are entirely omitted.
18
Sentence 1: *Approach* a task.
Sentence 2: To *approach* the city.
Question: Is the word ""approach"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""approach"" means to deal with something; in Sentence 2, ""approach"" means to come near to something in distance. 
They are different, thus the answer is “No”.
Sentence 1: Der *Hausmeister* kam und er sah nichts als die Schatten unserer fleißigen Körper, die putzten und wischten.
Sentence 2: Der *Hausmeister* war hinzugekommen.
Question: Is the word ""éclater"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""Hausmeister"" means caretaker; in Sentence 2, ""Hausmeister"" means caretaker. They are the same, thus the answer 
is  “Yes”.
Sentence 1: L’intelligence *éclate* dans ses yeux.
Sentence 2: L’or et les pierreries *éclataient* de toutes parts.
Question: Is the word ""approach"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""éclate"" means shining in someone's eyes; in Sentence 2, ""éclataient"" means things are physically shining. They are 
different, thus the answer is “No”.
Sentence 1: *difettare* di denaro, di coraggio, di empatia.
Sentence 2: la statua è così realistica, *difetta* solo della parola.
Question: Is the word ""difettare"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""difettare"" means being lack of something; in Sentence 2, ""difetta"" means be lack of something. They are the same, 
thus the answer is “Yes”.
XL-WiC
Figure 10: The multilingual chain-of-thought prompt used in the XL-WiC experiments.
19
Sentence 1: *Approach* a task.
Sentence 2: To *approach* the city.
Question: Is the word ""approach"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""approach"" means to deal with something; in Sentence 2, ""approach"" means to come near to something in distance. 
They are different, thus the answer is “No”.
Sentence 1: Der *Hausmeister* kam und er sah nichts als die Schatten unserer fleißigen Körper, die putzten und wischten.
Sentence 2: Der *Hausmeister* war hinzugekommen.
Question: Is the word ""éclater"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""Hausmeister"" means caretaker; in Sentence 2, ""Hausmeister"" means caretaker. They are the same, thus the answer 
is  “Yes”.
Sentence 1: L’intelligence *éclate* dans ses yeux.
Sentence 2: L’or et les pierreries *éclataient* de toutes parts.
Question: Is the word ""approach"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""éclate"" means shining in someone's eyes; in Sentence 2, ""éclataient"" means things are physically shining. They are 
different, thus the answer is “No”.
Sentence 1: *difettare* di denaro, di coraggio, di empatia.
Sentence 2: la statua è così realistica, *difetta* solo della parola.
Question: Is the word ""difettare"" (marked with *) used in the same sense in both sentences above?
Options:
- Yes
- No
Answer: In Sentence 1, ""difettare"" means being lack of something; in Sentence 2, ""difetta"" means be lack of something. They are the same, 
thus the answer is “Yes”.
XL-WiC
Figure 11: The English-language chain-of-thought prompt used in the XL-WiC experiments.
20
</Content></Document>
<Document index=3><Published>2024-02-27</Published><Title>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</Title><Authors>KaShun Shum, Shizhe Diao, Tong Zhang</Authors><Summary>Chain-of-thought (CoT) advances the reasoning abilities of large language
models (LLMs) and achieves superior performance in complex reasoning tasks.
However, most CoT studies rely on carefully designed human-annotated rational
chains to prompt LLMs, posing challenges for real-world applications where
labeled data is available without rational chains. This paper proposes a new
strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with
Chain-of-Thought), that can bypass human engineering of CoT by automatically
augmenting rational chains from a small labeled dataset, and then pruning
low-quality chains to construct a candidate pool of machine-generated rationale
chains based on the labels. Finally, it selects the optimal combination of
several rationale chains from the pool for CoT prompting by employing a
variance-reduced policy gradient strategy to estimate the significance of each
example. Automate-CoT enables a quick adaptation of the CoT technique to
different tasks. Experimental results demonstrate the effectiveness of our
method, where competitive results are achieved on arithmetic reasoning (+2.7%),
commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning
tasks (+2.5%). The code is available at
https://github.com/SHUMKASHUN/Automate-CoT.</Summary><Source>https://arxiv.org/pdf/2302.12822.pdf</Source><Content>Automatic Prompt Augmentation and Selection
with Chain-of-Thought from Labeled Data
Kashun Shum♡∗, Shizhe Diao♡∗, Tong Zhang♡
♡The Hong Kong University of Science and Technology
{ksshumab, sdiaoaa, tongzhang}@ust.hk
Abstract
Chain-of-thought (CoT) advances the reason-
ing abilities of large language models (LLMs)
and achieves superior performance in complex
reasoning tasks. However, most CoT studies
rely on carefully designed human-annotated
rational chains to prompt LLMs, posing chal-
lenges for real-world applications where la-
beled data is available without rational chains.
This paper proposes a new strategy, Automate-
CoT (Automatic Prompt Augmentation and
Selection with Chain-of-Thought), that can by-
pass human engineering of CoT by automati-
cally augmenting rational chains from a small
labeled dataset, and then pruning low-quality
chains to construct a candidate pool of machine-
generated rationale chains based on the labels.
Finally, it selects the optimal combination of
several rationale chains from the pool for CoT
prompting by employing a variance-reduced
policy gradient strategy to estimate the signif-
icance of each example. Automate-CoT en-
ables a quick adaptation of the CoT technique
to different tasks. Experimental results demon-
strate the effectiveness of our method, where
competitive results are achieved on arithmetic
reasoning (+2.7%), commonsense reasoning
(+3.4%), symbolic reasoning (+3.2%), and non-
reasoning tasks (+2.5%).1
1
Introduction
The recent success in large language models
(LLMs) has shown that properly prompted LLMs
demonstrate emergent capabilities on complex un-
derstanding and question-answering tasks (Wei
et al., 2022a). Especially, with the recently pro-
posed chain-of-thought (CoT) prompting (Wei
et al., 2022b), LLMs are capable of solving rea-
soning tasks including arithmetic reasoning, com-
monsense reasoning, and symbolic reasoning. The
*Equal Contribution.
1The
code
is
available
at
https://github.com/
SHUMKASHUN/Automate-CoT.
basic idea of CoT prompting is adding a few ra-
tionale chains to the answer as exemplars to illus-
trate the intermediate reasoning steps. Following
CoT, several recent studies improve it by leveraging
self-consistency (Wang et al., 2023), explanation
learning (Lampinen et al., 2022), complexity-based
prompting (Fu et al., 2023), self-training (Huang
et al., 2022), voting verifier (Li et al., 2022a), and
bootstrapping (Zelikman et al., 2022).
However, most of them are constrained to a few
fixed human-written exemplars, which require sig-
nificant human efforts to create and adapt to new
datasets. The annotation process is nontrivial be-
cause humans need to not only select the ques-
tions but also carefully design the reasoning steps
for each question. In the process of searching for
the perfect exemplars, we identify four critical fac-
tors that affect the performance of chain-of-thought
prompting and require large human effort to deal
with: (1) order sensitivity (Zhao et al., 2021): the
order combination of the exemplars; (2) complex-
ity (Sugawara et al., 2018; Lai et al., 2021; Fu
et al., 2023): the number of reasoning steps of
the rationale chains; (3) diversity: the combina-
tion of different complex-level exemplars; (4) style
sensitivity (Papadopoulos et al., 2010): the writ-
ing/linguistic style of the rationale chains. Detailed
analysis of the four factors is covered in Section 2.
All of these sensitivities make human-based prompt
engineering costly and motivate us to find an au-
tomatic and task-agnostic way to adapt chain-of-
thought exemplars to any downstream tasks.
In this paper, we solve these problems by a CoT
augmentation and selection process to find suitable
exemplars automatically. This can be divided into
three steps: (1) Augment: The language model
generates multiple pseudo-chains for query ques-
tions automatically. (2) Prune: Based on an as-
sumption: Generating correct reasoning is a nec-
essary condition for generating correct answers.
This assumption is natural because the answer is
arXiv:2302.12822v3  [cs.CL]  27 Feb 2024
generated after several reasoning steps. When a
correct answer is generated, the rationale chain
of these steps is most likely correct, contributing
to the final correctness. As a result, We prune
the pseudo-chains according to the consistency be-
tween generated and ground-truth answers to re-
duce the noise. (3) Select: Given that all the data
have been annotated with rationale paths, we pro-
pose to apply a variance-reduced policy gradient
strategy (Williams, 1992; Dong et al., 2020; Zhou
et al., 2021; Diao et al., 2022) to estimate the gradi-
ents and optimize the selection process to find the
most helpful chain-of-thought for each task. Com-
pared to prior manually written CoT, Automate-
CoT could find the optimal and diverse CoT au-
tomatically, adaptable to any task without human
effort. Compared with Auto-CoT (Zhang et al.,
2023), which samples diverse questions by clus-
tering and generates rationale chains, Automate-
CoT considers and mitigates the aforementioned
sensitivity issues, while achieving a greater per-
formance boost for each task. Automate-CoT is
a fully automatic pipeline for finding better chain-
of-thought prompts, mitigating the sensitivity is-
sues of manually written exemplars, and further
improving the performance by a large margin. Ex-
perimental results demonstrate the effectiveness of
Automate-CoT on arithmetic reasoning (+2.7%),
commonsense reasoning (+3.4%), symbolic reason-
ing (+3.2%), and non-reasoning tasks (+2.5%).
2
Motivation
Recent studies observed sensitivity issues of GPT-
3’s few-shot learning caused by different selec-
tions of in-context examples such as order instabil-
ity (Zhao et al., 2021; Zhang et al., 2022; Liu et al.,
2022; Lu et al., 2022). Based on their findings,
we first investigate whether these sensitivities still
exist in chain-of-thought methods. Then we further
explore other factors that would not only affect the
performance but require human efforts to deal with.
We conclude with the following four factors:
• Order Sensitivity: Different orders of few-shot
exemplars may cause a huge impact on the per-
formance in traditional few-shot prompting (Lu
et al., 2022). Thus we conduct experiments on
GPT-3 to test if there is such sensitivity in chain-
of-thought methods. Although Manual-CoT (Wei
et al., 2022b) reports that the human-written CoT
is robust to order changes (<2%) with the LaMDA
model, we observed that the performance of GPT-3
Figure 1: The performance across different numbers of
hops (reasoning steps of rationale chains) on GSM8K.
Manual-CoT refers to the human-written chain-of-
thought by Wei et al. (2022b). Complex-CoT refers
to the chain-of-thought using 9-hop rationale chains.
fluctuates with different orders of chain-of-though
exemplars. For the GSM8K dataset, we simply
randomly shuffle the order of the exemplars in
Manual-CoT 10 times and the lowest accuracy can
be 59.8% which is 3.3% lower than the average
accuracy (63.1%) they report, suggesting that order
sensitivity still exists.
• Complexity: We first define complexity as the
number of hops (reasoning steps) in an exemplar
where more steps indicate greater complexity. It
is observed that human-written CoT tends to be
simple (≤3 hops), achieving good accuracy in sim-
ple math questions while suffering from complex
questions, as shown in Figure 1. In addition, a pre-
vious study (Fu et al., 2023) suggested that using all
complex exemplars can improve CoT performance.
However, in our experiments (Figure 1), we found
that Complex-CoT can improve the accuracy of
complex questions, but perform poorly in simple
questions. Therefore, we conjecture that the incon-
sistency between the hops of provided exemplars
and the required hops of the real question causes
the performance drop, suggesting that determining
the appropriate complexity level of exemplars is
crucial.
• Diversity: Based on the above discovery about
complexity, a natural question is what combination
of different complex-level exemplars is most effec-
tive. However, testing various combinations is a
challenging task for humans and requires signif-
icant effort to determine the optimal one. In our
Test Question
Q: There are 15 trees in the grove. Grove workers will…
A: There are 15 trees originally…… The answer is 6.
1
2
3
9
…
……
K-1
K
K-2
1
2
3
9
…
……
K-1
K
K-2
…
1
2
3
9
…
……
K-1
K
K-2
Pool Size = K
Inference Stage
Q: Olivia has $23. She bought five bagels for $3 each…
A: Olivia had 23 dollars. 5 bagels…… The answer is 8.
Q: Natalia sold clips to 48 of her friends in April, and …
Manual-CoT
…
+
Pool Construction Question
(1) Augment
LLM 
A: She sold 48 clips in April. In May she sold half as many, 
so she sold 48 / 2 = 24 clips. In total she sold 48 + 24 = 72 
clips. The answer is 72.
Ground Truth : 72
Match
Not Match
Add to Pool
Drop
Prune
(2) Select
Q: Ralph is going to practice playing tennis with a tennis ball …
A:Ralph started with 175 tennis balls. He hit 2/5 of the first 100 
balls, so he hit 2/5 * 100 = 40 balls. He hit 1/3 of the next 75 
balls, so he hit 1/3 * 75 = 25 balls. In total he hit 40 + 25 = 65 
balls. He did not hit 175 - 65 = 110 balls. The answer is 110.
Q: Janet’s ducks lay 16 eggs per day. She eats three for 
breakfast…
…
+
5-hop
3-hop
Q: Hans booked a room in a hotel. The hotel has 10 floors …
A: here are 10 floors with 10 rooms each. The last floor is 
unavailable. So there are 9 * 10 = 90 rooms available.The 
answer is 90.
ℒ(prediction, label)
Estimate Gradient :
✅
❌
Index ~
Training Stage
+
Training 
Batch
……
✅
✅
6-hop
2-hop
OR
Lack of Manual-CoT (Zero-shot)
Q: Natalia sold clips to 48 of her friends in April, and …
A: Let’s think step by step.
…
…
Index = argmax 
= [3, 9, … , K]
Update 
𝒈𝒑𝒊
𝒗𝒓=
𝟏
𝑰−𝟏&
𝒌%𝟏
𝑰
𝓛𝑻𝒌
−𝟏
𝑰&
𝒋%𝟏
𝑰
𝓛𝑻𝒋
𝜵𝒑𝒊𝒍𝒐𝒈𝑷(𝒕𝒊)
𝒑𝒊←𝒑𝒓𝒐𝒋𝑪𝒑𝒊−𝜼( 𝒈𝒑𝒊
𝒗𝒓, 𝒊= 𝟏, (((, 𝒏
𝒑𝒊
Figure 2: Illustrations of our proposed approach. The left and middle parts of the figure contain two steps of our
method: (1) Augment and Prune and (2) Select. The right part illustrates the training stage (right top) and the
inference stage (right bottom), respectively.
experiments (Figure 1), we found that a combina-
tion of different complex-level exemplars outper-
forms CoT with all complex exemplars, suggesting
a complexity-diversity trade-off.
• Style Sensitivity: Previous research in educa-
tional psychology found that different learning
styles would limit the cognitive benefit for students
from the prompting (Papadopoulos et al., 2010).
We further argue that students with specific learn-
ing styles benefit to varying degrees from different
styles of prompting. In addition, the empirical evi-
dence from Manual-CoT (Wei et al., 2022b) shows
that different annotators can cause up to 28.2%
accuracy difference in a symbolic reasoning task,
verifying our conjecture. As a result, some bad
styles may lead to a huge performance drop. How-
ever, humans cannot determine the performance of
a particular style beforehand, so it requires trial and
error by checking on the validation set, which fur-
ther increases the effort of writing chain-of-thought
exemplars.
In light of this empirical evidence, we are moti-
vated to design a framework not only to augment
rationale chains but also to select helpful chains
adaptively. With this framework, it is expected to
bypass the order and style sensitivities and reach
a better complexity-diversity trade-off without hu-
man effort, finally boosting performance.
3
Approach
Our approach receives a training dataset D con-
taining n questions Q = {q1, q2, ..., qn}, and n
answers A = {a1, a2, ..., an}. The overall archi-
tecture of our approach is shown in Figure 2. In
this section, we start with a detailed description
of augment and prune operation and end with an
illustration of select operation.
3.1
Augment and Prune
Inspired by Wang et al. (2022), which shows that
the generated rationale chains are of comparable
quality to the human-annotated ones, we aim to
automatically generate the rationale chains to aug-
ment the candidate exemplars. Given m fixed ra-
tionale chains C = {c1, c2, ..., cm}, a question q,
we ask the large language model G to generate k
rationale chains for each q. A larger k can form a
larger pool and some post-processes can be done
to improve the quality of the pool. Considering the
cost and efficiency, we choose k = 1 for our ex-
periments. Our method works well even without C
(i.e., m = 0), which is based on zero-shot prompt-
ing. Then we prune those incorrect ones out and
only keep those with the correct final answer. In
other words, the final answer should be consistent
with the ground-truth answer. After pruning, we
obtain a pool of K high-quality exemplars.
3.2
Select
With a large pool of high-quality exemplars, we
cannot directly apply all of them due to four con-
siderations: (1) context length limit: the maximum
length is 2,048 for GPT-3, so we cannot feed too
many exemplars into the model. (2) fair compar-
ison: existing studies usually take 4-8 question-
answer pairs as exemplars following Wei et al.
(2022b). (3) sensitivity: the model performance
may be sensitive to the contexts (Jiang et al., 2020),
orders (Lu et al., 2022) and lengths (Lester et al.,
2021) from the observation of prompt learning lit-
erature. (4) adaptation: different downstream tasks
may require different exemplars. Therefore, a natu-
ral idea is to select the most suitable 4-8 exemplars
automatically.
The process can be deemed as optimizing a su-
pervised model with latent variables. For each
chain-of-thought index i, we initialize a latent vari-
able ji ∼Cat(pi). The random variable ji is
sampled with the probability distribution pi =
[pi,1, · · · , pi,N] over the N candidate demonstra-
tion indexes, where pi ∈C and C = {p : ∥p∥1 =
1, 0 ⪯p ⪯1}. Since pi is independent of each
other, the joint probability of the whole input ex-
emplars is P(T) = Πn
i=1P(ti) = Πn
i=1pi,ji. The
loss is formulated as L(G([T, S], y)), where T rep-
resents the full few-shot exemplars, ti denotes the
i-th exemplar, and S is the current question (user’s
query). However, directly updating the prompts
by back-propagating through ∇piL(G([T, S], y))
is not possible because of the inaccessible gra-
dients, where y is the label.
We resort to the
variance-reduced policy gradient estimator (VR-
PGE) (Williams, 1992; Dong et al., 2020; Zhou
et al., 2021; Diao et al., 2022), a kind of reinforce-
ment learning method to optimize the loss function
via forward propagation with:
ET [L(T)] =
Z
L(T)P(T) dT,
(1)
and estimate the gradient of pi by:
gvr
pi =
1
I −1
I
X
k=1
 
L(T (k)) −1
I
I
X
j=1
L(T (j))
!
∇pi log P(ti)
(2)
where T (k), k = 1, · · · , I are sampled indepen-
dently from P(T). Therefore, the exemplar distri-
bution pi can be updated by a projected stochastic
gradient descent algorithm:
pi ←projC(pi −η · gvr
pi), i = 1, · · · , n
(3)
where η is the learning rate, I is the sample size,
and projC is the projection calculation (details are
presented in the Appendix A).
4
Experimental Settings
In this section, we first introduce the setting of
eleven datasets and their corresponding evaluation
metrics (§ 4.1). Then the baseline models (§ 4.2)
and implementation details (§ 4.3) are presented in
the following two subsections, respectively. Full
details about the experimental setting are illustrated
in Appendix B.
4.1
Datasets and Evaluation Metrics
Following Wei et al. (2022b), we conduct our exper-
iments on eight reasoning tasks, including five math
word problem datasets: GSM8K, ASDiv, SVAMP,
AQuA, and SingleOp; two commonsense reason-
ing datasets: CommonsenseQA (CSQA) and Strat-
egyQA, and one symbolic reasoning task: Last
Letter Concatenation (Letter (4)). We also gener-
alize our method to non-reasoning tasks including
one question-answering task (OpenBookQA), one
natural language inference task (e-SNLI), and one
sentiment analysis task (SST-2). The detailed statis-
tics of the datasets are listed in Table 5. The evalua-
tion metric for all tasks is the exact match accuracy.
First, we conduct pre-processing for predictions
to remove all the special symbols. For example,
""$100,000"" will be processed to ""100000"". Then
we check if it has the same value as the ground
truth to calculate the exact match accuracy.
4.2
Baselines
We compare our method with the following
baseline methods:
chain-of-thought (Manual-
CoT)
(Wei
et
al.,
2022b),
self-consistency
(SC) (Wang et al., 2023), and Auto-CoT (Zhang
et al., 2023).
And we utilize the public
APIs from OpenAI’s services2 and test with
text-davinci-002 and code-davinci-002.
4.3
Implementation
Augment and Prune: Following Wei et al.
(2022b) and Wang et al. (2022), we keep the same
number of exemplars (4-8) listed in Table 5. For
main experiments, we augment and prune a pool
of 100 high-quality exemplars for all datasets.
2https://openai.com/api/
METHOD
GSM8K
ASDIV
SVAMP
AQUA
SINGLEOP
CSQA
STQA
LETTER (4)
OBQA
E-SNLI
SST-2
AVG.
Prior Best*
55.0a
75.3b
57.4c
37.9d
-
91.2e
73.9f
-
-
-
97.5g
-
text-davinci-002
Auto-CoT
47.9
-
69.5
36.5
-
74.4
65.4
59.7
-
-
-
-
Manual-CoT
46.9
71.3
68.9
35.8
88.8
73.5
65.4
56.6
75.5
79.1
86.2
68.0
+ Automate-CoT
49.7↑2.8
74.2↑2.9
73.3↑4.4
37.9↑2.1
90.0↑1.2
76.1↑2.6
67.9↑2.5
58.9↑2.3
79.1↑3.6
82.3↑3.2
87.5↑1.3
70.6↑2.6
SC
58.2
76.9
78.2
41.8
90.8
72.9
70.7
57.6
81.5
83.4
89.2
72.8
+ Automate-CoT
67.8↑9.6
78.9↑2.0
80.5↑2.3
43.4↑1.6
91.9↑1.1
80.2↑7.3
76.3↑5.6
60.8↑3.2
84.8↑3.3
86.4↑3.0
90.6↑1.4
76.5↑3.7
code-davinci-002
Auto-CoT
62.8
-
-
-
-
-
-
-
-
-
-
-
Manual-CoT
63.1
80.4
76.4
45.3
91.8
77.9
73.2
70.4
80.4
67.5
89.7
74.2
+ Automate-CoT
67.6↑4.5
83.1↑2.7
78.2↑1.8
47.8↑2.5
92.4↑0.6
81.3↑3.4
75.3↑2.1
75.0↑4.6
83.2↑2.8
71.2↑3.7
90.8↑1.1
76.9↑2.7
SC
78.0
87.8
86.8
52.0
92.8
81.5
79.8
73.4
88.4
74.8
91.5
80.6
+ Automate-CoT
82.4↑4.4
88.9↑1.1
87.8↑1.0
55.6↑3.6
94.0↑1.2
84.0↑2.5
80.6↑0.8
76.2↑2.8
89.7↑1.3
78.3↑3.5
92.8↑1.3
82.8↑2.2
Table 1: The overall performance of Automate-CoT and the comparison against existing models on eleven
downstream tasks. Manual-CoT and SC represent chain-of-thought (Wei et al., 2022b) and self-consistency (Wang
et al., 2023) methods. Bold denotes the best in code-davinci-002-based methods and Underline denotes the best
in text-davinci-002-based methods. *: Prior Best is the best performance before CoT comes out. a: Cobbe et al.
(2021), b: Lan et al. (2022), c: Pi et al. (2022), d: Amini et al. (2019), e: Xu et al. (2022), f: Chowdhery et al. (2022),
g: Raffel et al. (2020). Most statistics of Manual-CoT and SC have been obtained directly from their latest version.
For some entries they did not report, we obtain the result from DIVERSE (Li et al., 2022b).
Select: Both the training and validation sets have
a size of 100 to reach a performance and cost trade-
off. Then by utilizing the log probability returned
by API calls, we calculate the cross-entropy loss
of the answer token. Finally, we optimize the la-
tent variables by AdamW (Loshchilov and Hutter,
2019) for 5 epochs with a learning rate of 1 × 10−3
and batch size of 10. After optimization, we choose
the exemplars combination (arg max pi) with the
highest validation accuracy to be further evaluated
on the test set. By default, we query the language
model once to get the answer. Under the self-
consistency setting, similar to Wang et al. (2023),
we query the language model 40 times and choose
the most consistent one as the final answer.
Hyper-parameter Setting: Under few-shot set-
ting, we set max_tokens = 256 for all augmenta-
tion, selection and inference. In addition, we set
logprobs = 5 when training. Moreover, we set tem-
perature = 0.7 for evaluation under self-consistency
while temperature = 0 for all other cases.
5
Experimental Results
The experimental results are shown in Table 1. We
discuss our results in three sections based on the
task categories. Automate-CoT are averaged over
three runs, and the variance over different runs is
reported in Appendix Table 7. Overall, Automate-
CoT achieves superior results on all tasks. With
text-davinci-002, Automate-CoT outperforms
Manual-CoT and SC by 2.6% and 3.7% on average.
With code-davinci-002, Automate-CoT also out-
performs Manual-CoT and SC by 2.7% and 2.2%,
respectively.
Arithmetic Reasoning: For text-davinci-002,
Automate-CoT improves Manual-CoT by 2.7%
over five arithmetic reasoning tasks.
In addi-
tion, under the self-consistency setting, Automate-
CoT improves SC by a large margin by an av-
erage of 3.3%.
Moreover, compared to Auto-
CoT, Automate-CoT also outperforms it on all
three arithmetic tasks (GSM8K, SVAMP, and
AQuA). While for code-davinci-002, Automate-
CoT achieves an average of 2.4% improvement
across all five arithmetic reasoning tasks, illustrat-
ing the effectiveness of our proposed approach with
different language models. Additionally, Automate-
CoT outperforms Auto-CoT in GSM8K by 4.8%,
since Auto-CoT only constructs experiments on
GSM8K under code-davinci-002.
Automate-
CoT demonstrates consistent improvement over
arithmetic tasks, especially on GSM8K, where it
can outperform Manual-CoT by a large margin. Fi-
nally, under the self-consistency setting, Automate-
CoT also shows similar trends to improve the SC
baseline, demonstrating the synergistic effects of
our proposed method and self-consistency method.
Commonsense and Symbolic Reasoning Sim-
ilarly, on commonsense and symbolic reason-
ing tasks, Automate-CoT demonstrates signifi-
cant improvement over Manual-CoT, SC, and
Auto-CoT. It achieves an average of 2.5% and
Figure 3: Comparisons between Random Selection, Manual-CoT and Automate-CoT on six datasets.
3.4% improvement on text-davinci-002 and
code-davinci-002 respectively, demonstrating
that our method is effective on different task types.
More surprisingly, the improvement in the Letter
(4) is significant, demonstrating our method’s ro-
bustness to deal with out-of-distribution data.
Non-Reasoning Tasks Automate-CoT has also
reached
great
success
on
question
answer-
ing (OpenBookQA), natural language inference (e-
SNLI), and sentiment analysis (SST-2) tasks by
an improvement of 2.8%, 3.4% and 1.3%, respec-
tively. The results show that our method can be
generalized to various task types and is not limited
to reasoning tasks.
6
Additional Experiments and Analysis
We further conduct several experiments to evaluate
the effectiveness of Automate-CoT and analyze
the contributions of each module. Since queries
to text-davinci-002 are limited and expensive,
most additional experiments are conducted with
code-davinci-002.
6.1
Effects of Selection Algorithm
After obtaining a large pool of exemplars, a natural
question would be what is the performance if we
randomly select from the pool regardless of order.
In Figure 3, we compare the accuracy obtained by
random selection, human-written (Manual-CoT),
and our Automate-CoT. For random selection, we
randomly sample exemplars from the pool and com-
bine them regardless of order to form the prompts.
We repeat this process five times and report the
accuracy with an error bar. The results show that
random selection suffers from high variance and rel-
atively low accuracy compared to Manual-CoT and
Automate-CoT. Surprisingly, we observed the aver-
age performance of a random selection from model-
generated exemplars can outperform Manual-CoT
in some datasets (e.g. GSM8K, CSQA). This also
suggests that manual prompt engineering needs to
take efforts to design carefully in terms of difficulty,
Figure 4: The performance across different pool sizes
of Automate-CoT compare with Manual-CoT. Pool size
refers to the number of exemplars in the pool.
diversity, and style. In conclusion, if we simply ran-
domly select the exemplars from the pool, it is very
likely to obtain a much lower accuracy than the
manually written method. However, our Automate-
CoT can consistently outperform random selection
and Manual-CoT which shows the effectiveness of
our method.
6.2
Effects of Pool Size
We further conduct a set of experiments to test dif-
ferent pool sizes. As shown in Figure 4, if the
pool size is limited to only 10, the performance of
Automate-CoT is worse than Manual-CoT or com-
parable with Manual-CoT. It turns out that if the
pool size is small, Automate-CoT is unable to se-
lect a good combination to beat carefully designed
Manual-CoT. However, Automate-CoT can outper-
form Manual-CoT when the pool size reaches 20
or larger. The trends show that the performance
would be better as pool size keeps increasing. This
is intuitive and matches our hypothesis because as
pool size increases, there would be more complex,
diverse exemplars to choose from. It is expected
that the performance would keep increasing, but
since more queries for GPT-3 are time-consuming
and expensive, we limited these additional experi-
ments to have a max pool size of 150.
6.3
Effects of Chain Complexity
It is observed that exemplars written by human
are rather simple, so we further explore how chain
complexity affect performance. We randomly pick
RUNS
GSM8K
SVAMP
Letter(4)
Rand(Training Set)1
67.55
78.2
75.0
Rand(Training Set)2
67.93
77.8
76.6
Rand(Training Set)3
67.25
77.6
75.8
Variance
0.077
0.062
0.426
Table 2: The effect of different randomly chosen train-
ing set on performance over three datasets.
8 exemplars with complex rationale chains (each
has 9 hops) and refer to them as Complex-CoT.
For human-written exemplars (Manual-CoT) Wei
et al. (2022b), exemplars are all 2-3 hops. We com-
pare them with our Automate-CoT which has an
average hop of 4 and ranges from 2-hop to 6-hop
on GSM8K dataset. From Figure 1, Manual-CoT
has an overall accuracy of 62%, achieving good
results on simple questions. However, it suffers
from complex math questions, especially 7-hop
and 8-hop questions. Complex-CoT can improve
the accuracy on complex questions by a large mar-
gin but it performs poorly on simple questions,
which only has an overall accuracy of 60%. In
contrast, our Automate-CoT can select a combina-
tion of different complex-level exemplars automat-
ically. It achieves good results on simple questions
and reasonable results on complex questions at the
same time, outperforming both Manual-CoT and
Complex-CoT by a large margin. The result shows
the superiority of our method because it can auto-
matically achieve a complexity-diversity trade-off.
6.4
Effects of Training Example Selection
Since training examples to construct CoT are ran-
domly chosen, we also measure the performance
vary regarding this random selection. Three dif-
ferent randomly chosen training sets are used to
train Automate-CoT and the results are reported in
Table 2. According to the result, Automate-CoT
shows its robustness to training examples. Ran-
domly chosen training examples have quite a small
impact on the result.
6.5
Bypass Manual Effort by Zero-shot-CoT
Starting with 4-8 manually constructed chain-of-
thought exemplars, our methods show great success
in automatically generating, pruning, and selecting
suitable exemplars for each task. After that, we
raise a new question: Can we further bypass the
effort of writing the initial chain-of-thought exem-
plars? Based on current research of Zero-Shot-
CoT (Kojima et al., 2022), we found it is possible.
METHOD
GSM8K
SVAMP
Letter (4)
Zero-Shot-CoT
40.7
62.1
57.6
Manual-CoT
46.9
73.5
56.6
Auto-CoT
48.9
69.5
59.7
Zero-Shot-Automate-CoT
49.1
74.3
59.3
Automate-CoT
49.7
76.1
58.9
Table 3: The performance of Automate-CoT in zero-
shot setting compared with other baselines. Lightgray
highlights our main model which uses a manually con-
structed chain-of-thought and is not intended for com-
parison. We list it here only for reference.
Instead of using 4-8 manual-written exemplars to
generate the chains, we simply add ""Let’s think step
by step."" and let LLMs generate the chains. We
test the result under text-davinci-002 model on
GSM8K, SVAMP, and Letter (4) and compare it
with Zero-shot-CoT, Manual-CoT and Auto-CoT.
Surprisingly, we observe the result can be compa-
rable and even outperform Manual-CoT and Auto-
CoT a bit as shown in Table 3. The results fur-
ther demonstrate that our method can effectively
select a suitable combination of exemplars even
from a pool that may contain low-quality chains.
In conclusion, if a dataset already has manually
written chains, our method can be applied to boost
the performance. If a dataset does not have man-
ually written chains, our method can still be used
to achieve higher accuracy than if it had manually
written chains, demonstrating the superiority of our
method.
7
Ablation Study
In this section, we further conduct ablation exper-
iments to verify the advantage of the generated
prompts on four factors, respectively.
Advantage over Order Factor The advantages
of Automate-CoT on order factor can be viewed in
two ways. Firstly, it requires a large human effort
to determine a good order by trying many different
orders on validation sets. However, Automate-CoT
can automatically construct the exemplars without
further adjustment to have a good result. Secondly,
Automate-CoT is less affected by the order sensitiv-
ity. We further conduct an experiment to compare
selected exemplars and random permutations of
Automate-CoT’s selected exemplars as shown in
Table 4. We randomly permutate the selected exem-
plars to see how performance varies compared to
the selected order by Automate-CoT. It is observed
that the order sensitivity still exists and our se-
RUNS
GSM8K
SVAMP
Letter(4)
perm(Automate-CoT)1
66.7
77.2
73.0
perm(Automate-CoT)2
66.6
78.4
72.6
perm(Automate-CoT)3
66.9
78.0
72.0
perm(Automate-CoT)4
67.8
78.2
74.2
perm(Automate-CoT)5
67.5
78.1
75.0
Automate-CoT
68.4
78.7
75.2
Mean±std
67.3±0.64
78.2±0.46
73.7±1.21
Table 4: Comparison of different permutations orders
of Automate-CoT’s selected examplars.
lected exemplars have better performance than that
of all 5 random permutation runs, demonstrating
Automate-CoT can automatically choose a good
order without any human effort.
Advantage over Complexity Factor As dis-
cussed in the complexity factor of Section 2, we
show that the complexity of manually written
chains is quite simple (less than or equal to 3 hops).
It would require more human effort to design com-
plex rationales. However, Automate-CoT can auto-
matically augment and select examples with differ-
ent complexity, reaching a better trade-off accuracy
between simple questions and complex questions
(Appendix Table 9).
Advantage over Diversity Factor The diversity
of Manual-CoT or Complexity-CoT is limited.
For example, every exemplar of Complexity-CoT
has the same complexity and every exemplar of
Manual-CoT ranges from 1-3 hops as illustrated in
the motivation section. However, Automate-CoT
can automatically select an optimal combination of
complexity that best suits the dataset. For example,
our selected exemplars on GSM8K have an aver-
age hop of 5.4 and range from 3 hops to 8 hops
as shown in Appendix G. It contains both simple
exemplars and complex exemplars which reach the
best performance.
Advantage over Style Factor Our extensive ex-
perience with multiple experiments indicates that
a good linguistic style is typically formal and de-
tailed. This style entails the use of (1) explicit and
logical connection words (e.g., ""so"", ""that means""),
(2) detailed reasoning steps within a single sen-
tence, (3) symbols when appropriate (e.g., using the
$ symbol to denote monetary values), and (4) min-
imizing the use of abbreviations. We further con-
duct an ablation experiment to test how our method
can choose the examples with better style. Firstly,
we use Automate-CoT to select 8 rationale exem-
plars S1 = [A1, B1, C1, D1, E1, F1, G1, H1] for
GSM8K. Then we copy this set and edit its written /
linguistic style manually to be worse while keeping
the order, complexity, and diversity the same which
gives S2
=
[A2, B2, C2, D2, E2, F2, G2, H2].
Now
we
have
16
examplars
says
S
=
[A1, A2, B1, B2, ..., H1, H2]. A-H represents the
No.1-8 exemplars. Subscript 1 represents the origi-
nally selected exemplars and 2 represents the edited
ones. Then, Automate-CoT selects 8 exemplars
from the previous 16 exemplars. Note that we limit
Automate-CoT to select exactly one of [A1, A2]
and [B1, B2] ... and keep the same order A-H.
Subsequently, when we perform Automate-CoT al-
gorithm , we observe that Automate-CoT is able to
successfully select the original exemplars S1. Fur-
thermore, we find that the selected exemplars can
outperform the non-selected exemplars by 2%.
8
Related Work
In this section, we first review the recent progress of
prompt-based learning (§8.1) and chain-of-thought
prompting (§8.2), and then discuss the black-box
optimization methods (§8.3).
8.1
Prompt-based Learning
Prompt-based Learning (Prompting) aims to lever-
age large language models (LLMs) (Devlin et al.,
2018; Liu et al., 2019; He et al., 2021; Diao et al.,
2020, 2021) to trigger helpful knowledge for down-
stream tasks. Existing prompting methods can be
categorized into two types based on their nature:
1) discrete prompts (Wallace et al., 2019; Shin
et al., 2020; Jiang et al., 2020; Yuan et al., 2021;
Haviv et al., 2021; Gao et al., 2021; Ben-David
et al., 2022; Davison et al., 2019; Su et al., 2022;
Diao et al., 2022; Guo et al., 2023) and continuous
prompts (Zhong et al., 2021; Qin and Eisner, 2021;
Hambardzumyan et al., 2021; Liu et al., 2021; Han
et al., 2021; Li and Liang, 2021; Yang et al., 2023).
Discrete prompts optimize a sequence of discrete
tokens, while continuous prompts optimize a se-
quence of vectors. One of the most important ad-
vantages of prompting is saving fine-tuning costs
by refraining from the parameter changes of large
language models, and we only need to optimize a
small set of parameters.
8.2
Chain-of-thought Prompting
Chain-of-thought (Wei et al., 2022b) introduces
a chain of rationale steps for each exemplar of
in-context learning and significantly improves the
performance on several complex tasks like arith-
metic reasoning, commonsense reasoning, and
symbolic reasoning. Based on this simple yet effec-
tive idea, many following works propose different
strategies to improve it: self-consistency (Wang
et al., 2023), explanation learning (Lampinen et al.,
2022), complexity-based prompting (Fu et al.,
2023), self-training (Huang et al., 2022), voting
verifier (Li et al., 2022a), zero-shot prompting (Ko-
jima et al., 2022; Fung et al., 2022), and bootstrap-
ping (Zelikman et al., 2022).
8.3
Black-box Optimization
Nowadays, large language models provide services
as commercial APIs deployed in the cloud, such
as OpenAI’s GPT-3 (Brown et al., 2020) and Chat-
GPT3. It usually accepts query inputs and outputs
the predictions with a web interface. Their model
parameters and gradients are not accessible, caus-
ing difficulties in optimization with gradients. Pre-
vious research on black-box optimization mainly
focuses on score-based black-box adversarial at-
tack (Ilyas et al., 2018, 2019; Huang and Zhang,
2020; Andriushchenko et al., 2020; Cheng et al.,
2019).
Most recently, black-box prompt learn-
ing (Diao et al., 2022; Sun et al., 2022; Prasad
et al., 2022) is introduced, aiming to optimize the
prompts without accessing gradients, but their mod-
els suffer from limited reasoning abilities and are
limited to zero-shot settings with classification task.
9
Conclusion
In this paper, we proposed a chain-of-thought op-
timization method consisting of three steps: aug-
ment, prune, and select. Automate-CoT first gener-
ates rationale chains according to the standard CoT
process with several exemplars, and then prunes
those incorrect ones according to the consistency of
the predicted answer and ground-truth answer. Fi-
nally, we apply a variance-reduced policy gradient
strategy to estimate the gradients and optimize the
latent variables to select better CoTs. Experimental
results demonstrate the effectiveness of our method
on arithmetic reasoning, commonsense reasoning,
symbolic reasoning tasks, and non-reasoning tasks.
10
Limitations
It is shown that Automate-CoT demonstrates supe-
rior performance over previous chain-of-thought
3https://openai.com/blog/chatgpt/
prompting methods. However, despite these excit-
ing results, there are still some limitations to our
current work, as well as potential opportunities for
future research.
Comparision with Fine-tuning : Our main base-
lines include original chain-of-thought (Wei et al.,
2022b), self-consistency (Wang et al., 2023) which
are manual-written based prompt method.
In
addition, we also compare the clustering-based
and retrieval-based methods to select the prompt
exemplars like Auto-CoT (Zhang et al., 2023),
BM25 (Robertson, 2009), PromptPG (Lu et al.,
2023). As large language models are dominating
the field, the performance of training the large lan-
guage models by using these labeled data might be
interesting. However, it is not covered in this study
due to the prompt setting of this study and limited
resources.
Prompt Style Definition : Another limitation of
this work is that it does not provide a rigorous
definition of what constitutes good versus bad lin-
guistic style. While we have observed several pat-
terns of good and bad style during numerous exper-
iments, and the results show that Automate-CoT
is able to mitigate style sensitivity in Manual-CoT,
we cannot determine what perfect style entails. As
such, we acknowledge that defining what consti-
tutes good versus bad linguistic style can be a chal-
lenging task and an important area for further ex-
ploration and development.
Acknowledgments
We thank the anonymous reviewers for their valu-
able suggestions. This work was supported by the
General Research Fund (GRF) of Hong Kong (No.
16310222). Shizhe Diao was supported by the
Hong Kong Ph.D. Fellowship Scheme (HKPFS).
References
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. MathQA: Towards interpretable math
word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
2357–2367, Minneapolis, Minnesota. Association for
Computational Linguistics.
Maksym Andriushchenko, Francesco Croce, Nicolas
Flammarion, and Matthias Hein. 2020. Square at-
tack: a query-efficient black-box adversarial attack
via random search. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–
28, 2020, Proceedings, Part XXIII, pages 484–501.
Springer.
Eyal Ben-David, Nadav Oved, and Roi Reichart. 2022.
PADA: Example-based prompt learning for on-the-
fly adaptation to unseen domains. Transactions of the
Association for Computational Linguistics, 10:414–
433.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual.
Oana-Maria Camburu, Tim Rocktäschel, Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-
ral language inference with natural language explana-
tions. In Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, Decem-
ber 3-8, 2018, Montréal, Canada, pages 9560–9572.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large lan-
guage models trained on code.
ArXiv preprint,
abs/2107.03374.
Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su,
and Jun Zhu. 2019. Improving black-box adversar-
ial attacks with a transfer-based prior. In Advances
in Neural Information Processing Systems 32: An-
nual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, pages 10932–10942.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. ArXiv preprint,
abs/2204.02311.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. 2021.
Training veri-
fiers to solve math word problems. ArXiv preprint,
abs/2110.14168.
Joe Davison, Joshua Feldman, and Alexander Rush.
2019. Commonsense knowledge mining from pre-
trained models. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP),
pages 1173–1178, Hong Kong, China. Association
for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. arXiv.
Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and
Yonggang Wang. 2020. Zen: Pre-training chinese
text encoder enhanced by n-gram representations.
In Findings of the Association for Computational
Linguistics: EMNLP 2020, pages 4729–4740.
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li,
Yong Lin, and Tong Zhang. 2022. Black-box prompt
learning for pre-trained language models.
ArXiv
preprint, abs/2201.08531.
Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan
Song, and Tong Zhang. 2021. Taming pre-trained
language models with n-gram representations for low-
resource domain adaptation. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 3336–3349.
Zhe Dong, Andriy Mnih, and George Tucker. 2020. Dis-
arm: An antithetic gradient estimator for binary latent
variables. In Advances in Neural Information Pro-
cessing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and
Tushar Khot. 2023. Complexity-based prompting for
multi-step reasoning. In International Conference on
Learning Representations.
Yi R Fung, Tuhin Chakraborty, Hao Guo, Owen
Rambow, Smaranda Muresan, and Heng Ji. 2022.
Normsage: Multi-lingual multi-cultural norm discov-
ery from conversations on-the-fly. arXiv preprint
arXiv:2210.08604.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics, 9:346–
361.
Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li,
Zhihua Wen, Kaixuan Wang, and Ting Wang. 2023.
Retrieval-augmented gpt-3.5-based text-to-sql frame-
work with sample-aware prompting and dynamic re-
vision chain. arXiv preprint arXiv:2307.05074.
Karen Hambardzumyan,
Hrant Khachatrian,
and
Jonathan May. 2021. WARP: Word-level Adversarial
ReProgramming. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 4921–4933, Online. Association for
Computational Linguistics.
Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu,
and Maosong Sun. 2021.
PTR: Prompt Tuning
with Rules for Text Classification. ArXiv preprint,
abs/2105.11259.
Adi Haviv, Jonathan Berant, and Amir Globerson. 2021.
BERTese: Learning to speak to BERT. In Proceed-
ings of the 16th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Main Volume, pages 3618–3623, Online. Association
for Computational Linguistics.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2021.
DEBERTA: Decoding-
enhanced bert with disentangled attention. In Inter-
national Conference on Learning Representations.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,
Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.
Large language models can self-improve.
ArXiv
preprint, abs/2210.11610.
Zhichao Huang and Tong Zhang. 2020. Black-box ad-
versarial attack with transferable model-based embed-
ding. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and
Jessy Lin. 2018. Black-box adversarial attacks with
limited queries and information. In Proceedings of
the 35th International Conference on Machine Learn-
ing, ICML 2018, Stockholmsmässan, Stockholm, Swe-
den, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pages 2142–2151.
PMLR.
Andrew Ilyas, Logan Engstrom, and Aleksander Madry.
2019. Prior convictions: Black-box adversarial at-
tacks with bandits and priors. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019. OpenRe-
view.net.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know? Transactions of the Association for
Computational Linguistics, 8:423–438.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In Advances
in Neural Information Processing Systems.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate
Kushman, and Hannaneh Hajishirzi. 2016. MAWPS:
A math word problem repository. In Proceedings of
the 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1152–1157, San
Diego, California. Association for Computational
Linguistics.
Yuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang,
and Dongyan Zhao. 2021. Why machine reading
comprehension models learn shortcuts?
In Find-
ings of the Association for Computational Linguis-
tics: ACL-IJCNLP 2021, pages 989–1002, Online.
Association for Computational Linguistics.
Andrew Lampinen, Ishita Dasgupta, Stephanie Chan,
Kory Mathewson, Mh Tessler, Antonia Creswell,
James McClelland, Jane Wang, and Felix Hill. 2022.
Can language models learn from explanations in con-
text? In Findings of the Association for Computa-
tional Linguistics: EMNLP 2022. Association for
Computational Linguistics.
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan,
Bing Tian Dai, Yan Wang, Dongxiang Zhang, and
Ee-Peng Lim. 2022. Mwptoolkit: an open-source
framework for deep learning-based math word prob-
lem solvers. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 36, pages 13188–
13190.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,
Jian-Guang Lou, and Weizhu Chen. 2022a. On the
advance of making language models better reasoners.
ArXiv preprint, abs/2206.02336.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,
Jian-Guang Lou, and Weizhu Chen. 2022b. On the
advance of making language models better reasoners.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 158–167, Vancouver,
Canada. Association for Computational Linguistics.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022.
What
makes good in-context examples for GPT-3?
In
Proceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures,
pages 100–114, Dublin, Ireland and Online. Associa-
tion for Computational Linguistics.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT
Understands, Too. ArXiv preprint, abs/2103.10385.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A Robustly Optimized BERT Pretrain-
ing Approach. arXiv.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019. OpenRe-
view.net.
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,
Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,
and Ashwin Kalyan. 2023. Dynamic prompt learning
via policy gradient for semi-structured mathematical
reasoning. In International Conference on Learning
Representations (ICLR).
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2022. Fantastically ordered
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
8086–8098, Dublin, Ireland. Association for Compu-
tational Linguistics.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and developing
English math word problem solvers. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 975–984, Online.
Association for Computational Linguistics.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question an-
swering. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 2381–2391, Brussels, Belgium. Association
for Computational Linguistics.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022.
Training language models to follow in-
structions with human feedback.
ArXiv preprint,
abs/2203.02155.
Pantelis Papadopoulos, Stavros Demetriadis, Ioannis
Stamelos, and Ioannis Tsoukalas. 2010.
The ef-
fect of prompting to students with different learn-
ing styles. Multicultural Education and Technology
Journal, 4:198–213.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2080–2094, Online.
Association for Computational Linguistics.
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin,
Qiang Fu, Yan Gao, Jian-Guang Lou, and Weizhu
Chen. 2022. Reasoning like program executors. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, pages 761–
779. Association for Computational Linguistics.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit
Bansal. 2022. Grips: Gradient-free, edit-based in-
struction search for prompting large language models.
ArXiv preprint, abs/2203.07281.
Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying LMs with mixtures of soft prompts.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 5203–5212, Online. Association for Computa-
tional Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.
S. Robertson. 2009. The Probabilistic Relevance Frame-
work: BM25 and Beyond. Foundations and Trends®
in Information Retrieval, 3(4):333–389.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric
Wallace, and Sameer Singh. 2020. AutoPrompt: Elic-
iting Knowledge from Language Models with Auto-
matically Generated Prompts. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 4222–4235,
Online. Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta, Adrià
Garriga-Alonso, et al. 2022. Beyond the imitation
game: Quantifying and extrapolating the capabilities
of language models. ArXiv preprint, abs/2206.04615.
Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,
Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,
Luke Zettlemoyer, Noah A Smith, et al. 2022. Selec-
tive annotation makes language models better few-
shot learners. ArXiv preprint, abs/2209.01975.
Saku Sugawara, Kentaro Inui, Satoshi Sekine, and
Akiko Aizawa. 2018. What makes reading compre-
hension questions easier?
In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 4208–4219, Brussels,
Belgium. Association for Computational Linguistics.
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing
Huang, and Xipeng Qiu. 2022. Black-box tuning for
language-model-as-a-service. In Proceedings of the
39th International Conference on Machine Learning,
volume 162 of Proceedings of Machine Learning
Research, pages 20841–20855. PMLR.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019. Universal adversarial
triggers for attacking and analyzing NLP. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 2153–2162, Hong
Kong, China. Association for Computational Linguis-
tics.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc
Le, Ed Chi, and Denny Zhou. 2022.
Rationale-
augmented ensembles in language models. ArXiv
preprint, abs/2207.00747.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2023. Self-consistency improves
chain of thought reasoning in language models. In
International Conference on Learning Representa-
tions.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022a. Emer-
gent abilities of large language models. Transactions
on Machine Learning Research. Survey Certifica-
tion.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022b. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems.
Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229–256.
Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi
Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao,
Pengcheng He, Michael Zeng, and Xuedong Huang.
2022. Human parity on commonsenseqa: Augment-
ing self-attention with external attention. In Pro-
ceedings of the Thirty-First International Joint Con-
ference on Artificial Intelligence, IJCAI-22, pages
2762–2768. Main Track.
Ke Yang, Charles Yu, Yi R Fung, Manling Li, and Heng
Ji. 2023. Adept: A debiasing prompt framework.
In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 37, pages 10780–10788.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
BARTScore: Evaluating generated text as text gener-
ation. In Advances in Neural Information Processing
Systems.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-
man. 2022. STar: Bootstrapping reasoning with rea-
soning. In Advances in Neural Information Process-
ing Systems.
Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-
tive example selection for in-context learning. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing, pages 9134–
9148.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2023. Automatic chain of thought prompting
in large language models. In International Confer-
ence on Learning Representations.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Vir-
tual Event, volume 139 of Proceedings of Machine
Learning Research, pages 12697–12706. PMLR.
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
Factual probing is [MASK]: Learning vs. learning
to recall. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 5017–5033, Online. Association
for Computational Linguistics.
Xiao Zhou, Weizhong Zhang, Zonghao Chen, Shizhe
Diao, and Tong Zhang. 2021. Efficient neural net-
work training via forward and backward propagation
sparsification. In Advances in Neural Information
Processing Systems, volume 34, pages 15216–15229.
Algorithm 1 The black-box optimization procedures.
Require: Input batch S, Label batch Y , Parameter of categorical distribution p1, · · · , pn, Prediction
model G, Loss function L.
1: for k ≤I do
2:
Sample j(k)
1
∼Cat(p1), · · · , j(k)
n
∼Cat(pn)
3:
T (k) = t(k)
1
· · · t(k)
n
= V[j(k)
1 ] · · · V[j(k)
n ]
4: end for
5: Lavg = 1
I
PI
k=1 L(G[T (k), S], Y )
6: for i ≤n do
7:
gvr
pi =
1
I−1
PI
k=1 ∇pi log P(t(k)
i
)(L(G[T (k), S], Y ) −Lavg)
8:
pi ←projC(pi −η · gvr
pi)
9: end for
10: return p1, · · · pn
A
Algorithm Details
In this section, we provide more details about the derivation of the equation (1) in Section 3.2. Given the
loss function:
ET [L(T)] =
Z
L(T)P(T) dT
(4)
We can estimate the gradient of pi by:
∇piET [L(T)] =
Z
L(T)∇piP(T) dT
=
Z
L(T)P(T)
P(T)∇piP(T) dT
=
Z
P(T)L(T)∇pi log P(T) dT
=EP(T)

L(T)∇pi log Πn
j=1P(tj)

=EP(T)

L(T)∇pi
n
X
j=1
log P(tj)


=EP(T) [L(T)∇pi log P(ti)]
(5)
The j-th component of ∇pi log P(ti) could be solved explicitly by:
∇pi,j log P(ti) = ∇pi,j log pi,ji
(6)
When j = ji, it is obvious that ∇pi,j log P(ti) =
1
pi,ji . When j ̸= ji, equation (6) is calculated by:
∇pi,j log P(ti) =∇pi,j log(1 −
N
X
k=1,k̸=ji
pi,k)
= −
1
1 −PN
k=1,k̸=ji pi,k
= −
1
pi,ji
(7)
Therefore, we adopted a variance-reduced policy gradient estimator (VR-PGE) as described in Williams
(1992); Dong et al. (2020); Zhou et al. (2021) to mitigate the high-variance issue of PGE. The estimated
gradient is calculated by:
gvr
pi =
1
I −1
I
X
k=1
 
L(T (k)) −1
I
I
X
j=1
L(T (j))
!
∇pi log P(ti)
(8)
where T (k), k = 1, · · · , I are sampled independently from P(T).
Thus, the prompt token distribution pi can be updated by a projected stochastic gradient descent
algorithm:
pi ←projC(pi −η · gvr
pi), i = 1, · · · , n
(9)
where η is the learning rate of prompt learning, I is the sample size, and projC is the projection calculation.
The detailed training procedure of our VR-PGE algorithm is displayed in Algorithm 1.
B
Detailed Experimental Setting
DATASET
TASK TYPE
# EX.
# EVAL.
EVAL. SPLIT
TRANSFERRED
GSM8K (Cobbe et al., 2021)
Arithmetic
8
1319
Test
✗
ASDiv (Miao et al., 2020)
Arithmetic
8
2096
Test
✓
SVAMP (Patel et al., 2021)
Arithmetic
8
1000
Test
✓
AQuA (Ling et al., 2017)
Arithmetic
4
254
Test
✗
SingleOp♣
Arithmetic
8
562
Test
✓
CSQA♦(Talmor et al., 2019)
Commonsense
7
1221
Validation
✗
StrategyQA♦(Geva et al., 2021)
Commonsense
6
1880
Validation
✗
Letter (4) (Wei et al., 2022b)
Symbolic
4
500
Test (OOD)
✗
OpenBookQA (Mihaylov et al., 2018)
Question Answering
4
500
Test
✗
e-SNLI♥(Camburu et al., 2018)
Narural Language Inference
6
1000
Test
✗
SST-2♦(Socher et al., 2013)
Sentiment Analysis
6
872
Validation
✗
Table 5: The overall statistics of the datasets. # EX.: the number of few-shot chain-of-thought exemplars used to
prompt each task. # EVAL.: the number of evaluation data. EVAL. SPLIT: evaluation split. TRANSFERRED: a
checkmark means that the exemplars are generated and trained from other datasets and then applied to this task. ♣:
SingleOp is a subset of MAWPS (Koncel-Kedziorski et al., 2016). ♦: CSQA, StrategyQA, and SST-2 do not have
publicly available test set labels, so we simply follow the setting by Wei et al. (2022b) and Wang et al. (2022) to
evaluate the performance of the validation set. ♥: Following Wang et al. (2022), we evaluate the first 1,000 data
points for a fair comparison.
B.1
Datasets and Evaluation Metrics
Following Wei et al. (2022b), we conduct our experiments on eight reasoning tasks, including five math
word problem datasets: GSM8K, ASDiv, SVAMP, AQuA, and SingleOp; two commonsense reasoning
datasets: CommonsenseQA (CSQA) and StrategyQA, and one symbolic reasoning task: Last Letter
Concatenation (Letter (4)). We also generalize our method to non-reasoning tasks including one question-
answering task (OpenBookQA), one natural language inference task (e-SNLI), and one sentiment analysis
task (SST-2). The detailed statistics of the datasets are listed in Table 5.
To make a fair comparison with our baselines, we use the same number of exemplars as Wei et al. (2022b)
and Wang et al. (2022), as shown in Table 5. We keep the same setting for the evaluation split as well.
By default, we use the test split for evaluation, and for datasets that do not have publicly available test
set labels, we evaluate the validation set instead. In addition, for last letter concatenation, since the
model has already achieved almost 100% accuracy under the in-distribution setting, we only test the
out-of-distribution (OOD) setting, Letter (4), where prompts are 2-letters, and test examples are 4-letters.
The evaluation metric for all tasks is the exact match accuracy. First, we conduct pre-processing for
predictions to remove all the special symbols. For example, ""$100,000"" will be processed to ""100000"".
Then we check if it has the same value as the ground truth to calculate the exact match accuracy.
B.2
Baselines
In our experiments, the following three methods serve as the main baselines:
• chain-of-thought (Manual-CoT) (Wei et al., 2022b): standard chain-of-thought prompting which
provides manual-written intermediate reasoning steps.
• self-consistency (SC) (Wang et al., 2023): an improved version of CoT. Instead of greedy decoding, it
samples a diverse set of reasoning paths and chooses the most common answer.
• Auto-CoT (Zhang et al., 2023): an automatic exemplars construction method that applies clustering
techniques to sample questions and then generates chains.
Our experiments are conducted with two popular large language models:
• GPT-3 (Brown et al., 2020): we test an advanced version of GPT-3, text-davinci-002, which
corresponds to InstructGPT (Ouyang et al., 2022) model.
• CodeX (Chen et al., 2021): we test code-davinci-002 which has better code representation ability.
We utilize the public APIs directly from OpenAI’s services4. In our main experiments, we test on both
text-davinci-002 and code-davinci-002 engines. However, in additional experiments, we mainly
test on code-davinci-002 for two reasons : (1) It is the most capable model available at the time we
were conducting our experiments, consistent with the observations in previous studies (Wei et al., 2022b;
Wang et al., 2023; Miao et al., 2020). (2) Compared to costly text-davinci-002, it is free of charge
because we are in the initial limited beta period during our experiments process.
B.3
Implementation
Augment and Prune: Following Wei et al. (2022b) and Wang et al. (2022), we keep the same number of
exemplars (4-8) listed in Table 5. For main experiments, we augment and prune a pool of 100 high-quality
exemplars for all datasets. Firstly, pool construction questions are randomly sampled and then fed to
LLMs to construct model-generated answers with rationale chains. Given that some datasets only have the
test split, we use the pool result of GSM8K and transferred it to these datasets for further inference. Here
for arithmetic reasoning tasks, pool construction questions are randomly sampled from the training split
of GSM8K and AQuA. For CSQA and StrategyQA, exemplars are randomly sampled from the official
training split (Talmor et al., 2019) and question-only set from BIG-bench collaboration (Srivastava et al.,
2022). For letter concatenation, exemplars are randomly sampled from the 2-letter set. After the pool is
constructed, we use labels to prune the incorrect model-generated exemplars and retain 100 high-quality
exemplars.
Select: The train set and validation set are also randomly sampled following the same rule as above
except Letter (4) dataset. Since LLM has already reached almost 100% accuracy on the 2-letter set,
we choose to optimize the model based on the 3-letter OOD set. Thus the train set and validation set
are randomly sampled from the 3-letter set. Both the train and validation sets have a size of 100 to
reach a performance and cost trade-off. Then by utilizing the log probability returned by API calls,
we calculate the cross-entropy loss of the answer token. Finally, we optimize the latent variables by
AdamW (Loshchilov and Hutter, 2019) for 5 epochs with a learning rate of 1 × 10−3 and batch size
of 10. After optimization, as shown in Figure 2 inference stage, we choose the exemplars combination
(arg max pi) with the highest validation accuracy to be further evaluated on the test set. By default, we
query the language model once to get the answer. Under the self-consistency setting, similar to Wang et al.
(2023), we query the language model 40 times and choose the most consistent one as the final answer.
Hyper-parameter Setting: Under few-shot setting, we set max_tokens = 256 for all augmentation,
selection and inference. In addition, we set logprobs = 5 when training. Moreover, we set temperature =
0.7 for evaluation under self-consistency while temperature = 0 for all other cases. Under zero-shot setting
(§6.5), we keep the same hyper-parameters as Kojima et al. (2022) which first uses max_tokens = 128 for
generating the rationale chains and then uses max_tokens = 32 for generating the answers to construct the
pool. The hyper-parameters for selecting and evaluating are the same as the few-shot setting above.
C
More Experiment Results
C.1
Experiments under ChatGPT
To further verify the effectiveness of Automate-CoT, we further conduct the experiments on gpt-3.5-turbo.
Automate-CoT also shows consistent improvement on each task with 2.8% improvement on arithmetic
4https://openai.com/api/
METHOD
GSM8K
ASDIV
SVAMP
AQUA
SINGLEOP
CSQA
STQA
LETTER (4)
OBQA
E-SNLI
SST-2
AVG.
Manual-CoT
63.1
77.1
78.1
44.9
90.0
77.5
59.7
73.0
80.0
80.9
85.3
73.6
+ BM25
64.2
73.7
73.8
45.3
87.9
76.1
58.9
73.4
81.4
76.3
87.2
72.6
+ PromptPG
66.6
76.7
75.6
46.1
89.1
77.8
60.2
74.8
81.8
77.8
87.8
74.0
+ K-Means
66.4
76.6
77.6
45.7
89.7
79.0
60.0
73.6
80.4
78.4
84.1
73.8
+ Automate-CoT
68.0↑4.9
81.7 ↑4.6
79.1↑1.0
46.9↑2.0
91.5↑1.5
80.5↑3.0
64.5↑4.8
76.2↑3.2
83.0↑3.0
81.4↑0.5
87.7↑2.4
76.4↑2.8
Table 6: The overall performance of Automate-CoT under gpt-3.5-turbo and the comparison with retrieval-based
and clustering-based exemplars selection methods.
METHOD
GSM8K
ASDIV
SVAMP
AQUA
SINGLEOP
CSQA
STQA
LETTER (4)
text-davinci-002
Automate-CoT
0.14
0.29
0.17
0.21
0.08
0.06
0.26
0.04
Automate-CoT(SC)
0.02
0.18
0.06
0.14
0.04
0.01
0.07
0.04
code-davinci-002
Automate-CoT
0.19
0.78
0.33
0.09
0.05
0.17
0.95
0.02
Automate-CoT(SC)
0.09
0.09
0.13
0.01
0.06
0.03
0.09
0.08
Table 7: The variance of the results in Table 1 over 3 runs. (SC) denotes under self-consistency setting.
reasoning, 3.9% improvement on commonsense reasoning, 3.2% on symbolic reasoning, and 2.8%
improvement overall as shown in Table 6.
C.2
Comparison with Retrieval Methods
We also compare Automate-CoT with simple retrieval method BM25 (Robertson, 2009) and reinforcement
learning-based retrieval method PromptPG (Lu et al., 2023). We first implemented a BM25 selection
method and tested the performance on all the datasets. The results are shown in Table 6. It indicates that
retrieval-based methods can only select examples with similar meaning to the query question while the
diversity is overlooked. As shown in the table, the average performance of the BM25 retrieval-based
method even has a 1% degradation compared to Manual-CoT, and 3.8% lower than Automate-CoT. A
similar phenomenon is observed in Auto-CoT (Zhang et al., 2023), which indicates that with similar
questions being sampled for test questions, Retrieval-Q-CoT is negatively affected by misleading by
similarity.
In addition, we also compare with PromptPG (Lu et al., 2023), a dynamic example-selection baseline.
We adopt the same setting as ours for PomptPG, where the number of training examples is 100, the
size of the candidate pool is 100, and the backbone model is gpt-3.5-turbo. Further, we keep the same
prompt format as the original chain-of-thought and ours. The other settings we use are consistent with the
settings provided by their original code. The results are shown in Table 6. It indicates that Automate-CoT
outperforms PromptPG.
C.3
Comparison with Clustering Methods
We further conduct additional experiments to compare Automate-CoT with methods selecting demon-
stration exemplars through clustering. We use K-Means as the clustering method and create k clusters
according to the number of exemplars specified in Table 5. Then we use these k representative exemplars
as the demonstration exemplars to prompt the language models. The results are shown in Table 6. It indi-
cates that clustering-based methods can select examples with different semantic meanings and generally
perform better than Manual-CoT. However, the complexity and diversity are overlooked. For example,
most of the selected few-shot exemplars in GSM8K have around 3-4 hops where complex questions and
moderately difficult questions are overlooked. As a result, it generally performs worse than Automate-CoT
with a 2.6% gap.
C.4
Variance Report
Since Automate-CoT’s results in Table 1 are averaged over three runs, we also report the variance in Table
7 here. It is observed that Automate-CoT achieves quite a low variance, especially compared to the large
variance of Manual-CoT as shown in § 2 Motivation.
D
Additional Comparison with Fine-tuning
Since our method uses a training-based pipeline, we also compare it with fine-tuning large language
models in terms of the number of parameters, training cost, estimated total training cost, and required
training set size. As shown in the study of Cobbe et al. (2021), fine-tuning on gpt-3 requires thousands
(e.g., 8000) of training examples to be effective while Automate-CoT only needs 100 training examples.
In addition, fine-tuning has a larger training and inference cost than Automate-CoT because it not only
requires a one-off fine-tuning cost but also has a higher unit price on subsequent usage.
For Automate-CoT, under the setting of gpt-3.5-turbo, the direct usage is $ 0.0015 / 1k tokens for input and
$ 0.002 / 1k tokens for output. With the training epochs of 3, a training set size of 100 and a validation set
size of 100, an input length of around 750 tokens and an average output length of 150 tokens, it takes about
(750/1000 · 0.0015 + 150/1000 · 0.002) · 100 · 10 · 3 + (750/1000 · 0.0015 + 150/1000 · 0.002) · 100 · 3=
$ 4.7. However, for fine-tuneing, given the training price of gpt-3.5-turbo is $ 0.008 / 1K tokens, the usage
of finetuned gpt-3.5-turbo is $ 0.0015 / 1K tokens for input and $ 0.002 / 1K tokens for output tokens.
Under the finetuning setting, suppose the average length of training examples is 300 tokens, and training a
whole training set of 8000 examples for 3 epochs takes about 300/1000 · 8000 · 3 · 0.008= $ 57.6, which
costs 12x more than Automate-CoT.
It is also worth noting that the further usage of finetuned gpt-3.5-turbo is $ 0.012 / 1K tokens for input
and $ 0.016 / 1K tokens for output while Automate-CoT remains the normal cost, which is 8x less cost
than fine-tuning.
METHOD
# of Training Params
Cost
Est. Total Cost
Train Set Size
Fine-tuning
$ 9.1
500
Unknown
$0.008/1K tokens (Train)
$ 12.7
1000
$ 20.0
2000
but should ≥175B
$0.012/1K tokens (Input Usage)
$ 34.3
4000
$0.016/1K tokens (Output Usage)
$ 63.1
8000
Automate-CoT
# of exemplars × Pool Size
$0.0015/1K tokens (Input Usage)
$ 6.6
100
$0.002/1K tokens (Output Usage)
Table 8: Comparison between Fine-tuning and Automate-CoT on GSM8K. The cost is copied from the OpenAI
official website. 5
E
Additional Analysis
We list some additional analysis here that cannot be put in the main section because of the page limit.
E.1
Effects of Several Tricks
Previous studies have found some tricks like add ""Let’s think step by step."" before each rationale chain
and replace ""Q:"" with ""Question:"" (Fu et al., 2023; Kojima et al., 2022) can boost the performance on
top of Manual-CoT. Following their settings, we also test Automate-CoT with tricks on GSM8K as an
additional experiment. By adding tricks, Automate-CoT can further boost the accuracy to 69.8% (+2.2%)
under the normal setting and 83.0% (+0.6%) under the self-consistency setting, respectively.
F
Exact Match Number over Each Hop
The exact match number over each hop of Figure 1 is reported in Table 9.
5https://openai.com/pricing
HOP
Total Number
Manual-CoT
Automate-CoT
Complex-CoT
2
326
269 = 82.52%
272 = 83.44%
227 = 69.63%
3
370
265 = 71.62%
268 = 72.44%
232 = 62.70%
4
298
159 = 53.35%
196 = 65.77%
166 = 55.70%
5
174
83 = 47.70%
110 = 63.21%
98 = 56.32%
6
88
26 = 29.54%
37 = 42.05%
39 = 44.32%
7
40
10 = 25%
12 = 30%
16 = 40%
8
20
3 = 15%
4 = 20%
5 = 25%
9
2
0
0
0
11
1
0
0
1
Overall
1319
815 = 61.8%
899 = 68.2%
784 = 59.4%
Table 9: The exact match number across the different numbers of hops on GSM8K. Bold represents the best among
each hop. The percentage accuracy is calculated for each hop.
G
Full Exemplars generated by Automate-CoT
DATASET
Exemplars
Hops
GSM8K
Q: Ralph is going to practice playing tennis with a tennis ball machine that shoots out tennis balls for
Ralph to hit. He loads up the machine with 175 tennis balls to start with. Out of the first 100 balls, he
manages to hit 2/5 of them. Of the next 75 tennis balls, he manages to hit 1/3 of them. Out of all the
tennis balls, how many did Ralph not hit?
4-hop
A: Ralph started with 175 tennis balls. He hit 2/5 of the first 100 balls, so he hit 2/5 * 100 = 40 balls. He
hit 1/3 of the next 75 balls, so he hit 1/3 * 75 = 25 balls. In total he hit 40 + 25 = 65 balls. He did not hit
175 - 65 = 110 balls. The answer is 110.
Q: Hans booked a room in a hotel. The hotel has 10 floors with 10 identical rooms on each floor. Because
of an accident, the last floor is unavailable for the guests. Considering there are no other guests, in how
many different rooms could Hans be checked in?
3-hop
A: There are 10 floors with 10 rooms each. The last floor is unavailable. So there are 9 * 10 = 90 rooms
available. The answer is 90.
Q: James earns $20 an hour while working at his main job. He earns 20% less while working his second
job. He works 30 hours at his main job and half that much at his second job. How much does he earn per
week?
8-hop
A: James earns $20 an hour at his main job. He earns 20% less at his second job. That means he earns
$20 - 20% of $20 = $20 - $4 = $16 an hour at his second job. He works 30 hours at his main job and half
that much at his second job. That means he works 30 / 2 = 15 hours at his second job. In total he works
30 + 15 = 45 hours. He earns $20 an hour at his main job and $16 an hour at his second job. In total he
earns $20 * 30 + $16 * 15 = $600 + $240 = $840. The answer is 840.
Q: Leah earned $28 working odd jobs around the neighborhood. She spent a seventh of it on a milkshake
and put half of the rest in her savings account. She left the remaining money in her wallet. Her dog got
ahold of her wallet and shredded all the money inside but $1. How many dollars did Leah lose?
6-hop
A: Leah earned $28. She spent a seventh of it on a milkshake, so she spent $28 / 7 = $4 on the milkshake.
She put half of the rest in her savings account, so she put $28 - $4 / 2 = $12 in her savings account. She
left the remaining money in her wallet. She lost all the money in her wallet except $1. So she lost $28 -
$4 - $12 - $1 =$11. The answer is 11.
Q: Sam and Jeff had a skipping competition at recess. The competition was split into four rounds. Sam
completed 1 more skip than Jeff in the first round. Jeff skipped 3 fewer times than Sam in the second
round. Jeff skipped 4 more times than Sam in the third round. Jeff got tired and only completed half the
number of skips as Sam in the last round. If Sam skipped 16 times in each round, what is the average
number of skips per round completed by Jeff?
6-hop
A: Sam skipped 16 times in each round. In the first round, Jeff skipped 16 - 1 = 15 times. In the second
round, Jeff skipped 16 - 3 = 13 times. In the third round, Jeff skipped 16 + 4 = 20 times. In the last round,
Jeff skipped 16 / 2 = 8 times. The average number of skips per round completed by Jeff is (15 + 13 + 20
+ 8) / 4 = 56 / 4 = 14. The answer is 14.
Q: A Statistics student wants to find out the average daily allowance of the middle school students.
According to his survey, 2/3 of the students receive an average of $6 allowance per day while the rest
gets an average of $4 a day. If he surveyed 60 students, what is the total amount of money those 60
students get in a day?
6-hop
A: 2/3 of the students receive $6 a day. The rest receive $4 a day. The total number of students is 60. So,
2/3 of 60 students receive $6 a day. The rest of the students, which is 60 - 2/3 * 60 = 20 students, receive
$4 a day. The total amount of money those 60 students get in a day is 2/3 * 60 * $6 + 20 * $4 = $240 +
$80 = $320. The answer is 320.
Q: Ali had a collection of seashells. He started with 180 seashells. He then gave away 40 seashells to his
friends. He also gave 30 seashells to his brothers. If he sold half of the remaining seashells, how many
seashells did he have left?
5-hop
A: He started with 180 seashells. He gave away 40 to his friends and 30 to his brothers. That means he
gave away 40 + 30 = 70 seashells. He has 180 - 70 = 110 seashells left. He sold half of them, so he has
110 / 2 = 55 seashells left. The answer is 55.
Q: Marvin and Tina were selling candy bars to help fund their class trip. The candy bars cost $2 each.
Marvin sold 35 candy bars total. Tina sold three times the number of candy bars as Marvin. How much
more money did Tina make for the class trip selling candy bars compared to Marvin?
5-hop
A: Marvin sold 35 candy bars. Tina sold 3 times as many, so she sold 35 * 3 = 105 candy bars. Each
candy bar cost $2, so Marvin made 35 * $2 = $70. Tina made 105 * $2 = $210. The difference is $210 -
$70 = $140. The answer is 140.
Table 10: One example of selected model-generated exemplars with rationale chains of average hops = 5.4. This set
of exemplars is trained and selected on GSM8K and transferred to other arithmetic reasoning tasks.
DATASET
Exemplars
AQuA
Q: If Tim had lunch at $50 and he gave 20% tip, how much did he spend? Answer Choices: (a) $60.00 (b)
$35.42 (c) $60.60 (d) $21.56 (e) $78.45
A: The tip is 20% of what he paid for lunch. tip = 20% of 50.00 = (20/100)*50.00 = = $10.00. Total spent
50.00 + 10.00 = $60.00. The answer is (a).
Q: A person can walk at a constant rate of 8mph and can bike at a rate of 16mph. If he wants to travel 64 miles
in 8 hours using bike and walking at their constant rates, how much distance would he require to walk? Answer
Choices: (a) 20 (b) 30 (c) 48 (d) 64 (e) 72
A: Total distance = 64. Distance = Speed * Time. Walking speed = s1 = 8. Walking time = t1. Bike speed =
s2 = 16. Time traveled in bike = t2. d1 + d2 = 64. s1t1 + s2t2 = 64. 8*t1 + 16*t2 = 64. t1 + 2*t2 = 8 —– (1).
Given: t1 + t2 = 8 —– (2). (1) - (2) −−> t2 = 0 and t1 = 8 - 0 = 8. Walking distance = s1*t1 = 8*8 = 64. The
answer is (d).
Q: The output of a factory was increased by 10% to keep up with rising demand. To handle the holiday rush,
this new output was increased by 20%. By approximately what percent would the output now have to be
decreased in order to restore the original output? Answer Choices: (a) 20% (b) 24% (c) 30% (d) 32% (e) 79%
A: Let initial output is O then after 10% increase it will be 1.1O and after 20% increase on this new output the
latest output will be 1.1O * 1.20 = 1.32O. Now we have to decrease the output by some percentage so that the
new output is same as the starting output (O). so, 1.32O * (1-x/100) = O. => x = 24.24%. So, answer will be B.
The answer is (b).
In a graduate physics course, 70 percent of the students are male and 30 percent of the students are married. If
two-sevenths of the male students are married, what fraction of the male students is single? Answer Choices:
(a) 2/7 (b) 1/3 (c) 1/2 (d) 2/3 (e) 5/7
A: let assume there are 100 students of which 70 are male and 30 are females. if 30 are married then 70 will be
single. now its given that two-sevenths of the male students are married that means 2/7 of 70 = 20 males are
married. if 30 is the total number of students who are married and out of that 20 are males then the remaining
10 will be females who are married. total females = 70. married males = 20. then single males = 70-20 = 50. we
need to find the fraction of male students who are single i.e single male students / total male student = 50/70 =
5/7 [E]. The answer is (e).
Table 11: One example of selected model-generated exemplars with rationale chains. Note that there are newlines
between the answer choices which are omitted in the table to save space.
DATASET
Exemplars
CSQA
Q: John was punching and punching at the wall but succeeded only in bloodying his knuckles. This was bad. He
would be unable to hold his tools if he injured what? Answer Choices: (a) hostility (b) anger (c) nose (d) fists (e)
hands
A: The answer should be something that John would be unable to hold his tools if he injured it. Of the above
choices, only hands make sense. So the answer is (e).
Q: What is required to be good at playing tennis? Answer Choices: (a) tennis racket (b) skill (c) competition (d)
tennis court (e) legs
A: The answer must be something that is required to be good at playing tennis. Of the above choices, only skill
is required to be good at playing tennis. So the answer is (b).
Q: What is someone who gets angry after getting drunk likely to participate in? Answer Choices: (a) violence
(b) falling down (c) vomiting (d) vomiting (e) lower standards
A: The answer should be something that someone who gets angry after getting drunk is likely to do. Of the above
choices, the closest answer is violence. So the answer is (a).
Q: James felt himself losing consciousness. He was frightened. He didn’t want this. He was scared of what?
Answer Choices: (a) death (b) coma (c) peace (d) forgetfulness (e) loss of life
A: The answer should be something that James would be scared of. Of the above choices, the closest thing is
death. So the answer is (a).
Q: Sally lost her kite because she wasn’t careful. She thought that there was more string on the spool, but it
slipped out of her fingers when she reached the what?. Answer Choices: (a) child’s hand (b) the last straw (c)
hobby shop (d) end of line (e) toy store
A: The answer should be the end of the string. Of the above choices, the end of the line makes the most sense. So
the answer is (d).
Q: The sensor would just the distance then set off an alarm, the installation expert explained it was called a what
kind of sensor? Answer Choices: (a) near (b) closeness (c) here (d) proximity (e) this
A: The answer must be a type of sensor that would set off an alarm when something is near. Of the above choices,
only proximity sensor makes sense. So the answer is (d).
Q: The student needed to get some new pencils, where did he go? Answer Choices: (a) store (b) classroom (c)
backpack (d) cabinet (e) teacher
A: The answer should be the place where pencils are sold. Of the above choices, only store makes sense. So the
answer is (a).
Table 12: One example of selected model-generated exemplars with rationale chains. This set of exemplars is
trained and selected on CommonsenseQA. Note that there are newlines between the answer choices which are
omitted in the table to save space.
DATASET
Exemplars
StrategyQA
Q: Did the death of Helen Palmer have a significant effect on Dr. Seuss?
A: Helen Palmer was Dr. Seuss’s wife. The death of a spouse can have a significant effect on a person. Thus,
the death of Helen Palmer could have a significant effect on Dr. Seuss. So the answer is yes.
Q: Would a moose hypothetically be too much for a minotaur to devour whole?
A: A moose is about 1,000 pounds. A minotaur is about 1,000 pounds. Thus, a moose would be too much for
a minotaur to devour whole. So the answer is yes.
Q: Yes or no: Is H’s most common two letter pair partner a freebie in Wheel of Fortune bonus round?
A: The most common two letter pair partner for H is E. E is a freebie in the Wheel of Fortune bonus round.
Thus, H’s most common two letter pair partner is a freebie in the Wheel of Fortune bonus round. So the
answer is yes.
Q: Yes or no: Would bald eagle deliver an urgent message before B-52?
A: The top speed of a bald eagle is about 80 mph. The top speed of a B-52 is about 650 mph. Thus, a B-52
would be faster than a bald eagle. So the answer is no.
Q: Yes or no: Does an individual oceanographer study many sciences?
A: Oceanography is the study of the ocean. The ocean is a complex system. Thus, oceanography is a
multidisciplinary science. So the answer is yes.
Q: Yes or no: Could Steven Spielberg send emails as a child?
A: Steven Spielberg was born in 1946. Email was invented in 1971. Thus, Steven Spielberg could not send
emails as a child. So the answer is no.
Table 13: One example of selected model-generated exemplars with rationale chains. This set of exemplars is
trained and selected on StrategyQA. Note that there are newlines between the answer choices which are omitted in
the table to save space.
DATASET
Exemplars
Letter (4)
Q: Take the last letters of the words in ""Delmer Andersen"" and concatenate them.
A: The last letter of ""Delmer"" is ""r"". The last letter of ""Andersen"" is ""n"". Concatenating them is ""rn"". So the
answer is rn.
Q: Take the last letters of the words in ""Jason Gonzalez"" and concatenate them.
A: The last letter of ""Jason"" is ""n"". The last letter of ""Gonzalez"" is ""z"". Concatenating them is ""nz"". So the
answer is nz.
Q: Take the last letters of the words in ""Ulysses Brown"" and concatenate them.
A: The last letter of ""Ulysses"" is ""s"". The last letter of ""Brown"" is ""n"". Concatenating them is ""sn"". So the
answer is sn.
Q: Take the last letters of the words in ""Frank Ortiz"" and concatenate them.
A: The last letter of ""Frank"" is ""k"". The last letter of ""Ortiz"" is ""z"". Concatenating them is ""kz"". So the answer
is kz.
Table 14: One example of selected model-generated exemplars with rationale chains. This set of exemplars is
trained on Letter (3) and selected on Letter (2).
DATASET
Exemplars
OpenBookQA
Q: As a car approaches you in the night (a) the headlights become more intense (b) the headlights recede
into the dark (c) the headlights remain at a constant (d) the headlights turn off
A: The headlights become more intense as the car approaches. The answer is (a).
Q: Many animals that give birth to live young have (a) gills (b) scales (c) exoskeletons (d) legs
A: Animals that give birth to live young are mammals. Mammals have hair and give milk to their young. The
answer is (a).
Q: A person is lost in a dense forest, and needs to find their home. They know their home is to the south, and
they are headed north. They can find home by using a (a) northern-directing device (b) northern light reader
(c) northeastern winds (d) north central credit
A: he person needs to find south, so they need a northern-directing device. The answer is (a).
Q: When the weather changes as it does from Christmas to Easter, (a) the air may chill (b) the ground may
freeze (c) the plants may die (d) the ground may warm
A: The weather changes from Christmas to Easter, the ground may warm. The answer is (d).
Table 15: One example of selected exemplars with rationale chains. This set of exemplars is trained and selected on
OpenBookQA.
DATASET
Exemplars
e-SNLI
Premise:
""A woman wearing all white and eating, walks next to a man holding a briefcase.""
Based on this premise,can we conclude the hypothesis ""A female is next to a man."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: A woman is another word for a female. The answer is yes.
Premise:
""A couple play in the tide with their young son.""
Based on this premise,can we conclude the hypothesis ""The family is sitting down for dinner."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: The family cannot simultaneously be playing in the tide and sitting down to dinner The answer is no.
Premise:
""A man, woman, and child enjoying themselves on a beach.""
Based on this premise,can we conclude the hypothesis ""A child with mom and dad, on summer vacation at the
beach."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: A man and a woman with a child are not necessarily its mom and dad. A man, woman, and child at a beach
are not necessarily on summer vacation. The answer is it is not possible to tell.
Premise:
""Two women, holding food carryout containers, hug.""
Based on this premise,can we conclude the hypothesis ""The two sisters saw each other across the crowded
diner and shared a hug, both clutching their doggie bags."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: Just because two women are hugging does not mean they are sisters or they are in a crowded diner. The
answer is it is not possible to tell.
Premise:
""A few people in a restaurant setting, one of them is drinking orange juice.""
Based on this premise,can we conclude the hypothesis ""The people are eating omelettes."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: We only know that someone is drinking orange juice, we do not know that they are eating omelettes The
answer is it is not possible to tell.
Premise:
""A man and a woman cross the street in front of a pizza and gyro restaurant.""
Based on this premise,can we conclude the hypothesis ""Near a couple of restaurants, two people walk across
the street."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: man and woman are people. The answer is yes.
Table 16: One example of selected exemplars with rationale chains. This set of exemplars is trained and selected on
e-SNLI.
DATASET
Exemplars
SST-2
What is the sentiment of the following sentence?
""more than another "" best man "" clone by weaving a theme throughout this funny film""
A: ""weaving a theme throughout this funny film"" indicates positive sentiment. The answer is positive.
What is the sentiment of the following sentence?
""that ’s far too tragic to merit such superficial treatment""
A: ""far too tragic"" and ""to merit such superficial treatment"" both mean negative sentiments. The answer is
negative.
What is the sentiment of the following sentence?
""are more deeply thought through than in most ’ right-thinking ’ films""
A: ""more deeply thought through"" indicates positive sentiment. The answer is positive.
What is the sentiment of the following sentence?
""excruciatingly unfunny and pitifully unromantic""
A: ""excruciatingly unfunny"" and ""pitifully unromantic"" both mean negative sentiments. The answer is
negative..
What is the sentiment of the following sentence?
""with his usual intelligence and subtlety""
A: ""with his usual intelligence and subtlety"" indicates positive sentiment. The answer is positive.
What is the sentiment of the following sentence?
""goes to absurd lengths""
A: ""goes to absurd lengths"" is a negative sentiment. The answer is negative.
Table 17: One example of selected exemplars with rationale chains. This set of exemplars is trained and selected on
SST-2.
</Content></Document>
<Document index=4><Published>2023-03-07</Published><Title>Self-Consistency Improves Chain of Thought Reasoning in Language Models</Title><Authors>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou</Authors><Summary>Chain-of-thought prompting combined with pre-trained large language models
has achieved encouraging results on complex reasoning tasks. In this paper, we
propose a new decoding strategy, self-consistency, to replace the naive greedy
decoding used in chain-of-thought prompting. It first samples a diverse set of
reasoning paths instead of only taking the greedy one, and then selects the
most consistent answer by marginalizing out the sampled reasoning paths.
Self-consistency leverages the intuition that a complex reasoning problem
typically admits multiple different ways of thinking leading to its unique
correct answer. Our extensive empirical evaluation shows that self-consistency
boosts the performance of chain-of-thought prompting with a striking margin on
a range of popular arithmetic and commonsense reasoning benchmarks, including
GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and
ARC-challenge (+3.9%).</Summary><Source>https://arxiv.org/pdf/2203.11171.pdf</Source><Content>Published as a conference paper at ICLR 2023
SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT
REASONING IN LANGUAGE MODELS
Xuezhi Wang†‡
Jason Wei†
Dale Schuurmans†
Quoc Le†
Ed H. Chi†
Sharan Narang†
Aakanksha Chowdhery†
Denny Zhou†§
†Google Research, Brain Team
‡xuezhiw@google.com, §dennyzhou@google.com
ABSTRACT
Chain-of-thought prompting combined with pre-trained large language models has
achieved encouraging results on complex reasoning tasks. In this paper, we propose
a new decoding strategy, self-consistency, to replace the naive greedy decoding
used in chain-of-thought prompting. It ﬁrst samples a diverse set of reasoning paths
instead of only taking the greedy one, and then selects the most consistent answer
by marginalizing out the sampled reasoning paths. Self-consistency leverages the
intuition that a complex reasoning problem typically admits multiple different ways
of thinking leading to its unique correct answer. Our extensive empirical evaluation
shows that self-consistency boosts the performance of chain-of-thought prompting
with a striking margin on a range of popular arithmetic and commonsense reasoning
benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%),
StrategyQA (+6.4%) and ARC-challenge (+3.9%).
1
INTRODUCTION
Although language models have demonstrated remarkable success across a range of NLP tasks, their
ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by
increasing model scale (Rae et al., 2021; BIG-bench collaboration, 2021, inter alia). In an effort
to address this shortcoming, Wei et al. (2022) have proposed chain-of-thought prompting, where
a language model is prompted to generate a series of short sentences that mimic the reasoning
process a person might employ in solving a task. For example, given the question “If there are 3
cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?”, instead
of directly responding with “5”, a language model would be prompted to respond with the entire
chain-of-thought: “There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 +
2 = 5 cars. The answer is 5.”. It has been observed that chain-of-thought prompting signiﬁcantly
improves model performance across a variety of multi-step reasoning tasks (Wei et al., 2022).
In this paper, we introduce a novel decoding strategy called self-consistency to replace the greedy
decoding strategy used in chain-of-thought prompting (Wei et al., 2022), that further improves
language models’ reasoning performance by a signiﬁcant margin. Self-consistency leverages the
intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct
answer (Stanovich & West, 2000). The more that deliberate thinking and analysis is required for a
problem (Evans, 2010), the greater the diversity of reasoning paths that can recover the answer.
Figure 1 illustrates the self-consistency method with an example. We ﬁrst prompt the language model
with chain-of-thought prompting, then instead of greedily decoding the optimal reasoning path, we
propose a “sample-and-marginalize” decoding procedure: we ﬁrst sample from the language model’s
decoder to generate a diverse set of reasoning paths; each reasoning path might lead to a different
ﬁnal answer, so we determine the optimal answer by marginalizing out the sampled reasoning paths
to ﬁnd the most consistent answer in the ﬁnal answer set. Such an approach is analogous to the
human experience that if multiple different ways of thinking lead to the same answer, one has greater
conﬁdence that the ﬁnal answer is correct. Compared to other decoding methods, self-consistency
avoids the repetitiveness and local-optimality that plague greedy decoding, while mitigating the
stochasticity of a single sampled generation.
1
arXiv:2203.11171v4  [cs.CL]  7 Mar 2023
Published as a conference paper at ICLR 2023
Language 
model
Q: If there are 3 cars in the parking 
lot and 2 more cars arrive, how many 
cars are in the parking lot?
A: There are 3 cars in the parking lot 
already. 2 more arrive. Now there are 
3 + 2 = 5 cars. The answer is 5.
…
Q: Janet’s ducks lay 16 eggs per day. 
She eats three for breakfast every 
morning and bakes muffins for her 
friends every day with four. She sells 
the remainder for $2 per egg. How 
much does she make every day?
A:
She has 16 - 3 - 4 = 9 eggs 
left. So she makes $2 * 9 = 
$18 per day. 
Sample a diverse set of 
reasoning paths
She eats 3 for breakfast, so 
she has 16 - 3 = 13 left. Then 
she bakes muffins, so she 
has 13 - 4 = 9 eggs left. So 
she has 9 eggs * $2 = $18. 
This means she she sells the 
remainder for $2 * (16 - 4 - 3) 
= $26 per day. 
The answer is $18.
The answer is $26.
The answer is $18.
The answer is $18.
Marginalize out reasoning paths 
to aggregate final answers
Language 
model
This means she uses 3 + 4 = 7 eggs every day.  
She sells the remainder for $2 per egg, so in 
total she sells 7 * $2 = $14 per day. 
The answer is $14.
The answer is $14.
Greedy decode
Figure 1: The self-consistency method contains three steps: (1) prompt a language model using
chain-of-thought (CoT) prompting; (2) replace the “greedy decode” in CoT prompting by sampling
from the language model’s decoder to generate a diverse set of reasoning paths; and (3) marginalize
out the reasoning paths and aggregate by choosing the most consistent answer in the ﬁnal answer set.
Self-consistency is far simpler than prior approaches that either train an additional veriﬁer (Cobbe
et al., 2021) or train a re-ranker given additional human annotations to improve generation quality
(Thoppilan et al., 2022). Instead, self-consistency is entirely unsupervised, works off-the-shelf with
pre-trained language models, requires no additional human annotation, and avoids any additional
training, auxiliary models or ﬁne-tuning. Self-consistency also differs from a typical ensemble
approach where multiple models are trained and the outputs from each model are aggregated, it acts
more like a “self-ensemble” that works on top of a single language model.
We evaluate self-consistency on a wide range of arithmetic and commonsense reasoning tasks over
four language models with varying scales: the public UL2-20B (Tay et al., 2022) and GPT-3-175B
(Brown et al., 2020), and two densely-activated decoder-only language models: LaMDA-137B
(Thoppilan et al., 2022) and PaLM-540B (Chowdhery et al., 2022). On all four language models,
self-consistency improves over chain-of-thought prompting by a striking margin across all tasks. In
particular, when used with PaLM-540B or GPT-3, self-consistency achieves new state-of-the-art levels
of performance across arithmetic reasoning tasks, including GSM8K (Cobbe et al., 2021) (+17.9%
absolute accuracy gains), SVAMP (Patel et al., 2021) (+11.0%), AQuA (Ling et al., 2017) (+12.2%),
and across commonsense reasoning tasks such as StrategyQA (Geva et al., 2021) (+6.4%) and ARC-
challenge (Clark et al., 2018) (+3.9%). In additional experiments, we show self-consistency can
robustly boost performance on NLP tasks where adding a chain-of-thought might hurt performance
compared to standard prompting (Ye & Durrett, 2022). We also show self-consistency signiﬁcantly
outperforms sample-and-rank, beam search, ensemble-based approaches, and is robust to sampling
strategies and imperfect prompts.
2
SELF-CONSISTENCY OVER DIVERSE REASONING PATHS
A salient aspect of humanity is that people think differently. It is natural to suppose that in tasks
requiring deliberate thinking, there are likely several ways to attack the problem. We propose that
such a process can be simulated in language models via sampling from the language model’s decoder.
For instance, as shown in Figure 1, a model can generate several plausible responses to a math
question that all arrive at the same correct answer (Outputs 1 and 3). Since language models are not
perfect reasoners, the model might also produce an incorrect reasoning path or make a mistake in
one of the reasoning steps (e.g., in Output 2), but such solutions are less likely to arrive at the same
answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to
have greater agreement in their ﬁnal answer than incorrect processes.
We leverage this intuition by proposing the following self-consistency method. First, a language
model is prompted with a set of manually written chain-of-thought exemplars (Wei et al., 2022). Next,
2
Published as a conference paper at ICLR 2023
GSM8K
MultiArith
AQuA
SVAMP
CSQA
ARC-c
Greedy decode
56.5
94.7
35.8
79.0
79.0
85.2
Weighted avg (unnormalized)
56.3 ± 0.0
90.5 ± 0.0
35.8 ± 0.0
73.0 ± 0.0
74.8 ± 0.0
82.3 ± 0.0
Weighted avg (normalized)
22.1 ± 0.0
59.7 ± 0.0
15.7 ± 0.0
40.5 ± 0.0
52.1 ± 0.0
51.7 ± 0.0
Weighted sum (unnormalized)
59.9 ± 0.0
92.2 ± 0.0
38.2 ± 0.0
76.2 ± 0.0
76.2 ± 0.0
83.5 ± 0.0
Weighted sum (normalized)
74.1 ± 0.0
99.3 ± 0.0
48.0 ± 0.0
86.8 ± 0.0
80.7 ± 0.0
88.7 ± 0.0
Unweighted sum (majority vote)
74.4 ± 0.1
99.3 ± 0.0
48.3 ± 0.5
86.6 ± 0.1
80.7 ± 0.1
88.7 ± 0.1
Table 1: Accuracy comparison of different answer aggregation strategies on PaLM-540B.
we sample a set of candidate outputs from the language model’s decoder, generating a diverse set of
candidate reasoning paths. Self-consistency is compatible with most existing sampling algorithms,
including temperature sampling (Ackley et al., 1985; Ficler & Goldberg, 2017), top-k sampling (Fan
et al., 2018; Holtzman et al., 2018; Radford et al., 2019), and nucleus sampling (Holtzman et al.,
2020). Finally, we aggregate the answers by marginalizing out the sampled reasoning paths and
choosing the answer that is the most consistent among the generated answers.
In more detail, assume the generated answers ai are from a ﬁxed answer set, ai ∈A, where
i = 1, . . . , m indexes the m candidate outputs sampled from the decoder. Given a prompt and a
question, self-consistency introduces an additional latent variable ri, which is a sequence of tokens
representing the reasoning path in the i-th output, then couples the generation of (ri, ai) where
ri →ai, i.e., generating a reasoning path ri is optional and only used to reach the ﬁnal answer ai. As
an example, consider Output 3 from Figure 1: the ﬁrst few sentences “She eats 3 for breakfast ... So
she has 9 eggs * $2 = $18.” constitutes ri, while the answer 18 from the last sentence, “The answer
is $18”, is parsed as ai.1 After sampling multiple (ri, ai) from the model’s decoder, self-consistency
applies a marginalization over ri by taking a majority vote over ai, i.e., arg maxa
Pm
i=1 1(ai = a),
or as we deﬁned as the most “consistent” answer among the ﬁnal answer set.
In Table 1, we show the test accuracy over a set of reasoning tasks by using different answer
aggregation strategies. In addition to majority vote, one can also weight each (ri, ai) by P(ri, ai |
prompt, question) when aggregating the answers. Note to compute P(ri, ai | prompt, question), we
can either take the unnormalized probability of the model generating (ri, ai) given (prompt, question),
or we can normalize the conditional probability by the output length (Brown et al., 2020), i.e.,
P(ri, ai | prompt, question) = exp
1
K
PK
k=1 log P (tk|prompt,question,t1,...,tk−1),
(1)
where log P(tk | prompt, question, t1, . . . , tk−1) is the log probability of generating the k-th token
tk in (ri, ai) conditioned on the previous tokens, and K is the total number of tokens in (ri, ai).
In Table 1, we show that taking the “unweighted sum”, i.e., taking a majority vote directly over ai
yields a very similar accuracy as aggregating using the “normalized weighted sum”. We took a closer
look at the model’s output probabilities and found this is because for each (ri, ai), the normalized
conditional probabilities P(ri, ai | prompt, question) are quite close to each other, i.e., the language
model regards those generations as “similarly likely”.2 Additionally, when aggregating the answers,
the results in Table 1 show that the “normalized” weighted sum (i.e., Equation 1) yields a much
higher accuracy compared to its unnormalized counterpart. For completeness, in Table 1 we also
report the results by taking a “weighted average”, i.e., each a gets a score of its weighted sum divided
by Pm
i=1 1(ai = a), which results in a much worse performance.
Self-consistency explores an interesting space between open-ended text generation and optimal
text generation with a ﬁxed answer. Reasoning tasks typically have ﬁxed answers, which is why
researchers have generally considered greedy decoding approaches (Radford et al., 2019; Wei et al.,
2022; Chowdhery et al., 2022). However, we have found that even when the desired answer is ﬁxed,
introducing diversity in the reasoning processes can be highly beneﬁcial; therefore we leverage
1The parser is task dependent. For arithmetic reasoning, we parse the ﬁrst numerical part as the ﬁnal answer
after the model generates “The answer is ”. For commonsense reasoning, we parse the full string answer as the
ﬁnal answer after the model generates “The answer is ”. Most generated outputs have a consistent format of
“{Reasoning paths}. The answer is X.” if we prompt the language model in this format.
2This also means that the language model is not well calibrated and thus cannot distinguish well between
correct solutions and wrong solutions, which also explains why additional re-rankers were trained to better judge
the quality of the solutions in previous work (Cobbe et al., 2021; Thoppilan et al., 2022).
3
Published as a conference paper at ICLR 2023
sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020;
Thoppilan et al., 2022), to achieve this goal. One should note that self-consistency can be applied
only to problems where the ﬁnal answer is from a ﬁxed answer set, but in principle this approach can
be extended to open-text generation problems if a good metric of consistency can be deﬁned between
multiple generations, e.g., whether two answers agree or contradict each other.
3
EXPERIMENTS
We conducted a series of experiments to compare the proposed self-consistency method with existing
approaches on a range of reasoning benchmarks. We ﬁnd that self-consistency robustly improves
reasoning accuracy for every language model considered, spanning a wide range of model scales.
3.1
EXPERIMENT SETUP
Tasks and datasets.
We evaluate self-consistency on the following reasoning benchmarks.3
• Arithmetic reasoning. For these tasks, we used the Math Word Problem Repository (Koncel-
Kedziorski et al., 2016), including AddSub (Hosseini et al., 2014), MultiArith (Roy & Roth,
2015), and ASDiv (Miao et al., 2020). We also included AQUA-RAT (Ling et al., 2017), a
recently published benchmark of grade-school-math problems (GSM8K; Cobbe et al., 2021),
and a challenge dataset over math word problems (SVAMP; Patel et al., 2021).
• Commonsense reasoning. For these tasks, we used CommonsenseQA (Talmor et al., 2019),
StrategyQA (Geva et al., 2021), and the AI2 Reasoning Challenge (ARC) (Clark et al., 2018).
• Symbolic Reasoning. We evaluate two symbolic reasoning tasks: last letter concatenation (e.g.,
the input is “Elon Musk” and the output should be “nk”), and Coinﬂip (e.g., a coin is heads-up,
after a few ﬂips is the coin still heads-up?) from Wei et al. (2022).
Language models and prompts.
We evaluate self-consistency over four transformer-based lan-
guage models with varying scales:
• UL2 (Tay et al., 2022) is an encoder-decoder model trained on a mixture of denoisers with 20-
billion parameters. UL2 is completely open-sourced4 and has similar or better performance than
GPT-3 on zero-shot SuperGLUE, with only 20B parameters and thus is more compute-friendly;
• GPT-3 (Brown et al., 2020) with 175-billion parameters. We use two public engines code-davinci-
001 and code-davinci-002 from the Codex series (Chen et al., 2021) to aid reproducibility;5
• LaMDA-137B (Thoppilan et al., 2022) is a dense left-to-right, decoder-only language model with
137-billion parameters, pre-trained on a mixture of web documents, dialog data and Wikipedia;
• PaLM-540B (Chowdhery et al., 2022) is a dense left-to-right, decoder-only language model with
540-billion parameters, pre-trained on a high quality corpus of 780 billion tokens with ﬁltered
webpages, books, Wikipedia, news articles, source code, and social media conversations.
We perform all experiments in the few-shot setting, without training or ﬁne-tuning the language
models. For a fair comparison we use the same prompts as in Wei et al. (2022): for all arithmetic
reasoning tasks we use the same set of 8 manually written exemplars; for each commonsense
reasoning task, 4-7 exemplars are randomly chosen from the training set with manually composed
chain-of-thought prompts.6 Full details on the prompts used are given in Appendix A.3.
Sampling scheme.
To sample diverse reasoning paths, we followed similar settings to those
suggested in Radford et al. (2019); Holtzman et al. (2020) for open-text generation. In particular, for
UL2-20B and LaMDA-137B we applied temperature sampling with T = 0.5 and truncated at the
top-k (k = 40) tokens with the highest probability, for PaLM-540B we applied T = 0.7, k = 40, and
for GPT-3 we use T = 0.7 without top-k truncation. We provide an ablation study in Section 3.5 to
show that self-consistency is generally robust to sampling strategies and parameters.
3By default we use the test split for all datasets if the labels are available for evaluation. For CommonsenseQA
we use the dev split; for StrategyQA we use the question-only set from BIG-bench collaboration (2021):
https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/strategyqa.
4Model checkpoints at https://github.com/google-research/google-research/tree/master/ul2.
5Public API available at https://openai.com/api/.
6Self-consistency is robust to different sets of prompts and we provide a study in Appendix A.1.2.
4
Published as a conference paper at ICLR 2023
3.2
MAIN RESULTS
We report the results of self-consistency averaged over 10 runs, where we sampled 40 outputs
independently from the decoder in each run. The baseline we compare to is chain-of-thought
prompting with greedy decoding (Wei et al., 2022), referred to as CoT-prompting, which has been
previously used for decoding in large language models (Chowdhery et al., 2022).
Arithmetic Reasoning
The results are shown in Table 2.7 Self-consistency improves the arithmetic
reasoning performance over all four language models signiﬁcantly over chain-of-thought prompting.
More surprisingly, the gains become more signiﬁcant when the language model’s scale increases,
e.g., we see +3%-6% absolute accuracy improvement over UL2-20B but +9%-23% for LaMDA-
137B and GPT-3. For larger models that already achieve high accuracy on most tasks (e.g., GPT-3
and PaLM-540B), self-consistency still contributes signiﬁcant additional gains with +12%-18%
absolute accuracy on tasks like AQuA and GSM8K, and +7%-11% on SVAMP and ASDiv. With
self-consistency, we achieve new state-of-the-art results on almost all tasks: despite the fact that self-
consistency is unsupervised and task-agnostic, these results compare favorably to existing approaches
that require task-speciﬁc training, or ﬁne-tuning with thousands of examples (e.g., on GSM8K).
Method
AddSub
MultiArith ASDiv
AQuA
SVAMP
GSM8K
Previous SoTA
94.9a
60.5a
75.3b
37.9c
57.4d
35e / 55g
UL2-20B
CoT-prompting
18.2
10.7
16.9
23.6
12.6
4.1
Self-consistency 24.8 (+6.6)
15.0 (+4.3)
21.5 (+4.6) 26.9 (+3.3)
19.4 (+6.8)
7.3 (+3.2)
LaMDA-137B CoT-prompting
52.9
51.8
49.0
17.7
38.9
17.1
Self-consistency 63.5 (+10.6) 75.7 (+23.9) 58.2 (+9.2) 26.8 (+9.1)
53.3 (+14.4) 27.7 (+10.6)
PaLM-540B
CoT-prompting
91.9
94.7
74.0
35.8
79.0
56.5
Self-consistency 93.7 (+1.8)
99.3 (+4.6)
81.9 (+7.9) 48.3 (+12.5) 86.6 (+7.6)
74.4 (+17.9)
GPT-3
Code-davinci-001
CoT-prompting
57.2
59.5
52.7
18.9
39.8
14.6
Self-consistency 67.8 (+10.6) 82.7 (+23.2) 61.9 (+9.2) 25.6 (+6.7)
54.5 (+14.7) 23.4 (+8.8)
GPT-3
Code-davinci-002
CoT-prompting
89.4
96.2
80.1
39.8
75.8
60.1
Self-consistency 91.6 (+2.2)
100.0 (+3.8) 87.8 (+7.6) 52.0 (+12.2) 86.8 (+11.0) 78.0 (+17.9)
Table 2: Arithmetic reasoning accuracy by self-consistency compared to chain-of-thought prompting
(Wei et al., 2022). The previous SoTA baselines are obtained from: a: Relevance and LCA operation
classiﬁer (Roy & Roth, 2015), b: Lan et al. (2021), c: Amini et al. (2019), d: Pi et al. (2022), e:
GPT-3 175B ﬁnetuned with 7.5k examples (Cobbe et al., 2021), g: GPT-3 175B ﬁnetuned plus an
additional 175B veriﬁer (Cobbe et al., 2021). The best performance for each task is shown in bold.
Method
CSQA
StrategyQA ARC-e
ARC-c
Letter (4) Coinﬂip (4)
Previous SoTA
91.2a
73.9b
86.4c
75.0c
N/A
N/A
UL2-20B
CoT-prompting
51.4
53.3
61.6
42.9
0.0
50.4
Self-consistency 55.7 (+4.3) 54.9 (+1.6)
69.8 (+8.2) 49.5 (+6.8)
0.0 (+0.0)
50.5 (+0.1)
LaMDA-137B CoT-prompting
57.9
65.4
75.3
55.1
8.2
72.4
Self-consistency 63.1 (+5.2) 67.8 (+2.4)
79.3 (+4.0) 59.8 (+4.7)
8.2 (+0.0)
73.5 (+1.1)
PaLM-540B
CoT-prompting
79.0
75.3
95.3
85.2
65.8
88.2
Self-consistency 80.7 (+1.7) 81.6 (+6.3)
96.4 (+1.1) 88.7 (+3.5)
70.8 (+5.0) 91.2 (+3.0)
GPT-3
Code-davinci-001
CoT-prompting
46.6
56.7
63.1
43.1
7.8
71.4
Self-consistency 54.9 (+8.3) 61.7 (+5.0)
72.1 (+9.0) 53.7 (+10.6) 10.0 (+2.2) 75.9 (+4.5)
GPT-3
Code-davinci-002
CoT-prompting
79.0
73.4
94.0
83.6
70.4
99.0
Self-consistency 81.5 (+2.5) 79.8 (+6.4)
96.0 (+2.0) 87.5 (+3.9)
73.4 (+3.0) 99.5 (+0.5)
Table 3: Commonsense and symbolic reasoning accuracy by self-consistency compared to chain-
of-thought prompting (Wei et al., 2022). The previous SoTA baselines are obtained from: a:
DeBERTaV3-large + KEAR (Xu et al., 2021b), b: Chowdhery et al. (2022), c: UniﬁedQA-FT
(Khashabi et al., 2020). The best performance for each task is shown in bold.
7The standard deviation of self-consistency is ≤0.5 for all tasks and is thus omitted in the table. Please refer
to Figure 2, Figure 7 and 8 for the standard deviations under varying numbers of sampled paths.
5
Published as a conference paper at ICLR 2023
Commonsense and Symbolic Reasoning
Table 3 shows the results on commonsense and symbolic
reasoning tasks. Similarly, self-consistency yields large gains across all four language models, and
obtained SoTA results on 5 out of 6 tasks. For symbolic reasoning, we test the out-of-distribution
(OOD) setting where the input prompt contains examples of 2-letters or 2-ﬂips but we test examples
of 4-letters and 4-ﬂips (this setting is more challenging as PaLM-540B or GPT-3 can already achieve
perfect in-distribution accuracy). In this challenging OOD setting, the gain of self-consistency is still
quite signiﬁcant compared to CoT-prompting with sufﬁcient model sizes.
To show the effect of the number of sampled reasoning paths, we plot the accuracy (mean and
standard deviation over 10 runs) with respect to varying numbers of sampled paths (1, 5, 10, 20, 40)
in Figure 2. The results show that sampling a higher number (e.g., 40) of reasoning paths leads to a
consistently better performance, further emphasizing the importance of introducing diversity in the
reasoning paths. In Table 4, we show self-consistency yields a richer set of reasoning paths compared
to greedy decoding with a few example questions from two tasks.
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
50
55
60
65
70
75
Accuracy (%)
MultiArith
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
33
36
39
42
45
48
51
54
SVAMP
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
56
58
60
62
Commonsense QA
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Paths
50
52
54
56
58
60
ARC (Challenge)
Greedy Decode (Single-path)
Self Consistency (Multi-path)
Figure 2: Self-consistency (blue) signiﬁcantly improves accuracy over CoT-prompting with greedy
decoding (orange) across arithmetic and commonsense reasoning tasks, over LaMDA-137B. Sampling
a higher number of diverse reasoning paths consistently improves reasoning accuracy.
GSM8K
StrategyQA
Question
Henry made two stops during his 60-mile bike trip. He ﬁrst stopped after 20
miles. His second stop was 15 miles before the end of the trip. How many miles
did he travel between his ﬁrst and second stops?
Yes or no: Is Albany, Georgia the most populous
US Albany?
Greedy
Decode
He traveled 60 miles in total. He stopped after 20 miles, so he traveled 60 - 20
= 40 miles between the ﬁrst and second stops. The answer is 40.
Albany, Georgia is the most populous US Al-
bany. Thus, the answer is yes.
Sampled
Path 1
Henry travelled a total of 60 miles. His ﬁrst stop was 20 miles in, and his
second stop was 15 miles before the end. So between his ﬁrst and second stops
he travelled 60 - 20 -15 = 25 miles. The answer is 25.
The most populous US Albany is Albany, New
York. Thus, Albany, Georgia is not the most
populous US Albany. So the answer is no.
Sampled
Path 2
He made two stops during a 60-mile trip. The ﬁrst was 20 miles into the trip.
The second was 15 miles before the end of the trip. This means the second stop
was 60 - 15 = 45 miles into the trip. Since he made the stops in order, the second
stop must have been 45 - 20 = 25 miles after the ﬁrst stop. The answer is 25.
Albany, Georgia has a population of about
88,000. Albany, New York has a population of
about 95,000. Thus, Albany, Georgia is not the
most populous US Albany. So the answer is no.
Table 4: Examples where self-consistency helps repair the errors over greedy decode, on PaLM-540B.
Two sampled reasoning paths that are consistent with the ground truth are shown.
3.3
SELF-CONSISTENCY HELPS WHEN CHAIN-OF-THOUGHT HURTS PERFORMANCE
Ye & Durrett (2022) show that sometimes chain-of-thought prompting could hurt performance
compared to standard prompting in few-shot in-context learning. Here we perform a study using
self-consistency to see if it can help ﬁll in the gap, over a set of common NLP tasks, including (1)
Closed-Book Question Answering: BoolQ (Clark et al., 2019), HotpotQA (Yang et al., 2018), and
(2) Natural Language Inference: e-SNLI (Camburu et al., 2018), ANLI (Nie et al., 2020) and RTE
(Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009).
The results over PaLM-540B are shown in Table 5. For some tasks (e.g., ANLI-R1, e-SNLI, RTE),
adding chain-of-thought does hurt performance compared to standard prompting (Brown et al., 2020),
but self-consistency is able to robustly boost the performance and outperform standard prompting,
making it a reliable way to add rationales in few-shot in-context learning for common NLP tasks.
ANLI R1 / R2 / R3
e-SNLI
RTE
BoolQ
HotpotQA (EM/F1)
Standard-prompting (no-rationale)
69.1 / 55.8 / 55.8
85.8
84.8
71.3
27.1 / 36.8
CoT-prompting (Wei et al., 2022)
68.8 / 58.9 / 60.6
81.0
79.1
74.2
28.9 / 39.8
Self-consistency
78.5 / 64.5 / 63.4
88.4
86.3
78.4
33.8 / 44.6
Table 5: Compare Standard/CoT prompting with self-consistency on common NLP tasks.
6
Published as a conference paper at ICLR 2023
3.4
COMPARE TO OTHER EXISTING APPROACHES
We conduct a set of additional studies and show that self-consistency signiﬁcantly outperforms
existing methods including sample-and-rank, beam search, and ensemble-based approaches.
Comparison to Sample-and-Rank
One commonly used approach to improve generation quality is
sample-and-rank, where multiple sequences are sampled from the decoder and then ranked according
to each sequence’s log probability (Adiwardana et al., 2020). We compare self-consistency with
sample-and-rank on GPT-3 code-davinci-001, by sampling the same number of sequences from the
decoder as self-consistency and taking the ﬁnal answer from the top-ranked sequence. The results are
shown in Figure 3. While sample-and-rank does improve the accuracy with additionally sampled
sequences and ranking, the gain is much smaller compared to self-consistency.
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
12
14
16
18
20
22
24
Accuracy (%)
GSM8K
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
50
55
60
65
70
75
80
Accuracy (%)
MultiArith
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
30
35
40
45
50
55
Accuracy (%)
ARC (Challenge)
Self Consistency (Multi-path)
Sample & Rank (Multi-path)
Greedy Decode (Single-path)
Figure 3: Self-consistency signiﬁcantly outperforms sample-and-rank with the same # of samples.
Comparison to Beam Search
In Table 6, we compare self-consistency with beam search decoding
on the UL2-20B model. For a fair comparison we report the accuracy under the same number of
beams and reasoning paths. On both tasks self-consistency outperforms beam search signiﬁcantly.
Note self-consistency can also adopt beam search to decode each reasoning path (results are shown
as “Self-consistency using beam search”), but its performance is worse compared to self-consistency
with sampling. The reason is that beam search yields a lower diversity in the outputs (Li & Jurafsky,
2016), while in self-consistency the diversity of the reasoning paths is the key to a better performance.
Beam size / Self-consistency paths
1
5
10
20
40
AQuA
Beam search decoding (top beam)
23.6
19.3
16.1
15.0
10.2
Self-consistency using beam search
23.6
19.8 ± 0.3 21.2 ± 0.7 24.6 ± 0.4 24.2 ± 0.5
Self-consistency using sampling
19.7 ± 2.5 24.9 ± 2.6 25.3 ± 1.8 26.7 ± 1.0 26.9 ± 0.5
MultiArith
Beam search decoding (top beam)
10.7
12.0
11.3
11.0
10.5
Self-consistency using beam search
10.7
11.8 ± 0.0 11.4 ± 0.1 12.3 ± 0.1 10.8 ± 0.1
Self-consistency using sampling
9.5 ± 1.2
11.3 ± 1.2 12.3 ± 0.8 13.7 ± 0.9 14.7 ± 0.3
Table 6: Compare self-consistency with beam search decoding on the UL2-20B model.
Comparison to Ensemble-based Approaches
We further compare self-consistency to ensemble-
based methods for few-shot learning. In particular, we consider ensembling by: (1) prompt order
permutation: we randomly permute the exemplars in the prompt 40 times to mitigate model’s
sensitivity to prompt order (Zhao et al., 2021; Lu et al., 2021); and (2) multiple sets of prompts
(Gao et al., 2021): we manually write 3 different sets of prompts. We took majority vote of the
answers from greedy decoding in both approaches as an ensemble. Table 7 shows that compared to
self-consistency, existing ensemble-based approaches achieve a much smaller gain.8 In addition, note
that self-consistency is different from a typical model-ensemble approach, where multiple models
are trained and their outputs are aggregated. Self-consistency acts more like a “self-ensemble” on
top of a single language model. We additionally show the results of ensembling multiple models in
Appendix A.1.3 where the model-ensembles perform much worse compared to self-consistency.
GSM8K
MultiArith
SVAMP
ARC-e
ARC-c
CoT (Wei et al., 2022)
17.1
51.8
38.9
75.3
55.1
Ensemble (3 sets of prompts)
18.6 ± 0.5
57.1 ± 0.7
42.1 ± 0.6
76.6 ± 0.1
57.0 ± 0.2
Ensemble (40 prompt permutations)
19.2 ± 0.1
60.9 ± 0.2
42.7 ± 0.1
76.9 ± 0.1
57.0 ± 0.1
Self-Consistency (40 sampled paths)
27.7 ± 0.2
75.7 ± 0.3
53.3 ± 0.2
79.3 ± 0.3
59.8 ± 0.2
Table 7: Self-consistency outperforms prompt-order and multi-prompt ensembles on LaMDA-137B.
8Self-consistency is compatible with both ensemble approaches and we show the results in Appendix A.1.4.
7
Published as a conference paper at ICLR 2023
3.5
ADDITIONAL STUDIES
We conducted a number of additional experiments to analyze different aspects of the self-consistency
method, including its robustness to sampling strategies and parameters, and how it works with
imperfect prompts and non-natural-language reasoning paths.
Self-Consistency is Robust to Sampling Strategies and Scaling
We show self-consistency is
robust to sampling strategies and parameters, by varying T in temperature sampling (Ackley et al.,
1985; Ficler & Goldberg, 2017), k in top-k sampling (Fan et al., 2018; Holtzman et al., 2018; Radford
et al., 2019), and p in nucleus sampling (Holtzman et al., 2020), over PaLM-540B in Figure 4 (left).
Figure 4 (right) shows that self-consistency robustly improves performance across all scales for the
LaMDA-137B model series. The gain is relatively lower for smaller models due to certain abilities
(e.g., arithmetic) only emerge when the model reaches a sufﬁcient scale (Brown et al., 2020).
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
44
48
52
56
60
64
68
72
76
Accuracy (%)
T=0.7, k=40
T=0.5, k=40
T=0.3, k=40
T=0.7, k=20
T=0.7, no top k
p=0.95
p=0.9
Greedy Decode
1
2
5
10
20
50 100 200
Model size (#param in billions)
5
10
15
20
25
Accuracy (%)
Self Consistency
Greedy Decode
Figure 4: GSM8K accuracy. (Left) Self-consistency is robust to various sampling strategies and
parameters. (Right) Self-consistency improves performance across language model scales.
Self-Consistency Improves Robustness to Imperfect Prompts
For few-shot learning with man-
ually constructed prompts, human annotators sometimes make minor mistakes when creating the
prompts. We further study if self-consistency can help improve a language model’s robustness to
imperfect prompts.9 We show the results in Table 8: while imperfect prompts decrease accuracy with
greedy decoding (17.1 →14.9), self-consistency can ﬁll in the gaps and robustly improve the results.
Additionally, we found that the consistency (in terms of % of decodes agreeing with the ﬁnal
aggregated answer) is highly correlated with accuracy (Figure 5, over GSM8K). This suggests that
one can use self-consistency to provide an uncertainty estimate of the model in its generated solutions.
In other words, one can use low consistency as an indicator that the model has low conﬁdence; i.e.,
self-consistency confers some ability for the model to “know when it doesn’t know”.
LaMDA-137B
Prompt with correct chain-of-thought
17.1
Prompt with imperfect chain-of-thought
14.9
+ Self-consistency (40 paths)
23.4
Prompt with equations
5.0
+ Self-consistency (40 paths)
6.5
PaLM-540B
Zero-shot CoT (Kojima et al., 2022)
43.0
+ Self-consistency (40 paths)
69.2
Table 8: Self-consistency works under imperfect prompts, equa-
tion prompts and zero-shot chain-of-thought for GSM8K.
0
20
40
60
80
100
Consistency (%)
0
20
40
60
80
100
Accuracy (%)
Figure 5: The consistency is cor-
related with model’s accuracy.
Self-Consistency Works for Non-Natural-Language Reasoning Paths and Zero-shot CoT
We
also tested the generality of the self-consistency concept to alternative forms of intermediate reasoning
like equations (e.g., from “There are 3 cars in the parking lot already. 2 more arrive. Now there
are 3 + 2 = 5 cars.” to “3 + 2 = 5”). The results are shown in Table 8 (“Prompt with equations”):
self-consistency still improves accuracy by generating intermediate equations; however, compared to
generating natural language reasoning paths, the gain is smaller since the equations are much shorter
and less opportunity remains for generating diversity in the decoding process. In addition, we tested
self-consistency with zero-shot chain-of-thought (Kojima et al., 2022) and show that self-consistency
works for zero-shot CoT as well and improves the results signiﬁcantly (+26.2%) in Table 8.
9We use the same prompts as before, but swap all the numbers in the reasoning paths with random numbers
except the ﬁnal answer, e.g., from “There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 +
2 = 5 cars.” to “There are 7 cars in the parking lot already. 6 more arrive. Now there are 7 + 6 = 5 cars.”.
8
Published as a conference paper at ICLR 2023
4
RELATED WORK
Reasoning in language models.
Language models are known to struggle in Type 2 tasks, such as
arithmetic, logical and commonsense reasoning (Evans, 2010). Previous work has primarily focused
on specialized approaches for improving reasoning (Andor et al., 2019; Ran et al., 2019; Geva et al.,
2020; Pi˛
ekos et al., 2021). Compared to prior work, self-consistency is applicable to a wide range of
reasoning tasks without any additional supervision or ﬁne-tuning, while still substantially improving
the performance of the chain-of-thought prompting approach proposed in Wei et al. (2022).
Sampling and re-ranking in language models.
Multiple decoding strategies for language models
have been proposed in the literature, e.g., temperature sampling (Ackley et al., 1985; Ficler &
Goldberg, 2017), top-k sampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019),
nucleus sampling (Holtzman et al., 2020), minimum Bayes risk decoding (Eikema & Aziz, 2020; Shi
et al., 2022), and typical decoding (Meister et al., 2022). Other work has sought to explicitly promote
diversity in the decoding process (Batra et al., 2012; Li et al., 2016; Vijayakumar et al., 2018).
Re-ranking is another common approach to improve generation quality in language models (Adiwar-
dana et al., 2020; Shen et al., 2021). Thoppilan et al. (2022) collect additional human annotations
to train a re-ranker for response ﬁltering. Cobbe et al. (2021) train a “veriﬁer” to re-rank generated
solutions, which substantially improves the solve rate on math tasks compared to just ﬁne-tuning the
language model. Elazar et al. (2021) improve the consistency of factual knowledge extraction by
extending pre-training with an additional consistency loss. All these methods require either training
an additional re-ranker or collecting additional human annotation, while self-consistency requires no
additional training, ﬁne-tuning, nor extra data collection.
Extract reasoning paths.
Some previous work has considered task-speciﬁc approaches for iden-
tifying reasoning paths, such as constructing semantic graphs (Xu et al., 2021a), learning an RNN
to retrieve reasoning paths over the Wikipedia graph (Asai et al., 2020), ﬁne-tuning with human
annotated reasoning paths on math problems (Cobbe et al., 2021), or training an extractor with
heuristic-based pseudo reasoning paths (Chen et al., 2019). More recently, the importance of di-
versity in the reasoning processes has been noticed, but only leveraged via task-speciﬁc training,
either through an additional QA model over extracted reasoning paths (Chen et al., 2019), or by the
introduction of latent variables in a commonsense knowledge graph (Yu et al., 2022). Compared to
these approaches, self-consistency is far simpler and requires no additional training. The approach
we propose simply couples the generation of reasoning paths and a ﬁnal answer by sampling from
the decoder, using aggregation to recover the most consistent answer without additional modules.
Consistency in language models.
Some prior work has shown that language models can suffer
from inconsistency in conversation (Adiwardana et al., 2020), explanation generation (Camburu et al.,
2020), and factual knowledge extraction (Elazar et al., 2021). Welleck et al. (2020) use “consistency”
to refer to generating an inﬁnite-length sequence in recurrent language models. Nye et al. (2021)
improve the logical consistency of samples from a System 1 model by adding a System 2-inspired
logical reasoning module. In this paper we focus on a slightly different notion of “consistency”, i.e.,
utilizing answer consistency among diverse reasoning paths to improve accuracy.
5
CONCLUSION AND DISCUSSION
We introduced a simple yet effective method called self-consistency, and observed that it signiﬁcantly
improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large
language models with varying scales. Beyond accuracy gains, self-consistency is also useful for
collecting rationales when performing reasoning tasks with language models, and for providing
uncertainty estimates and improved calibration of language model outputs.
One limitation of self-consistency is that it incurs more computation cost. In practice people can try a
small number of paths (e.g., 5 or 10) as a starting point to realize most of the gains while not incurring
too much cost, as in most cases the performance saturates quickly (Figure 2). As part of future work,
one could use self-consistency to generate better supervised data to ﬁne-tune the model, such that the
model can give more accurate predictions in a single inference run after ﬁne-tuning. In addition, we
observed that language models can sometimes generate incorrect or nonsensical reasoning paths (e.g.,
the StrategyQA example in Table 4, the two population numbers are not exactly correct), and further
work is needed to better ground models’ rationale generations.
9
Published as a conference paper at ICLR 2023
REPRODUCIBILITY STATEMENT
In experiments, we included four different language models with varying scales. Two of them are pub-
lic models: UL2 is a completely open-sourced model with model checkpoints available at https://
github.com/google-research/google-research/tree/master/ul2; GPT-3 is
also a public model with public API available at https://openai.com/api/. For GPT-3,
we have included two public engines (“code-davinci-001” and “code-davinci-002”) to further aid
reproducibility, as Codex is currently free so anyone can reproduce the results. In addition, as our
results make use of LaMDA-137B and PaLM-540B that are not publicly available, we provide the
exact input prompts for all tasks in Appendix A.3 (and note that we do not perform any ﬁnetuning
and only apply prompting to off-the-shelf language models).
ETHICS STATEMENT
As we stated in the discussion, language models can sometimes generate nonsensical or non-factual
reasoning paths, so one should use language models’ outputs with extra caution. We deal with
reasoning tasks mostly and the generated rationales are only used for inspecting how a model reaches
its answer. One could potentially use the generated rationales to further check why the model makes
certain mistakes or whether the model contains any biases when performing a certain task. For
language model in real-world use, further work is needed to better ground models’ predictions and
improve model’s factuality and safety, to ensure the models do not cause harms to users.
REFERENCES
David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for boltzmann
machines. Cognitive Science, 9(1):147–169, 1985. ISSN 0364-0213. URL https://www.
sciencedirect.com/science/article/pii/S0364021385800124.
Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan,
Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. Towards a human-like
open-domain chatbot, 2020.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh
Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based
formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers), pp. 2357–2367. Association for Computational Linguistics, June 2019. URL
https://aclanthology.org/N19-1245.
Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Giving BERT a calculator: Finding
operations and arguments with reading comprehension. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), 2019. URL https://aclanthology.
org/D19-1609.
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learn-
ing to retrieve reasoning paths over wikipedia graph for question answering. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=SJgVHkrYDH.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and
Idan Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the
second PASCAL challenges workshop on recognising textual entailment, 2006.
Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse
m-best solutions in markov random ﬁelds. In Proceedings of the 12th European Conference on
Computer Vision - Volume Part V, ECCV’12, pp. 1–16, Berlin, Heidelberg, 2012. Springer-Verlag.
ISBN 9783642337147. URL https://doi.org/10.1007/978-3-642-33715-4_1.
10
Published as a conference paper at ICLR 2023
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing
textual entailment challenge. In TAC, 2009.
BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabil-
ities of language models. In preparation, 2021. URL https://github.com/google/
BIG-bench/.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information
Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/
file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom.
e-
snli:
Natural language inference with natural language explanations.
In S. Ben-
gio,
H.
Wallach,
H.
Larochelle,
K.
Grauman,
N.
Cesa-Bianchi,
and
R.
Garnett
(eds.),
Advances
in
Neural
Information
Processing
Systems
31,
pp.
9539–9549.
Curran
Associates,
Inc.,
2018.
URL
http://papers.nips.cc/paper/
8163-e-snli-natural-language-inference-with-natural-language-explanations.
pdf.
Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, and Phil
Blunsom. Make up your mind! adversarial generation of inconsistent natural language explanations.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
4157–4165, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
acl-main.382. URL https://aclanthology.org/2020.acl-main.382.
Jifan Chen, Shih-Ting Lin, and Greg Durrett. Multi-hop question answering via reasoning chains.
CoRR, abs/1910.02610, 2019. URL http://arxiv.org/abs/1910.02610.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon
Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,
Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL
https://arxiv.org/abs/2204.02311.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. Boolq: Exploring the surprising difﬁculty of natural yes/no questions. In NAACL,
2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
ArXiv, abs/1803.05457, 2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training veriﬁers to solve math word problems, 2021.
11
Published as a conference paper at ICLR 2023
Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment
challenge. In Machine Learning Challenges Workshop, pp. 177–190. Springer, 2005.
Bryan Eikema and Wilker Aziz. Is MAP decoding all you need? the inadequacy of the mode in neural
machine translation. In Proceedings of the 28th International Conference on Computational Lin-
guistics, pp. 4506–4520, Barcelona, Spain (Online), December 2020. International Committee on
Computational Linguistics. URL https://aclanthology.org/2020.coling-main.
398.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich
Schütze, and Yoav Goldberg. Measuring and improving consistency in pretrained language
models. Transactions of the Association for Computational Linguistics, 9:1012–1031, 2021. doi:
10.1162/tacl_a_00410. URL https://aclanthology.org/2021.tacl-1.60.
Jonathan St BT Evans. Intuition and reasoning: A dual-process perspective. Psychological Inquiry,
21(4):313–326, 2010.
Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics.
doi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.
Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. In
Proceedings of the Workshop on Stylistic Variation, pp. 94–104, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4912. URL https:
//aclanthology.org/W17-4912.
Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-
tics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pp. 3816–3830, Online, August 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021.acl-long.
295.
Mor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into language
models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguis-
tics, 2020. doi: 10.18653/v1/2020.acl-main.89. URL https://aclanthology.org/2020.
acl-main.89.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle
use a laptop? A question answering benchmark with implicit reasoning strategies. Transactions of
the Association for Computational Linguistics, 2021. URL https://aclanthology.org/
2021.tacl-1.21.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third pascal recognizing
textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment
and paraphrasing, pp. 1–9. Association for Computational Linguistics, 2007.
Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. Learning
to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 1638–1649, Melbourne,
Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1152.
URL https://aclanthology.org/P18-1152.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=rygGQyrFvH.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to
solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), 2014. doi: 10.3115/v1/D14-1058.
URL https://aclanthology.org/D14-1058.
12
Published as a conference paper at ICLR 2023
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings
of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907, Online, Novem-
ber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.171.
URL https://aclanthology.org/2020.findings-emnlp.171.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=e2TBb5y0yFf.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS:
A math word problem repository. In Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016.
doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,
and Ee-Peng Lim. MWPToolkit: An open-source framework for deep learning-based math word
problem solvers. arXiv preprint arXiv:2109.00799, 2021. URL https://arxiv.org/abs/
2109.00799.
Jiwei Li and Dan Jurafsky. Mutual information and diverse decoding improve neural machine
translation, 2016. URL https://arxiv.org/abs/1601.00372.
Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural
generation. CoRR, abs/1611.08562, 2016. URL http://arxiv.org/abs/1611.08562.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017. doi:
10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
Fantastically
ordered prompts and where to ﬁnd them: Overcoming few-shot prompt order sensitivity. ArXiv,
abs/2104.08786, 2021.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Typical decoding for natural language
generation. arXiv preprint arXiv:2202.00666, 2022.
Shen Yun Miao, Chao Chun Liang, and Keh Yih Su. A diverse corpus for evaluating and developing
English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, 2020. URL https://aclanthology.org/2020.
acl-main.92.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adver-
sarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics. Association for Computational
Linguistics, 2020.
Maxwell Nye, Michael Henry Tessler, Joshua B. Tenenbaum, and Brenden M. Lake. Improving
coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning.
In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural
Information Processing Systems, 2021. URL https://openreview.net/forum?id=
uyKk_avJ-p4.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, pp. 2080–2094,
Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.
168. URL https://aclanthology.org/2021.naacl-main.168.
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and
Weizhu Chen. Reasoning like program executors, 2022.
13
Published as a conference paper at ICLR 2023
Piotr Pi˛
ekos, Mateusz Malinowski, and Henryk Michalewski. Measuring and improving BERT’s
mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 2: Short Papers), 2021. doi: 10.18653/v1/2021.acl-short.49.
URL https://aclanthology.org/2021.acl-short.49.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension
with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), 2019. doi: 10.18653/v1/D19-1251. URL https://aclanthology.
org/D19-1251.
Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing, 2015. doi: 10.18653/v1/
D15-1202. URL https://aclanthology.org/D15-1202.
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate
& rank: A multi-task framework for math word problems. In Findings of the Association for
Computational Linguistics: EMNLP 2021, pp. 2269–2279, Punta Cana, Dominican Republic,
November 2021. Association for Computational Linguistics. URL https://aclanthology.
org/2021.findings-emnlp.195.
Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural
language to code translation with execution. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, pp. 3533–3546, Abu Dhabi, United Arab Emirates,
December 2022. Association for Computational Linguistics. URL https://aclanthology.
org/2022.emnlp-main.231.
Keith E Stanovich and Richard F West. Individual differences in reasoning: Implications for
the rationality debate?
Behavioral and brain sciences, 23(5):645–665, 2000. URL https:
//pubmed.ncbi.nlm.nih.gov/11301544/.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question
answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), 2019. URL https://aclanthology.
org/N19-1421.
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung,
Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. Unifying
language learning paradigms, 2022. URL https://arxiv.org/abs/2205.05131.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog
applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/
2201.08239.
Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 32, Apr. 2018. URL https:
//ojs.aaai.org/index.php/AAAI/article/view/12340.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc
Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models.
Conference on Neural Information Processing Systems (NeurIPS), 2022. URL https://arxiv.
org/pdf/2201.11903.
14
Published as a conference paper at ICLR 2023
Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. Consistency
of a recurrent language model with respect to incomplete decoding. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5553–5568,
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
emnlp-main.448. URL https://aclanthology.org/2020.emnlp-main.448.
Weiwen Xu, Yang Deng, Huihui Zhang, Deng Cai, and Wai Lam. Exploiting reasoning chains
for multi-hop science question answering. In Findings of the Association for Computational
Linguistics: EMNLP 2021, pp. 1143–1156, Punta Cana, Dominican Republic, November 2021a.
Association for Computational Linguistics. URL https://aclanthology.org/2021.
findings-emnlp.99.
Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng
Gao, Pengcheng He, Michael Zeng, and Xuedong Huang. Human parity on commonsenseqa:
Augmenting self-attention with external attention, 2021b. URL https://arxiv.org/abs/
2112.03254.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Lin-
guistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.
Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems, 2022. URL https://openreview.net/forum?
id=Bct2f8fRd8S.
Wenhao Yu, Chenguang Zhu, Lianhui Qin, Zhihan Zhang, Tong Zhao, and Meng Jiang. Diversifying
content generation for commonsense reasoning with mixture of knowledge graph experts. In
Findings of Annual Meeting of the Association for Computational Linguistics (ACL), 2022.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving
few-shot performance of language models. In Marina Meila and Tong Zhang (eds.), Proceed-
ings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of
Machine Learning Research. PMLR, 2021. URL https://proceedings.mlr.press/
v139/zhao21c.html.
15
Published as a conference paper at ICLR 2023
A
APPENDIX
A.1
ADDITIONAL EXPERIMENT RESULTS
A.1.1
ROBUSTNESS TO SAMPLING STRATEGIES AND PARAMETERS
In Figure 6 we ablate the results with respect to different sampling strategies and parameters by
varying T in temperature sampling and k in Top-k sampling, on LaMDA-137B. We show that
self-consistency is robust to various sampling strategies and parameters.
4
8 12 16 20 24 28 32 36 40
#Sampled Reasoning Paths
18
20
22
24
26
28
Accuracy (%)
T=0.7, k=40
T=0.5, k=40
T=0.3, k=40
T=0.5, k=20
T=0.5, no top k
Greedy Decode
Figure 6: GSM8K accuracy over LaMDA-137B. Self-consistency works under various sampling
strategies and sampling parameters.
In Figure 7 and Figure 8, we show the results of self-consistency compared with greedy decoding a
single path over LaMDA-137B and PaLM-540B, respectively. Self-consistency improves over greedy
decode by a quite signiﬁcant margin on both models, on top of high accuracy already achieved by
scaling up model sizes.
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
50
55
60
65
70
75
Accuracy (%)
MultiArith
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
44
46
48
50
52
54
56
58
ASDiv
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
33
36
39
42
45
48
51
54
SVAMP
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Paths
14
16
18
20
22
24
26
28
GSM8K
Greedy Decode (Single-path)
Self Consistency (Multi-path)
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
56
58
60
62
Accuracy (%)
Commonsense QA
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
62
63
64
65
66
67
68
Strategy QA
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
68
70
72
74
76
78
ARC (Easy)
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Paths
50
52
54
56
58
60
ARC (Challenge)
Greedy Decode (Single-path)
Self Consistency (Multi-path)
Figure 7: Self-consistency (blue) signiﬁcantly improves accuracy across various arithmetic and
commonsense reasoning tasks, over LaMDA-137B. Sampling a higher number of diverse reasoning
paths consistently improves reasoning accuracy.
We further show additional sampled reasoning paths from the LaMDA-137B model in Table 12, and
sampled reasoning paths from the PaLM-540B model in Table 13. We see that the diversity in the
additionally sampled reasoning paths indeed helps the model arrive at a more correct ﬁnal answer
after aggregation.
A.1.2
ROBUSTNESS TO DIFFERENT SETS OF PROMPTS
In Table 9, we further show that self-consistency is quite robust to different sets of input prompts.
We manually wrote 3 different sets of chain-of-thought as prompts to the model. Across all sets of
prompts, self-consistency yields consistent gains over the original CoT approach.
A.1.3
COMPARED TO MODEL ENSEMBLES
Additionally, we provide results of directly ensembling the outputs from multiple language models.
The results are shown in Table 10, by greedily decoding sequences from 3 language models and
16
Published as a conference paper at ICLR 2023
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Chains
86
88
90
92
94
Accuracy (%)
AddSub
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Chains
72
74
76
78
80
82
Accuracy (%)
ASDiv
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Chains
30
33
36
39
42
45
48
Accuracy (%)
AQuA
Greedy Decode (Single-path)
Self Consistency (Multi-path)
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Chains
88
90
92
94
96
98
Accuracy (%)
MultiArith
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Chains
70.0
72.5
75.0
77.5
80.0
82.5
85.0
87.5
Accuracy (%)
SVAMP
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Chains
50
55
60
65
70
75
Accuracy (%)
GSM8K
Greedy Decode (Single-path)
Self Consistency (Multi-path)
0
5
10 15 20 25 30 35 40
#Sampled Reasoning Paths
74
75
76
77
78
79
80
81
Accuracy (%)
Commonsense QA
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Paths
74
76
78
80
82
Strategy QA
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Paths
88
90
92
94
96
ARC (Easy)
0
5
10
15
20
25
30
35
40
#Sampled Reasoning Paths
78
80
82
84
86
88
ARC (Challenge)
Greedy Decode (Single-path)
Self Consistency (Multi-path)
Figure 8: Self-consistency (blue) signiﬁcantly improves accuracy across various arithmetic and
commonsense reasoning tasks, over PaLM-540B. Sampling a higher number of diverse reasoning
paths consistently helps reasoning accuracy.
Prompt set 1 (used in the main text)
Prompt set 2
Prompt set 3
CoT (Wei et al., 2022)
56.5
54.6
54.0
Self-consistency
74.4 (+17.9)
72.1 (+17.5)
70.4 (+16.4)
Table 9: GSM8K accuracy over PaLM-540B. The results show robustness of self-consistency with
respect to different prompts in the input.
taking the majority vote (averaged over 10 runs). Note this is a typical ensemble approach (averaging
over the predictions over multiple models) and it achieves a performance signiﬁcantly worse than
self-consistency (self-consistency over PaLM-540B gets an accuracy of 74.4%), as lower-capacity
models drag down the performance of higher-capacity models. In addition, this approach is limited in
two ways: 1) It requires multiple models for an ensemble which might not always be available, while
self-consistency only requires one single model to “self-ensemble”; 2) If one of the models is much
weaker, it can actually hurt the ﬁnal performance.
Method
GSM8K accuracy
Single model
PaLM-540B, greedy / self-consistency
56.5 / 74.4
Ensemble of models
LaMDA-137B + PaLM-540B
36.9 ± 0.5
PaLM-540B + GPT-3 (code-davinci-001, 175B)
36.6 ± 0.4
LaMDA-137B + GPT-3 (code-davinci-001, 175B)
16.0 ± 0.8
LaMDA-137B + PaLM-540B + GPT-3 (code-davinci-001, 175B)
33.3 ± 0.7
Table 10: Comparison of GSM8K accuracy over multiple-model ensembles.
A.1.4
COMBINING SELF-CONSISTENCY WITH OTHER ENSEMBLING STRATEGIES
Self-consistency is completely compatible with other ensemble strategies, although the gains achieved
by self-consistency are signiﬁcantly higher than other ensemble strategies (and can “override” the
performance gains achieved by other ensemble strategies). We further performed experiments and
include the results in Table 11 (for a fair comparison, we use 40 sets of prompts, or 40 prompt
permutations to compare with self-consistency with 40 paths, all experiments are based on PaLM-
540B).
17
Published as a conference paper at ICLR 2023
GSM8K accuracy
Different sets of prompts (x40)
58.9
Prompt-permutation (x40)
59.6
Self-consistency (x40)
74.4
Self-consistency + different sets of prompts (x40)
75.4
Self-consistency + prompt-permutation (x40)
73.8
Table 11: Combining self-consistency with other ensembling strategies.
A.2
DETAILS ON RESOURCES AND INFERENCE
For all four language models we perform prompting-based inference only. For UL2 we use TPU v3
(2x2 conﬁguration, 4 chips, 8 cores). For GPT-3 models the experiments are done though the public
API.10 For LaMDA-137B we use TPU v3 (8x8 conﬁguration, 64 chips, 128 cores). For PaLM-540B
we use TPU v4 (4x4x12 conﬁguration, 192 chips, 384 cores). Most inference jobs take 1 to 4 hours
(over about 1,000 examples) for each task on UL2 and LaMDA-137B, and about 2 to 12 hours on
PaLM-540B. Some tasks (e.g., commonsense reasoning) take longer but do not exceed 2 days for
each task.
For GPT-3 models, we use 128 max tokens for all methods, without frequency penalty or presence
penalty. For all models, we take the generated outputs until the start of the next “Q:"" to parse the ﬁnal
answers, consistent with our prompting format.
Question
Greedy Decode
Sampled Path 1
Sampled Path 2
[MultiArith] Sam had 10 socks.
If he threw away 3 old ones
that didn’t ﬁt and bought 36 new
ones, how many socks would he
have?
Sam had 10 socks.
He threw
away 3 old ones that didn’t ﬁt.
He bought 36 new ones. That
means he has 10 + 36 - 3 = 41
socks. The answer is 41.
Sam had 10 socks.
He
threw away 3 old ones. So
now he has 7 left. Then he
bought 36 new ones. So
now he has 7 + 36 = 43
socks. The answer is 43.
He had 10 socks. He threw away
3 old ones that didn’t ﬁt.
He
bought 36 new ones. So in total
he has 10 - 3 + 36 = 43 socks. The
answer is 43.
[SVAMP] In a school there are
697 girls and the rest are boys.
If there are 228 more girls than
boys. How many boys are there
in that school?
There are 697 girls and the rest
are boys. That means there are
697 + 228 = 925 girls.
The
rest are boys. So the number of
boys is 925 - 697 = 228. The
answer is 228.
The number of boys is the
difference
between
697
and 228. This is 697 - 228
= 469. The answer is 469.
In the beginning there are 697
girls and the rest are boys. So, the
number of boys must be 697 - 228
= 469 boys. The answer is 469.
[AQuA] There are 10 oranges in
a basket. Find the no. of ways in
which 2 oranges are chosen from
the basket? (a) 45 (b) 90 (c) 120
(d) 150 (e) 180
There are 10 oranges in a bas-
ket.
So there are 10 ways
in which 2 oranges are chosen
from the basket.
There are 10 oranges in a
basket. We need to choose
2 oranges from the basket.
So the number of ways are
10C2. So the answer is (a).
There are 10 oranges in the bas-
ket. If you choose 2 oranges, you
have 10C2 = 45 ways of choosing
2 oranges. So the answer is (a).
[CommonsenseQA] The man
laid on the soft moss and looked
up at the trees, where was the
man? (a) niagra falls (b) forest
(c) waterfall (d) ground (e) tree
The answer must be something
that is soft and mossy. Of the
above choices, only ground is
soft and mossy. So the answer
is (d).
The
answer
must
be
somewhere where moss
is found.
Of the above
choices, only forests are
known for having moss.
So the answer is (b).
The answer must be a place where
the man is laying down and look-
ing up at trees.
Of the above
choices, only forest is a place
where people lay down and look
at trees. So the answer is (b).
[ARC] Which of these will most
likely harm a habitat? (a) plant-
ing trees. (b) water pollution. (c)
rainfall. (d) sunlight.
Planting trees will not harm a
habitat. So the answer is (a).
The answer is (b), water
pollution, because it will
most likely harm the habi-
tat. The other answers are
not likely to harm the habi-
tat.
The answer is (b), water pollution.
Trees are good for a habitat, rain-
fall is necessary for a habitat, and
sunlight is necessary for a habitat.
So the answer is (b).
[ARC-challenge]
Which
pro-
cess in the carbon cycle takes the
longest to complete?
(a) emis-
sion of waste. (b) respiration in
animals.
(c) photosynthesis in
plants. (d) formation of fossil fu-
els.
The process of photosynthesis
in plants takes the longest to
complete. So the answer is (c).
The formation of fossil fu-
els takes the longest time
to complete.
So the an-
swer is (d).
The process that takes the longest
time to complete is the forma-
tion of fossil fuels, which hap-
pens over millions of years. So
the answer is (d).
Table 12: Additional examples where self-consistency helps repair the errors over greedy decode on
LaMDA-137B. Two sampled reasoning paths that are consistent with the ground truth are shown.
10https://beta.openai.com/docs/api-reference/making-requests
18
Published as a conference paper at ICLR 2023
Question
Greedy Decode
Sampled Path 1
Sampled Path 2
[GSM8K] Kyle bought last
year’s best-selling book for
$19.50. This is with a 25% dis-
count from the original price.
What was the original price of
the book?
The book was sold for $19.50.
This is 25% less than the origi-
nal price. This means the original
price was 100%. 25% of 100%
is 25%.
So the original price
was 100% - 25% = 75%.
The
original price was 75% of $19.50.
75% of $19.50 is 75% * $19.50 =
$14.625. The answer is $14.625.
The original price of the book
is $19.50.
Since it was dis-
counted by 25%, the original
price was 19.50 / 0.75 = 26.
The answer is 26.
Kyle bought a book that is sold
for $19.50. With a 25% discount,
the original price must be higher.
If we call the original price x,
then the discounted price is 0.75x.
Since this is $19.50, we can write
0.75x = 19.50.
Solving this, we
ﬁnd that the original price is x =
19.50 / 0.75 = $26. The answer is
$26.
[AQuA] The capacity of a
tank of dimensions (8 m x 6
m x 2.5 m) is (a) 120 litres (b)
1200 litres (c) 12000 litres (d)
120000 litres (e) None of these
The volume of the tank is 8 m *
6 m * 2.5 m = 120 m3. So the
capacity of the tank is 120 m3. So
the answer is (c).
The volume of the tank is 8m
* 6m* 2.5m = 120m3. This is
equal to 120000 litres. So the
answer is (d)
The volume of the tank is 8 m *
6 m * 2.5 m = 120 m3. One cu-
bic meter contains 1000 litres, so
the tank contains 120 m3 * 1000
liters/m3 = 120,000 liters. So the
answer is (d).
[StrategyQA]
Yes
or
no:
Can the Great Depression be
treated with Prozac?
The Great Depression was an
economic depression. Prozac is
a drug used to treat depression.
Thus, the Great Depression could
be treated with Prozac. So the an-
swer is yes.
Prozac is an anti-depressant
medication. The Great Depres-
sion is not a disease.
Thus,
Prozac cannot treat the Great
Depression. So the answer is
no.
The Great Depression is a time pe-
riod.
Prozac is used to treat de-
pression (the mental illness). Thus,
the Great Depression can not be
treated with Prozac. So the answer
is no.
[ARC-easy] Which source of
energy is nonrenewable?
(a)
coal. (b) sunlight. (c) water.
(d) wood.
Coal, sunlight, and water are
all renewable sources of energy.
Wood is a nonrenewable source of
energy. So the answer is (d).
A nonrenewable source of en-
ergy is one in which the source
is not replenished in a short
amount of time. Thus, the only
option that is nonrenewable is
coal. So the answer is (a).
Coal is a fossil fuel and is formed
from dead organisms from the past.
Coal cannot be produced in a short
period of time, so it is considered
a nonrenewable source of energy.
So the answer is (a).
[ARC-challenge]
Students
studied the revolution of Earth
around the Sun. During which
month will Florida have the
most sunlight energy avail-
able? (a) March. (b) June. (c)
September. (d) December.
The Earth is closest to the Sun
in December, hence Florida will
have the most sunlight energy
available in December. So the an-
swer is (d).
Earth is closest to the Sun on
June 21, the summer solstice.
So the answer is (b).
Florida is closer to the sun during
June, so it will have the most sun-
light energy during that month. So
the answer is (b).
Table 13: Additional examples where self-consistency helps repair the errors over greedy decode on
PaLM-540B. Two sampled reasoning paths that are consistent with the ground truth are shown.
A.3
FULL SETS OF PROMPTS
We list the full details of the prompts used for two newly-introduced datasets, AQUA-RAT (Ling
et al., 2017) and AI2 Reasoning Challenge (ARC) (Clark et al., 2018), where we manually composed
the example chain-of-thought in this paper, in Table 14 and Table 15, respectively.
Table 14: Few-shot exemplars for AQUA-RAT.
Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the
numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64
A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean
would be 50. The answer is (a).
Q: If a / b = 3/4 and 8a + 5b = 22,then ﬁnd the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e)
7/2
A: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simpliﬁes to 8a + 20a / 3 = 22, which means
44a / 3 = 22. So a is equal to 3/2. The answer is (b).
Q: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then ﬁnd the distance? Answer Choices:
(a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
A: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (e).
Q: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392
(c) 1480 (d) 1562 (e) 1788
A: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401
three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).
19
Published as a conference paper at ICLR 2023
Table 15: Few-shot exemplars for ARC easy/challenge.
Q: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most
heat? (a) dry palms. (b) wet palms. (c) palms covered with oil. (d) palms covered with lotion.
A: Dry surfaces will more likely cause more friction via rubbing than other smoother surfaces, hence dry
palms will produce the most heat. The answer is (a).
Q: Which factor will most likely cause a person to develop a fever? (a) a leg muscle relaxing after exercise.
(b) a bacterial population in the bloodstream. (c) several viral particles on the skin. (d) carbohydrates being
digested in the stomach.
A: Option (b), bacterial population is the most likely cause for a person developing fever. The answer is (b).
Q: Which change in the state of water particles causes the particles to become arranged in a ﬁxed position?
(a) boiling. (b) melting. (c) freezing. (d) evaporating.
A: When water is freezed, the particles are arranged in a ﬁxed position; the particles are still moving for all
other options. The answer is (c).
Q: When a switch is used in an electrical circuit, the switch can (a) cause the charge to build. (b) increase
and decrease the voltage. (c) cause the current to change direction. (d) stop and start the ﬂow of current.
A: The function of a switch is to start and stop the ﬂow of a current. The answer is (d).
Table 16: Few-shot exemplars for HotpotQA (closed-book setting).
Q: Which magazine was started ﬁrst Arthur’s Magazine or First for Women?
A: Arthur’s Magazine started in 1844. First for Women started in 1989. So Arthur’s Magazine was started ﬁrst.
The answer is Arthur’s Magazine.
Q: The Oberoi family is part of a hotel company that has a head ofﬁce in what city?
A: The Oberoi family is part of the hotel company called The Oberoi Group. The Oberoi Group has its head
ofﬁce in Delhi. The answer is Delhi.
Q: What nationality was James Henry Miller’s wife?
A: James Henry Miller’s wife is June Miller. June Miller is an American. The answer is American.
Q: The Dutch-Belgian television series that ""House of Anubis"" was based on ﬁrst aired in what year?
A: ""House of Anubis"" is based on the Dutch–Belgian television series Het Huis Anubis. Het Huis Anubis is ﬁrst
aired in September 2006. The answer is 2006.
As additional information, we also list the exact set of prompts used for all arithmetic reasoning tasks
in Table 17, since there are multiple sets of prompts introduced in Wei et al. (2022). The prompts for
CommonsenseQA and StrategyQA are the same as used in Wei et al. (2022).
We provide the exact prompts used for common NLP tasks in the following tables as well, including
NLI (Table 18, Table 19, Table 20) and Closed-Book Question-Answering tasks (Table 16, Table 21).
20
Published as a conference paper at ICLR 2023
Table 17: Few-shot exemplars for all arithmetic reasoning tasks, from Wei et al. (2022).
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,
there will be 21 trees. How many trees did the grove workers plant today?
A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.
So, they must have planted 21 - 15 = 6 trees. The answer is 6.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74
chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.
Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops
did Jason give to Denny?
A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of
lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.
Q: Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does
he have now?
A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so
in total he has 7 + 2 = 9 toys. The answer is 9.
Q: There were nine computers in the server room. Five more computers were installed each day, from
monday to thursday. How many computers are now in the server room?
A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =
20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.
The answer is 29.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many
golf balls did he have at the end of wednesday?
A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On
Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.
Q: Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in
beginning, so now she has $23 - $15 = $8. The answer is 8.
21
Published as a conference paper at ICLR 2023
Table 18: Few-shot exemplars for ANLI.
Premise:
""Conceptually cream skimming has two basic dimensions - product and geography.""
Based on this premise, can we conclude the hypothesis ""Product and geography are what make cream skimming
work."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: Based on ""cream skimming has two basic dimensions"" we can’t infer that these two dimensions are what
make cream skimming work. The answer is it is not possible to tell.
Premise:
""One of our member will carry out your instructions minutely.""
Based on this premise, can we conclude the hypothesis ""A member of my team will execute your orders with
immense precision."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: ""one of"" means the same as ""a member of"", ""carry out"" means the same as ""execute"", and ""minutely"" means
the same as ""immense precision"". The answer is yes.
Premise:
""Fun for adults and children.""
Based on this premise, can we conclude the hypothesis ""Fun for only children."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: ""adults and children"" contradicts ""only children"". The answer is no.
Premise:
""He turned and smiled at Vrenna.""
Based on this premise, can we conclude the hypothesis ""He smiled at Vrenna who was walking slowly behind
him with her mother."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: the premise does not say anything about ""Vrenna was walking"". The answer is it is not possible to tell.
Premise:
""well you see that on television also""
Based on this premise, can we conclude the hypothesis ""You can see that on television, as well."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: ""also"" and ""as well"" mean the same thing. The answer is yes.
Premise:
""Vrenna and I both fought him and he nearly took us.""
Based on this premise, can we conclude the hypothesis ""Neither Vrenna nor myself have ever fought him."" is
true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: ""Vrenna and I both"" contradicts ""neither Vrenna nor myself"". The answer is no.
22
Published as a conference paper at ICLR 2023
Table 19: Few-shot exemplars for e-SNLI (the rationales are crowd-sourced annotations from the
original dataset).
Premise:
""A person on a horse jumps over a broken down airplane.""
Based on this premise, can we conclude the hypothesis ""A person is training his horse for a competition."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: The person is not necessarily training his horse. The answer is it is not possible to tell.
Premise:
""A person on a horse jumps over a broken down airplane.""
Based on this premise, can we conclude the hypothesis ""A person is at a diner, ordering an omelette."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: One jumping horse cannot be in a diner ordering food. The answer is no.
Premise:
""A person on a horse jumps over a broken down airplane.""
Based on this premise, can we conclude the hypothesis ""A person is outdoors, on a horse."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: A broken down airplane is outdoors. The answer is yes.
Premise:
""Children smiling and waving at camera.""
Based on this premise, can we conclude the hypothesis ""They are smiling at their parents."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A: Just because they are smiling and waving at a camera does not imply their parents or anyone is anyone behind
it. The answer is it is not possible to tell.
Premise:
""Children smiling and waving at camera.""
Based on this premise, can we conclude the hypothesis ""The kids are frowning."" is true? OPTIONS:
- yes
- no
- it is not possible to tell
A: One cannot be smiling and frowning at the same time. The answer is no.
Premise:
""Children smiling and waving at camera.""
Based on this premise, can we conclude the hypothesis ""There are children present."" is true?
OPTIONS:
- yes
- no
- it is not possible to tell
A:The children must be present to see them smiling and waving. The answer is yes.
23
Published as a conference paper at ICLR 2023
Table 20: Few-shot exemplars for RTE.
Premise:
""No Weapons of Mass Destruction Found in Iraq Yet.""
Based on this premise, can we conclude the hypothesis ""Weapons of Mass Destruction Found in Iraq."" is true?
A: ""No Weapons of Mass Destruction Found"" contradicts ""Weapons of Mass Destruction Found"". The answer is
no.
Premise:
""A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful
gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.""
Based on this premise, can we conclude the hypothesis ""Pope Benedict XVI is the new leader of the Roman
Catholic Church."" is true?’
A: ""installation of new Pope Benedict XVI."" means ""Pope Benedict XVI is the new leader"". The answer is yes.
Premise:
""A man is due in court later charged with the murder 26 years ago of a teenager whose case was the ﬁrst to
be featured on BBC One’s Crimewatch. Colette Aram, 16, was walking to her boyfriend’s house in Keyworth,
Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a ﬁeld close to her
home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates
later.""
Based on this premise, can we conclude the hypothesis ""Paul Stewart Hutchinson is accused of having stabbed a
girl."" is true?
A: The premise does not say Paul Stewart Hutchinson ""stabbed"" this girl. The answer is no.
Premise:
""Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it
will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.""
Based on this premise, can we conclude the hypothesis ""Herceptin can be used to treat breast cancer."" is true?
A: ""Herceptin was approved to treat breast cancer"" implies that ""Herceptin can be used to treat breast cancer"".
The answer is yes.
Table 21: Few-shot exemplars for BoolQ (closed-book setting).
Q: does system of a down have 2 singers?
A: System of a Down currently consists of Serj Tankian, Daron Malakian, Shavo Odadjian and John Dolmayan.
Serj and Daron do vocals, so the band does have two singers. The answer is yes.
Q: do iran and afghanistan speak the same language?
A: Iran and Afghanistan both speak the Indo-European language Persian. The answer is yes.
Q: is a cello and a bass the same thing?
A: The cello is played sitting down with the instrument between the knees, whereas the double bass is played
standing or sitting on a stool. The answer is no.
Q: can you use oyster card at epsom station?
A: Epsom railway station serves the town of Epsom in Surrey and is not in the London Oyster card zone. The
answer is no.
24
</Content></Document>
    </context>

Lembre-se de que o objetivo de cada resumo é servir como um guia de estudo para um Cientista de Dados especialista em AI, Estatística e Deep Learning, com conhecimentos avançados em tecnologia e programação.

!!! Expressões matemáticas usando $ ao invés de \( e \), e $$ ao invés de \[ e \] !!!
!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \[ e \( !!!

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!

<template>

Crie um resumo avançado, aprofundado e elaborado sobre X (mínimo de 8 páginas, extenso, não poupe detalhes, aprofunde-se em conceitos técnicos e matemáticos)

**X =** 

Utilize a formatação abaixo como inspiração para o resumo, mas faça as adaptações necessárias com o objetivo de criar o melhor resumo possível. Lembre-se de que o objetivo é servir como um guia de estudo para um Cientista de Dados especialista em AI, Estatística e Deep Learning, com conhecimentos avançados em tecnologia e programação.

Orientações para escrever o resumo:

**Organização e Estrutura**: Garanta que cada seção do resumo esteja bem organizada e siga uma lógica clara. Utilize títulos e subtítulos para facilitar a navegação. Crie uma estrutura hierárquica coerente, com uma introdução, desenvolvimento e conclusão bem definidos.

**Detalhamento**: Aprofunde-se nos conceitos técnicos e matemáticos, fornecendo explicações detalhadas, exemplos práticos e demonstrações passo a passo quando necessário.

**Destaques**: Sempre que mencionar os conceitos principais no texto, utilize **negrito** para destacá-los. Quando quiser inserir uma citação importante ou parafrasear alguém, utilize *itálico*. Utilize caixas de destaque, como notas, avisos e dicas, para enfatizar informações cruciais.

**Estilo e tom:** Escreva de forma acadêmica e formal, mas use emojis quando necessário para dar destaque a alguma informação, por exemplo, ao destacar um tópico usando blockquotes. Utilize emojis como ⚠️❗✔️💡 e outros que façam sentido dado o conteúdo. Mantenha um tom instrutivo e explicativo ao longo do texto.

Template para o resumo:

## Título do Resumo (seja breve)

Inicie com uma introdução concisa, porém abrangente, que contextualize a importância do tema.

### Principais Conceitos

| Conceito       | Explicação                                                   |
| -------------- | ------------------------------------------------------------ |
| **Conceito 1** | Forneça uma explicação concisa do conceito, explorando as bases teóricas e suas aplicações práticas. |
| **Conceito 2** | Forneça uma explicação concisa do conceito, explorando as bases teóricas e suas aplicações práticas. |

Utilize as formatações abaixo como exemplo para destacar informações importantes e críticas:

> ⚠️ **Nota Importante**: Use esta formatação para destacar informações críticas ou observações que não podem ser ignoradas, assegurando que se destaquem no contexto do resumo.

> ❗ **Ponto de Atenção**: Use esta formatação para destacar informações críticas ou observações que requerem maior atenção ao implementar, pois colocam em risco o uso correto do conceito e devem ser levadas em conta pelo usuário.

> ✔️ **Ponto de Destaque** (técnicos e teóricos): Use esta formatação para destacar informações críticas ou observações teóricas ou técnicas que impactam de forma positiva na compreensão do fenômeno, como resultados importantes que não podem ser ignorados.

### Abstract

Copie o abstract ou sumario consolidado dos documentos usados no resumo nessa de forma que o leitor tenha essa referência em mãos quando for estudar o artigo.

### [Explicação de algum tópico ou conceito]

Elabore de forma aprofundada sobre os tópicos e conceitos do tema X, de modo que o resumo seja avançado, detalhado, bem escrito e cumpra os objetivos do texto. Não poupe detalhes!

Quando for contrastar, comparar, etc., informações, use a formatação de lista de tópicos como no exemplo:

#### 👍Vantagens

* Vantagem 1: explicação detalhada e concisa do ponto de vantagem (exemplo)
* Vantagem 2: explicação detalhada e concisa do ponto de vantagem (exemplo)

#### 👎Desvantagens

* Desvantagem 1: explicação detalhada e concisa do ponto de desvantagem (exemplo)
* Desvantagem 2: explicação detalhada e concisa do ponto de desvantagem (exemplo)

Ou de tabela, dependendo de qual melhor se ajustar ao conteúdo:

| 👍 Vantagens                                                  | 👎 Desvantagens                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Explicação detalhada e concisa do ponto de vantagem (exemplo) | Explicação detalhada e concisa do ponto de desvantagem (exemplo) |
| Explicação detalhada e concisa do ponto de vantagem (exemplo) | Explicação detalhada e concisa do ponto de desvantagem (exemplo) |

Use esse exemplo apenas como inspiração e utilize esses tipos de formatação de acordo com a necessidade de elaborar sobre algum ponto tópico do tema.

### [Explicação de algum tópico ou conceito teórico]

Apresente definições matemáticas e teóricas detalhadas, sem economizar em complexidade. Use a seguinte formatação para equações importantes, garantindo que sejam facilmente legíveis e centralizadas. Por exemplo:

O Teorema de Bayes é um resultado fundamental na teoria da probabilidade que descreve como atualizar as probabilidades de uma hipótese com base em novas evidências. Ele estabelece uma relação entre as probabilidades condicionais de dois eventos.

Seja $A$ e $B$ dois eventos, o Teorema de Bayes afirma que:

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

onde:

- $P(A|B)$ é a probabilidade condicional de $A$ dado $B$, também conhecida como probabilidade a posteriori.
- $P(B|A)$ é a probabilidade condicional de $B$ dado $A$, também conhecida como verossimilhança.
- $P(A)$ é a probabilidade a priori de $A$.
- $P(B)$ é a probabilidade marginal de $B$, que atua como uma constante normalizadora.

A probabilidade marginal $P(B)$ pode ser calculada usando a lei da probabilidade total:

$$
P(B) = P(B|A)P(A) + P(B|\neg A)P(\neg A)
$$

onde $\neg A$ denota o evento complementar de $A$.

Prossiga com uma discussão detalhada para explicar o funcionamento da equação e suas implicações do conceito. Faça uma análise de seu comportamento matemático, se possível. Por exemplo:

O Teorema de Bayes permite atualizar nossas crenças (probabilidades) sobre uma hipótese $A$ após observar novas evidências $B$. Ele combina nossa crença prévia em $A$ (probabilidade a priori) com a probabilidade de observar $B$ dado que $A$ é verdadeiro (verossimilhança) para obter nossa crença atualizada em $A$ dado $B$ (probabilidade a posteriori).

> ✔️ **Ponto de Destaque**: O Teorema de Bayes fornece uma estrutura matemática para o raciocínio probabilístico e a atualização de crenças com base em novas informações. Ele é amplamente aplicado em áreas como aprendizado de máquina, estatística, ciência da computação e tomada de decisão.

!!! Expressões matemáticas usando $ ao invés de \( e \), e $$ ao invés de \[ e \] !!!
!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \[ e \( !!!

### [Explicação de algum tópico ou conceito técnico]

Coloque aqui informações relevantes e concisas para explicar a aplicação do tópico e como implementá-lo. Quando houver necessidade de mostrar um código na linguagem apropriada, use a formatação:

```python
import lib # assuma que as dependências já estão instaladas

# Comentário para elucidar apenas aspectos importantes
def minha_funcao(param):
	return lib.outra_funcao(param)
```

Mantenha os snippets claros, concisos e o menor possível, com foco na funcionalidade principal. Não adicione códigos de setup como pip install, downloads, etc.

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!

### [Aplicações|Trabalhos futuros|Extensões|etc]

Se houver necessidade de falar sobre aplicações do conceito, trabalhos e pesquisas futuras, áreas de interesse e extensões do conceito, use o seguinte formato:

| Conceito       | Explicação                                                   |
| -------------- | ------------------------------------------------------------ |
| **Conceito 1** | Explicação detalhada do conceito, incluindo exemplos práticos e aplicações. |
| **Conceito 2** | Explicação detalhada do conceito, incluindo exemplos práticos e aplicações. |

### [Tópicos Relacionados]

Para orientar o usuário desse guia, crie uma lista de próximos tópicos avançados relacionados, quando houver necessidade:

- [ ] Tópico relacionado 1
- [ ] Tópico relacionado 2
- [ ] etc.

### Conclusão

Resuma todos os tópicos apresentados em uma conclusão sucinta e objetiva.

### Referências

Adicione aqui as referências da seguinte forma:

[1] Attention is All You Need
[2] Other paper name
[3] Etc

Quando uma referência é usada mais de uma vez não precisa repiti-la aqui. Basta indicar apenas uma vez.

!!! Lembre-se de que esse template é apenas um guia e você deve apenas se inspirar nele, sem a necessidade de replicar a mesma estrutura ao pé da letra. Foque no objetivo !!!

!!! NÃO POUPE DETALHES, SEJA O MAIS APROFUNDADO POSSÍVEL !!!

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!

!!! Expressões matemáticas usando $ ao invés de \( e \), e $$ ao invés de \[ e \] !!!
!!! E quando citar variáveis, funções ou trechos de expressões matemáticas use $f(x)$ ao invés de **f(x)** ou \[ e \( !!!

</template>

Diretrizes para o resumo:
Os resumos devem ser avançados;
Os resumos devem ser baseados nos principais aspectos do conceito abordado no texto, como técnicas ou funcionalidades específicas demonstradas em cada subcapítulo;
O resumo deve conter todas principais informações presentes no texto sem omitir nenhum dado importante, com foco especial em não pular nenhum conceitos, resultados importante, argumentos, etc;
O resumo deve conter as equações apresentadas, tabelas e outras informações críticas para um entendimento aprofundando e avançado do conteúdo;
O resumo deve ser escrito de uma maneira acadêmica, do not repeat text.
Você deve usar o <context> da melhor maneira possível para responder a query do usuário e escrever o resumo segundo as diretrizes;
You must not tell the user to open any link or visit any website to get the answer. You must provide the answer in the response itself;
Você não deve pedir para o usuário abrir um link ou visitar um site para ver a resposta. Você deve responder você mesmo;
You have to cite the answer using [number] notation. The number is the idx on the documents. You must cite the sentences with their relevent context number. You must cite each and every part of the answer so the user can know where the information is coming from.
Place these citations at the end of that particular sentence. You can cite the same sentence multiple times if it is relevant to the user's query like [number1][number2].
However you do not need to cite it using the same number. You can use different numbers to cite the same sentence multiple times. The number refers to the number of the search result (passed in the context) called index used to generate that part of the answer.
Coloque os resultados um texto coerente ao invés de apenas listar em tópicos, também foque em usar as formatações mostradas no template.

!!! CÓDIGO SOMENTE QUANDO ESTIVER PRESENTE EM ALGUM DOCUMENTO, NÃO CRIE TRECHOS DE CÓDIGO !!!


X = How chain of thought is used to improve reasoning in large language models",,,2024-07-08 17:17:34.841194
